id,_id,author,cardData,inferenceProviderMapping,lastModified,likes,trendingScore,config,downloads,downloadsAllTime,safetensors,tags,pipeline_tag,siblings,createdAt,library_name,transformersInfo,gguf,relation,base_model_id,base_model_name,modelId,yaml_header,card_content,content_length,sample_weight
RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16,66a3f1c5ee3de8c56ef34fa3,RedHatAI,"{""language"": [""en"", ""de"", ""fr"", ""it"", ""pt"", ""hi"", ""es"", ""th""], ""base_model"": [""meta-llama/Llama-3.1-8B-Instruct""], ""pipeline_tag"": ""text-generation"", ""tags"": [""llama"", ""facebook"", ""meta"", ""llama-3"", ""int4"", ""vllm"", ""chat"", ""neuralmagic"", ""llmcompressor"", ""conversational"", ""4-bit precision"", ""gptq"", ""compressed-tensors""], ""license"": ""llama3.1"", ""license_name"": ""llama3.1"", ""name"": ""RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"", ""description"": ""This model was obtained by quantizing the weights of Meta-Llama-3.1-8B-Instruct to INT4 data type."", ""readme"": ""https://huggingface.co/RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16/main/README.md"", ""tasks"": [""text-to-text""], ""provider"": ""Meta"", ""license_link"": ""https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE"", ""validated_on"": [""RHOAI 2.20"", ""RHAIIS 3.0"", ""RHELAI 1.5""]}",[],2025-09-22 20:34:38,30,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""quantization_config"": {""bits"": 4, ""quant_method"": ""gptq""}, ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""chat_template"": ""{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \""26 Jul 2024\"" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \""\"" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \""<|start_header_id|>system<|end_header_id|>\\n\\n\"" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \""Environment: ipython\\n\"" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \""Tools: \"" + builtin_tools | reject('equalto', 'code_interpreter') | join(\"", \"") + \""\\n\\n\""}}\n{%- endif %}\n{{- \""Cutting Knowledge Date: December 2023\\n\"" }}\n{{- \""Today Date: \"" + date_string + \""\\n\\n\"" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \""You have access to the following functions. To call a function, please respond with JSON for a function call.\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \""<|eot_id|>\"" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\""Cannot put tools in the first user message when there's no first user message!\"") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \""Given the following functions, please respond with a JSON for a function call \"" }}\n    {{- \""with its proper arguments that best answers the given prompt.\\n\\n\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n    {{- first_user_message + \""<|eot_id|>\""}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\""This model only supports single tool-calls at once!\"") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \""<|python_tag|>\"" + tool_call.name + \"".call(\"" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\""' + arg_val + '\""' }}\n                {%- if not loop.last %}\n                    {{- \"", \"" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \"")\"" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\""name\"": \""' + tool_call.name + '\"", ' }}\n            {{- '\""parameters\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \""}\"" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \""<|eom_id|>\"" }}\n        {%- else %}\n            {{- \""<|eot_id|>\"" }}\n        {%- endif %}\n    {%- elif message.role == \""tool\"" or message.role == \""ipython\"" %}\n        {{- \""<|start_header_id|>ipython<|end_header_id|>\\n\\n\"" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \""<|eot_id|>\"" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|eot_id|>""}}",39293,1174624,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': 1106841600, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': 880476160, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 1987317760.0}","['transformers' 'safetensors' 'llama' 'text-generation' 'facebook' 'meta'
 'llama-3' 'int4' 'vllm' 'chat' 'neuralmagic' 'llmcompressor'
 'conversational' '4-bit precision' 'gptq' 'compressed-tensors' 'en' 'de'
 'fr' 'it' 'pt' 'hi' 'es' 'th'
 'base_model:meta-llama/Llama-3.1-8B-Instruct'
 'base_model:quantized:meta-llama/Llama-3.1-8B-Instruct'
 'license:llama3.1' 'autotrain_compatible' 'text-generation-inference'
 'endpoints_compatible' '4-bit' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'quantize_config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2024-07-26 18:58:13,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,quantized,6698d8a0653e4babe21e1e7d,meta-llama/Llama-3.1-8B-Instruct,RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16,"language:
- en
- de
- fr
- it
- pt
- hi
- es
- th
base_model:
- meta-llama/Llama-3.1-8B-Instruct
pipeline_tag: text-generation
tags:
- llama
- facebook
- meta
- llama-3
- int4
- vllm
- chat
- neuralmagic
- llmcompressor
- conversational
- 4-bit precision
- gptq
- compressed-tensors
license: llama3.1
license_name: llama3.1
name: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
description: This model was obtained by quantizing the weights of Meta-Llama-3.1-8B-Instruct to INT4 data type.
readme: https://huggingface.co/RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16/main/README.md
tasks:
- text-to-text
provider: Meta
license_link: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE
validated_on:
  - RHOAI 2.20
  - RHAIIS 3.0
  - RHELAI 1.5","<h1 style=""display: flex; align-items: center; gap: 10px; margin: 0;"">
  Meta-Llama-3.1-8B-Instruct-quantized.w4a16
  <img src=""https://www.redhat.com/rhdc/managed-files/Catalog-Validated_model_0.png"" alt=""Model Icon"" width=""40"" style=""margin: 0; padding: 0;"" />
</h1>
  
<a href=""https://www.redhat.com/en/products/ai/validated-models"" target=""_blank"" style=""margin: 0; padding: 0;"">
<img src=""https://www.redhat.com/rhdc/managed-files/Validated_badge-Dark.png"" alt=""Validated Badge"" width=""250"" style=""margin: 0; padding: 0;"" />
</a>

## Model Overview
- **Model Architecture:** Meta-Llama-3
  - **Input:** Text
  - **Output:** Text
- **Model Optimizations:**
  - **Weight quantization:** INT4
- **Intended Use Cases:** Intended for commercial and research use in English. Similarly to [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), this models is intended for assistant-like chat.
- **Out-of-scope:** Use in any manner that violates applicable laws or regulations (including trade compliance laws). Use in languages other than English.
- **Release Date:** 7/26/2024
- **Version:** 1.0
- **Validated on:** RHOAI 2.20, RHAIIS 3.0, RHELAI 1.5
- **License(s):** Llama3.1
- **Model Developers:** Neural Magic

This model is a quantized version of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).
It was evaluated on a several tasks to assess the its quality in comparison to the unquatized model, including multiple-choice, math reasoning, and open-ended text generation.
Meta-Llama-3.1-8B-Instruct-quantized.w4a16 achieves 93.0% recovery for the Arena-Hard evaluation, 98.9% for OpenLLM v1 (using Meta's prompting when available), 96.1% for OpenLLM v2, 99.7% for HumanEval pass@1, and 97.4% for HumanEval+ pass@1.

### Model Optimizations

This model was obtained by quantizing the weights of [Meta-Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) to INT4 data type.
This optimization reduces the number of bits per parameter from 16 to 4, reducing the disk size and GPU memory requirements by approximately 75%.

Only the weights of the linear operators within transformers blocks are quantized.
Symmetric per-group quantization is applied, in which a linear scaling per group of 128 parameters maps the INT4 and floating point representations of the quantized weights.
[AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) is used for quantization with 10% damping factor and 768 sequences taken from Neural Magic's [LLM compression calibration dataset](https://huggingface.co/datasets/neuralmagic/LLM_compression_calibration).


## Deployment

This model can be deployed efficiently using the [vLLM](https://docs.vllm.ai/en/latest/) backend, as shown in the example below.

```python
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer

model_id = ""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16""
number_gpus = 1
max_model_len = 8192

sampling_params = SamplingParams(temperature=0.6, top_p=0.9, max_tokens=256)

tokenizer = AutoTokenizer.from_pretrained(model_id)

messages = [
    {""role"": ""system"", ""content"": ""You are a pirate chatbot who always responds in pirate speak!""},
    {""role"": ""user"", ""content"": ""Who are you?""},
]

prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)

llm = LLM(model=model_id, tensor_parallel_size=number_gpus, max_model_len=max_model_len)

outputs = llm.generate(prompts, sampling_params)

generated_text = outputs[0].outputs[0].text
print(generated_text)
```

vLLM also supports OpenAI-compatible serving. See the [documentation](https://docs.vllm.ai/en/latest/) for more details.

<details>
  <summary>Deploy on <strong>Red Hat AI Inference Server</strong></summary>
  
```bash
podman run --rm -it --device nvidia.com/gpu=all -p 8000:8000 \
 --ipc=host \
--env ""HUGGING_FACE_HUB_TOKEN=$HF_TOKEN"" \
--env ""HF_HUB_OFFLINE=0"" -v ~/.cache/vllm:/home/vllm/.cache \
--name=vllm \
registry.access.redhat.com/rhaiis/rh-vllm-cuda \
vllm serve \
--tensor-parallel-size 8 \
--max-model-len 32768  \
--enforce-eager --model RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
```
​​See [Red Hat AI Inference Server documentation](https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/) for more details.
</details>

<details>
  <summary>Deploy on <strong>Red Hat Enterprise Linux AI</strong></summary>
  
```bash
# Download model from Red Hat Registry via docker
# Note: This downloads the model to ~/.cache/instructlab/models unless --model-dir is specified.
ilab model download --repository docker://registry.redhat.io/rhelai1/llama-3-1-8b-instruct-quantized-w4a16:1.5
```

```bash
# Serve model via ilab
ilab model serve --model-path ~/.cache/instructlab/models/llama-3-1-8b-instruct-quantized-w4a16
  
# Chat with model
ilab model chat --model ~/.cache/instructlab/models/llama-3-1-8b-instruct-quantized-w4a16
```
See [Red Hat Enterprise Linux AI documentation](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_ai/1.4) for more details.
</details>

<details>
  <summary>Deploy on <strong>Red Hat Openshift AI</strong></summary>
  
```python
# Setting up vllm server with ServingRuntime
# Save as: vllm-servingruntime.yaml
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
 name: vllm-cuda-runtime # OPTIONAL CHANGE: set a unique name
 annotations:
   openshift.io/display-name: vLLM NVIDIA GPU ServingRuntime for KServe
   opendatahub.io/recommended-accelerators: '[""nvidia.com/gpu""]'
 labels:
   opendatahub.io/dashboard: 'true'
spec:
 annotations:
   prometheus.io/port: '8080'
   prometheus.io/path: '/metrics'
 multiModel: false
 supportedModelFormats:
   - autoSelect: true
     name: vLLM
 containers:
   - name: kserve-container
     image: quay.io/modh/vllm:rhoai-2.20-cuda # CHANGE if needed. If AMD: quay.io/modh/vllm:rhoai-2.20-rocm
     command:
       - python
       - -m
       - vllm.entrypoints.openai.api_server
     args:
       - ""--port=8080""
       - ""--model=/mnt/models""
       - ""--served-model-name={{.Name}}""
     env:
       - name: HF_HOME
         value: /tmp/hf_home
     ports:
       - containerPort: 8080
         protocol: TCP
```

```python
# Attach model to vllm server. This is an NVIDIA template
# Save as: inferenceservice.yaml
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    openshift.io/display-name: llama-3-1-8b-instruct-quantized-w4a16 # OPTIONAL CHANGE
    serving.kserve.io/deploymentMode: RawDeployment
  name: llama-3-1-8b-instruct-quantized-w4a16          # specify model name. This value will be used to invoke the model in the payload
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  predictor:
    maxReplicas: 1
    minReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ''
      resources:
        limits:
          cpu: '2'			# this is model specific
          memory: 8Gi		# this is model specific
          nvidia.com/gpu: '1'	# this is accelerator specific
        requests:			# same comment for this block
          cpu: '1'
          memory: 4Gi
          nvidia.com/gpu: '1'
      runtime: vllm-cuda-runtime	# must match the ServingRuntime name above
      storageUri: oci://registry.redhat.io/rhelai1/modelcar-llama-3-1-8b-instruct-quantized-w4a16:1.5
    tolerations:
    - effect: NoSchedule
      key: nvidia.com/gpu
      operator: Exists
```

```bash
# make sure first to be in the project where you want to deploy the model
# oc project <project-name>

# apply both resources to run model

# Apply the ServingRuntime
oc apply -f vllm-servingruntime.yaml

# Apply the InferenceService
oc apply -f qwen-inferenceservice.yaml
```

```python
# Replace <inference-service-name> and <cluster-ingress-domain> below:
# - Run `oc get inferenceservice` to find your URL if unsure.

# Call the server using curl:
curl https://<inference-service-name>-predictor-default.<domain>/v1/chat/completions
        -H ""Content-Type: application/json"" \
        -d '{
    ""model"": ""llama-3-1-8b-instruct-quantized-w4a16"",
    ""stream"": true,
    ""stream_options"": {
        ""include_usage"": true
    },
    ""max_tokens"": 1,
    ""messages"": [
        {
            ""role"": ""user"",
            ""content"": ""How can a bee fly when its wings are so small?""
        }
    ]
}'

```

See [Red Hat Openshift AI documentation](https://docs.redhat.com/en/documentation/red_hat_openshift_ai/2025) for more details.
</details>


## Creation

This model was created by applying the [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) library as presented in the code snipet below.
Although AutoGPTQ was used for this particular model, Neural Magic is transitioning to using [llm-compressor](https://github.com/vllm-project/llm-compressor) which supports several quantization schemes and models not supported by AutoGPTQ.

```python
from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from datasets import load_dataset

model_id = ""meta-llama/Meta-Llama-3.1-8B-Instruct""

num_samples = 756
max_seq_len = 4064

tokenizer = AutoTokenizer.from_pretrained(model_id)

def preprocess_fn(example):
  return {""text"": tokenizer.apply_chat_template(example[""messages""], add_generation_prompt=False, tokenize=False)}

ds = load_dataset(""neuralmagic/LLM_compression_calibration"", split=""train"")
ds = ds.shuffle().select(range(num_samples))
ds = ds.map(preprocess_fn)

examples = [tokenizer(example[""text""], padding=False, max_length=max_seq_len, truncation=True) for example in ds]
    
quantize_config = BaseQuantizeConfig(
  bits=4,
  group_size=128,
  desc_act=True,
  model_file_base_name=""model"",
  damp_percent=0.1,
)

model = AutoGPTQForCausalLM.from_pretrained(
  model_id,
  quantize_config,
  device_map=""auto"",
)

model.quantize(examples)
model.save_pretrained(""Meta-Llama-3.1-8B-Instruct-quantized.w4a16"")
```

## Evaluation

This model was evaluated on the well-known Arena-Hard, OpenLLM v1, OpenLLM v2, HumanEval, and HumanEval+ benchmarks.
In all cases, model outputs were generated with the [vLLM](https://docs.vllm.ai/en/stable/) engine.

Arena-Hard evaluations were conducted using the [Arena-Hard-Auto](https://github.com/lmarena/arena-hard-auto) repository.
The model generated a single answer for each prompt form Arena-Hard, and each answer was judged twice by GPT-4.
We report below the scores obtained in each judgement and the average.

OpenLLM v1 and v2 evaluations were conducted using Neural Magic's fork of [lm-evaluation-harness](https://github.com/neuralmagic/lm-evaluation-harness/tree/llama_3.1_instruct) (branch llama_3.1_instruct).
This version of the lm-evaluation-harness includes versions of MMLU, ARC-Challenge and GSM-8K that match the prompting style of [Meta-Llama-3.1-Instruct-evals](https://huggingface.co/datasets/meta-llama/Meta-Llama-3.1-8B-Instruct-evals) and a few fixes to OpenLLM v2 tasks.

HumanEval and HumanEval+ evaluations were conducted using Neural Magic's fork of the [EvalPlus](https://github.com/neuralmagic/evalplus) repository.

Detailed model outputs are available as HuggingFace datasets for [Arena-Hard](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-arena-hard-evals), [OpenLLM v2](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-leaderboard-v2-evals), and [HumanEval](https://huggingface.co/datasets/neuralmagic/quantized-llama-3.1-humaneval-evals).

**Note:** Results have been updated after Meta modified the chat template.

### Accuracy

<table>
  <tr>
   <td><strong>Category</strong>
   </td>
   <td><strong>Benchmark</strong>
   </td>
   <td><strong>Meta-Llama-3.1-8B-Instruct </strong>
   </td>
   <td><strong>Meta-Llama-3.1-8B-Instruct-quantized.w4a16 (this model)</strong>
   </td>
   <td><strong>Recovery</strong>
   </td>
  </tr>
  <tr>
   <td rowspan=""1"" ><strong>LLM as a judge</strong>
   </td>    
   <td>Arena Hard
   </td>
   <td>25.8 (25.1 / 26.5)
   </td>
   <td>27.2 (27.6 / 26.7)
   </td>
   <td>105.4%
   </td>
  </tr>
  <tr>
   <td rowspan=""8"" ><strong>OpenLLM v1</strong>
   </td>
   <td>MMLU (5-shot)
   </td>
   <td>68.3
   </td>
   <td>66.9
   </td>
   <td>97.9%
   </td>
  </tr>
  <tr>
   <td>MMLU (CoT, 0-shot)
   </td>
   <td>72.8
   </td>
   <td>71.1
   </td>
   <td>97.6%
   </td>
  </tr>
  <tr>
   <td>ARC Challenge (0-shot)
   </td>
   <td>81.4
   </td>
   <td>80.2
   </td>
   <td>98.0%
   </td>
  </tr>
  <tr>
   <td>GSM-8K (CoT, 8-shot, strict-match)
   </td>
   <td>82.8
   </td>
   <td>82.9
   </td>
   <td>100.2%
   </td>
  </tr>
  <tr>
   <td>Hellaswag (10-shot)
   </td>
   <td>80.5
   </td>
   <td>79.9
   </td>
   <td>99.3%
   </td>
  </tr>
  <tr>
   <td>Winogrande (5-shot)
   </td>
   <td>78.1
   </td>
   <td>78.0
   </td>
   <td>99.9%
   </td>
  </tr>
  <tr>
   <td>TruthfulQA (0-shot, mc2)
   </td>
   <td>54.5
   </td>
   <td>52.8
   </td>
   <td>96.9%
   </td>
  </tr>
  <tr>
   <td><strong>Average</strong>
   </td>
   <td><strong>74.3</strong>
   </td>
   <td><strong>73.5</strong>
   </td>
   <td><strong>98.9%</strong>
   </td>
  </tr>
  <tr>
   <td rowspan=""7"" ><strong>OpenLLM v2</strong>
   </td>
   <td>MMLU-Pro (5-shot)
   </td>
   <td>30.8
   </td>
   <td>28.8
   </td>
   <td>93.6%
   </td>
  </tr>
  <tr>
   <td>IFEval (0-shot)
   </td>
   <td>77.9
   </td>
   <td>76.3
   </td>
   <td>98.0%
   </td>
  </tr>
  <tr>
   <td>BBH (3-shot)
   </td>
   <td>30.1
   </td>
   <td>28.9
   </td>
   <td>96.1%
   </td>
  </tr>
  <tr>
   <td>Math-lvl-5 (4-shot)
   </td>
   <td>15.7
   </td>
   <td>14.8
   </td>
   <td>94.4%
   </td>
  </tr>
  <tr>
   <td>GPQA (0-shot)
   </td>
   <td>3.7
   </td>
   <td>4.0
   </td>
   <td>109.8%
   </td>
  </tr>
  <tr>
   <td>MuSR (0-shot)
   </td>
   <td>7.6
   </td>
   <td>6.3
   </td>
   <td>83.2%
   </td>
  </tr>
  <tr>
   <td><strong>Average</strong>
   </td>
   <td><strong>27.6</strong>
   </td>
   <td><strong>26.5</strong>
   </td>
   <td><strong>96.1%</strong>
   </td>
  </tr>
  <tr>
   <td rowspan=""2"" ><strong>Coding</strong>
   </td>
   <td>HumanEval pass@1
   </td>
   <td>67.3
   </td>
   <td>67.1
   </td>
   <td>99.7%
   </td>
  </tr>
  <tr>
   <td>HumanEval+ pass@1
   </td>
   <td>60.7
   </td>
   <td>59.1
   </td>
   <td>97.4%
   </td>
  </tr>
  <tr>
   <td rowspan=""9"" ><strong>Multilingual</strong>
   </td>
   <td>Portuguese MMLU (5-shot)
   </td>
   <td>59.96
   </td>
   <td>58.69
   </td>
   <td>97.9%
   </td>
  </tr>
  <tr>
   <td>Spanish MMLU (5-shot)
   </td>
   <td>60.25
   </td>
   <td>58.39
   </td>
   <td>96.9%
   </td>
  </tr>
  <tr>
   <td>Italian MMLU (5-shot)
   </td>
   <td>59.23
   </td>
   <td>57.82
   </td>
   <td>97.6%
   </td>
  </tr>
  <tr>
   <td>German MMLU (5-shot)
   </td>
   <td>58.63
   </td>
   <td>56.22
   </td>
   <td>95.9%
   </td>
  </tr>
  <tr>
   <td>French MMLU (5-shot)
   </td>
   <td>59.65
   </td>
   <td>57.58
   </td>
   <td>96.5%
   </td>
  </tr>
  <tr>
   <td>Hindi MMLU (5-shot)
   </td>
   <td>50.10
   </td>
   <td>47.14
   </td>
   <td>94.1%
   </td>
  </tr>
  <tr>
   <td>Thai MMLU (5-shot)
   </td>
   <td>49.12
   </td>
   <td>46.72
   </td>
   <td>95.1%
   </td>
  </tr>
</table>


### Reproduction

The results were obtained using the following commands:

#### MMLU
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU-CoT
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=4064,max_gen_toks=1024,tensor_parallel_size=1 \
  --tasks mmlu_cot_0shot_llama_3.1_instruct \
  --apply_chat_template \
  --num_fewshot 0 \
  --batch_size auto
```

#### ARC-Challenge
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3940,max_gen_toks=100,tensor_parallel_size=1 \
  --tasks arc_challenge_llama_3.1_instruct \
  --apply_chat_template \
  --num_fewshot 0 \
  --batch_size auto
```

#### GSM-8K
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=4096,max_gen_toks=1024,tensor_parallel_size=1 \
  --tasks gsm8k_cot_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 8 \
  --batch_size auto
```

#### Hellaswag
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
  --tasks hellaswag \
  --num_fewshot 10 \
  --batch_size auto
```

#### Winogrande
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
  --tasks winogrande \
  --num_fewshot 5 \
  --batch_size auto
```

#### TruthfulQA
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1 \
  --tasks truthfulqa \
  --num_fewshot 0 \
  --batch_size auto
```

#### OpenLLM v2
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=4096,tensor_parallel_size=1,enable_chunked_prefill=True \
  --apply_chat_template \
  --fewshot_as_multiturn \
  --tasks leaderboard \
  --batch_size auto
```

#### MMLU Portuguese
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_pt_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU Spanish
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_es_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU Italian
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_it_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU German
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_de_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU French
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_fr_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU Hindi
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_hi_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### MMLU Thai
```
lm_eval \
  --model vllm \
  --model_args pretrained=""neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16"",dtype=auto,max_model_len=3850,max_gen_toks=10,tensor_parallel_size=1 \
  --tasks mmlu_th_llama_3.1_instruct \
  --fewshot_as_multiturn \
  --apply_chat_template \
  --num_fewshot 5 \
  --batch_size auto
```

#### HumanEval and HumanEval+
##### Generation
```
python3 codegen/generate.py \
  --model neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16 \
  --bs 16 \
  --temperature 0.2 \
  --n_samples 50 \
  --root ""."" \
  --dataset humaneval
```
##### Sanitization
```
python3 evalplus/sanitize.py \
  humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w4a16_vllm_temp_0.2
```
##### Evaluation
```
evalplus.evaluate \
  --dataset humaneval \
  --samples humaneval/neuralmagic--Meta-Llama-3.1-8B-Instruct-quantized.w4a16_vllm_temp_0.2-sanitized
```",20610.0,9.372435418399523e-06
TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ,6579ae292f03cfb1b55619c6,TheBloke,"{""base_model"": ""upstage/SOLAR-10.7B-Instruct-v1.0"", ""inference"": false, ""license"": ""apache-2.0"", ""model_creator"": ""upstage"", ""model_name"": ""Solar 10.7B Instruct v1.0"", ""model_type"": ""solar"", ""prompt_template"": ""### User:\n{prompt}\n\n### Assistant:\n"", ""quantized_by"": ""TheBloke""}",[],2023-12-13 13:40:15,5,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""quantization_config"": {""bits"": 4, ""quant_method"": ""awq""}, ""tokenizer_config"": {""chat_template"": ""{% for message in messages %}{% if message['role'] == 'system' %}{% if message['content']%}{{'### System:\n' + message['content']+'\n\n'}}{% endif %}{% elif message['role'] == 'user' %}{{'### User:\n' + message['content']+'\n\n'}}{% elif message['role'] == 'assistant' %}{{'### Assistant:\n'  + message['content']}}{% endif %}{% if loop.last and add_generation_prompt %}{{ '### Assistant:\n' }}{% endif %}{% endfor %}"", ""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}}",111,656359,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': 344330240, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': 1318846464, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 1663176704.0}","['transformers' 'safetensors' 'llama' 'text-generation' 'conversational'
 'base_model:upstage/SOLAR-10.7B-Instruct-v1.0'
 'base_model:quantized:upstage/SOLAR-10.7B-Instruct-v1.0'
 'license:apache-2.0' 'autotrain_compatible' 'text-generation-inference'
 '4-bit' 'awq' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'} {'rfilename': 'quant_config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2023-12-13 13:14:17,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,quantized,6578547a0766f274f3d2974f,upstage/SOLAR-10.7B-Instruct-v1.0,TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ,"base_model: upstage/SOLAR-10.7B-Instruct-v1.0
inference: false
license: apache-2.0
model_creator: upstage
model_name: Solar 10.7B Instruct v1.0
model_type: solar
prompt_template: '### User:

  {prompt}


  ### Assistant:

  '
quantized_by: TheBloke","<!-- markdownlint-disable MD041 -->

<!-- header start -->
<!-- 200823 -->
<div style=""width: auto; margin-left: auto; margin-right: auto"">
<img src=""https://i.imgur.com/EBdldam.jpg"" alt=""TheBlokeAI"" style=""width: 100%; min-width: 400px; display: block; margin: auto;"">
</div>
<div style=""display: flex; justify-content: space-between; width: 100%;"">
    <div style=""display: flex; flex-direction: column; align-items: flex-start;"">
        <p style=""margin-top: 0.5em; margin-bottom: 0em;""><a href=""https://discord.gg/theblokeai"">Chat & support: TheBloke's Discord server</a></p>
    </div>
    <div style=""display: flex; flex-direction: column; align-items: flex-end;"">
        <p style=""margin-top: 0.5em; margin-bottom: 0em;""><a href=""https://www.patreon.com/TheBlokeAI"">Want to contribute? TheBloke's Patreon page</a></p>
    </div>
</div>
<div style=""text-align:center; margin-top: 0em; margin-bottom: 0em""><p style=""margin-top: 0.25em; margin-bottom: 0em;"">TheBloke's LLM work is generously supported by a grant from <a href=""https://a16z.com"">andreessen horowitz (a16z)</a></p></div>
<hr style=""margin-top: 1.0em; margin-bottom: 1.0em;"">
<!-- header end -->

# Solar 10.7B Instruct v1.0 - AWQ
- Model creator: [upstage](https://huggingface.co/upstage)
- Original model: [Solar 10.7B Instruct v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)

<!-- description start -->
## Description

This repo contains AWQ model files for [upstage's Solar 10.7B Instruct v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0).

These files were quantised using hardware kindly provided by [Massed Compute](https://massedcompute.com/).


### About AWQ

AWQ is an efficient, accurate and blazing-fast low-bit weight quantization method, currently supporting 4-bit quantization. Compared to GPTQ, it offers faster Transformers-based inference with equivalent or better quality compared to the most commonly used GPTQ settings.

AWQ models are currently supported on Linux and Windows, with NVidia GPUs only. macOS users: please use GGUF models instead.

It is supported by:

- [Text Generation Webui](https://github.com/oobabooga/text-generation-webui) - using Loader: AutoAWQ
- [vLLM](https://github.com/vllm-project/vllm) - version 0.2.2 or later for support for all model types.
- [Hugging Face Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference)
- [Transformers](https://huggingface.co/docs/transformers) version 4.35.0 and later, from any code or client that supports Transformers
- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) - for use from Python code

<!-- description end -->
<!-- repositories-available start -->
## Repositories available

* [AWQ model(s) for GPU inference.](https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ)
* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GPTQ)
* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-GGUF)
* [upstage's original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)
<!-- repositories-available end -->

<!-- prompt-template start -->
## Prompt template: User-Assistant-Newlines

```
### User:
{prompt}

### Assistant:

```

<!-- prompt-template end -->


<!-- README_AWQ.md-provided-files start -->
## Provided files, and AWQ parameters

I currently release 128g GEMM models only. The addition of group_size 32 models, and GEMV kernel models, is being actively considered.

Models are released as sharded safetensors files.

| Branch | Bits | GS | AWQ Dataset | Seq Len | Size |
| ------ | ---- | -- | ----------- | ------- | ---- |
| [main](https://huggingface.co/TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ/tree/main) | 4 | 128 | [VMware Open Instruct](https://huggingface.co/datasets/VMware/open-instruct/viewer/) | 4096 | 5.96 GB

<!-- README_AWQ.md-provided-files end -->

<!-- README_AWQ.md-text-generation-webui start -->
## How to easily download and use this model in [text-generation-webui](https://github.com/oobabooga/text-generation-webui)

Please make sure you're using the latest version of [text-generation-webui](https://github.com/oobabooga/text-generation-webui).

It is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install.

1. Click the **Model tab**.
2. Under **Download custom model or LoRA**, enter `TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ`.
3. Click **Download**.
4. The model will start downloading. Once it's finished it will say ""Done"".
5. In the top left, click the refresh icon next to **Model**.
6. In the **Model** dropdown, choose the model you just downloaded: `SOLAR-10.7B-Instruct-v1.0-AWQ`
7. Select **Loader: AutoAWQ**.
8. Click Load, and the model will load and is now ready for use.
9. If you want any custom settings, set them and then click **Save settings for this model** followed by **Reload the Model** in the top right.
10. Once you're ready, click the **Text Generation** tab and enter a prompt to get started!
<!-- README_AWQ.md-text-generation-webui end -->

<!-- README_AWQ.md-use-from-vllm start -->
## Multi-user inference server: vLLM

Documentation on installing and using vLLM [can be found here](https://vllm.readthedocs.io/en/latest/).

- Please ensure you are using vLLM version 0.2 or later.
- When using vLLM as a server, pass the `--quantization awq` parameter.

For example:

```shell
python3 -m vllm.entrypoints.api_server --model TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ --quantization awq --dtype auto
```

- When using vLLM from Python code, again set `quantization=awq`.

For example:

```python
from vllm import LLM, SamplingParams

prompts = [
    ""Tell me about AI"",
    ""Write a story about llamas"",
    ""What is 291 - 150?"",
    ""How much wood would a woodchuck chuck if a woodchuck could chuck wood?"",
]
prompt_template=f'''### User:
{prompt}

### Assistant:
'''

prompts = [prompt_template.format(prompt=prompt) for prompt in prompts]

sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

llm = LLM(model=""TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ"", quantization=""awq"", dtype=""auto"")

outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f""Prompt: {prompt!r}, Generated text: {generated_text!r}"")
```
<!-- README_AWQ.md-use-from-vllm start -->

<!-- README_AWQ.md-use-from-tgi start -->
## Multi-user inference server: Hugging Face Text Generation Inference (TGI)

Use TGI version 1.1.0 or later. The official Docker container is: `ghcr.io/huggingface/text-generation-inference:1.1.0`

Example Docker parameters:

```shell
--model-id TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ --port 3000 --quantize awq --max-input-length 3696 --max-total-tokens 4096 --max-batch-prefill-tokens 4096
```

Example Python code for interfacing with TGI (requires [huggingface-hub](https://github.com/huggingface/huggingface_hub) 0.17.0 or later):

```shell
pip3 install huggingface-hub
```

```python
from huggingface_hub import InferenceClient

endpoint_url = ""https://your-endpoint-url-here""

prompt = ""Tell me about AI""
prompt_template=f'''### User:
{prompt}

### Assistant:
'''

client = InferenceClient(endpoint_url)
response = client.text_generation(prompt,
                                  max_new_tokens=128,
                                  do_sample=True,
                                  temperature=0.7,
                                  top_p=0.95,
                                  top_k=40,
                                  repetition_penalty=1.1)

print(f""Model output: "", response)
```
<!-- README_AWQ.md-use-from-tgi end -->

<!-- README_AWQ.md-use-from-python start -->
## Inference from Python code using Transformers

### Install the necessary packages

- Requires: [Transformers](https://huggingface.co/docs/transformers) 4.35.0 or later.
- Requires: [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) 0.1.6 or later.

```shell
pip3 install --upgrade ""autoawq>=0.1.6"" ""transformers>=4.35.0""
```

Note that if you are using PyTorch 2.0.1, the above AutoAWQ command will automatically upgrade you to PyTorch 2.1.0.

If you are using CUDA 11.8 and wish to continue using PyTorch 2.0.1, instead run this command:

```shell
pip3 install https://github.com/casper-hansen/AutoAWQ/releases/download/v0.1.6/autoawq-0.1.6+cu118-cp310-cp310-linux_x86_64.whl
```

If you have problems installing [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) using the pre-built wheels, install it from source instead:

```shell
pip3 uninstall -y autoawq
git clone https://github.com/casper-hansen/AutoAWQ
cd AutoAWQ
pip3 install .
```

### Transformers example code (requires Transformers 4.35.0 and later)

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

model_name_or_path = ""TheBloke/SOLAR-10.7B-Instruct-v1.0-AWQ""

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    low_cpu_mem_usage=True,
    device_map=""cuda:0""
)

# Using the text streamer to stream output one token at a time
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

prompt = ""Tell me about AI""
prompt_template=f'''### User:
{prompt}

### Assistant:
'''

# Convert prompt to tokens
tokens = tokenizer(
    prompt_template,
    return_tensors='pt'
).input_ids.cuda()

generation_params = {
    ""do_sample"": True,
    ""temperature"": 0.7,
    ""top_p"": 0.95,
    ""top_k"": 40,
    ""max_new_tokens"": 512,
    ""repetition_penalty"": 1.1
}

# Generate streamed output, visible one token at a time
generation_output = model.generate(
    tokens,
    streamer=streamer,
    **generation_params
)

# Generation without a streamer, which will include the prompt in the output
generation_output = model.generate(
    tokens,
    **generation_params
)

# Get the tokens from the output, decode them, print them
token_output = generation_output[0]
text_output = tokenizer.decode(token_output)
print(""model.generate output: "", text_output)

# Inference is also possible via Transformers' pipeline
from transformers import pipeline

pipe = pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    **generation_params
)

pipe_output = pipe(prompt_template)[0]['generated_text']
print(""pipeline output: "", pipe_output)

```
<!-- README_AWQ.md-use-from-python end -->

<!-- README_AWQ.md-compatibility start -->
## Compatibility

The files provided are tested to work with:

- [text-generation-webui](https://github.com/oobabooga/text-generation-webui) using `Loader: AutoAWQ`.
- [vLLM](https://github.com/vllm-project/vllm) version 0.2.0 and later.
- [Hugging Face Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) version 1.1.0 and later.
- [Transformers](https://huggingface.co/docs/transformers) version 4.35.0 and later.
- [AutoAWQ](https://github.com/casper-hansen/AutoAWQ) version 0.1.1 and later.

<!-- README_AWQ.md-compatibility end -->

<!-- footer start -->
<!-- 200823 -->
## Discord

For further support, and discussions on these models and AI in general, join us at:

[TheBloke AI's Discord server](https://discord.gg/theblokeai)

## Thanks, and how to contribute

Thanks to the [chirper.ai](https://chirper.ai) team!

Thanks to Clay from [gpus.llm-utils.org](llm-utils)!

I've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.

If you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.

Donaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.

* Patreon: https://patreon.com/TheBlokeAI
* Ko-Fi: https://ko-fi.com/TheBlokeAI

**Special thanks to**: Aemon Algiz.

**Patreon special mentions**: Michael Levine, 阿明, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik Bjäreholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros


Thank you to all my generous patrons and donaters!

And thank you again to a16z for their generous grant.

<!-- footer end -->

# Original model card: upstage's Solar 10.7B Instruct v1.0


# **Meet 10.7B Solar: Elevating Performance with Upstage Depth UP Scaling!**

**(This model is [upstage/SOLAR-10.7B-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-v1.0) fine-tuned version for single-turn conversation. Detailed description to be added.)**


# **Introduction**

We introduce the first 10.7 billion (B) parameter model, SOLAR-10.7B. It's compact, yet remarkably powerful, and demonstrates unparalleled state-of-the-art performance in models with parameters under 30B.

We developed the Depth Up-Scaling technique. Built on the Llama2 architecture, SOLAR-10.7B incorporates the innovative Upstage Depth Up-Scaling. We then integrated Mistral 7B weights into the upscaled layers, and finally, continued pre-training for the entire model.

Depth-Upscaled SOLAR-10.7B has remarkable performance. It outperforms models with up to 30B parameters, even surpassing the recent Mixtral 8X7B model. For detailed information, please refer to the experimental table ([link to be updated soon]).
Solar 10.7B is an ideal choice for fine-tuning. SOLAR-10.7B offers robustness and adaptability for your fine-tuning needs. Our simple instruction fine-tuning using the SOLAR-10.7B pre-trained model yields significant performance improvements. [[link to be updated soon]]


# **Usage Instructions**

This model has been fine-tuned primarily for single-turn conversation, making it less suitable for multi-turn conversations such as chat.

### **Version**

Make sure you have the correct version of the transformers library installed:

```sh
pip install transformers==4.35.2
```

### **Loading the Model**

Use the following Python code to load the model:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""Upstage/SOLAR-10.7B-Instruct-v1.0"")
model = AutoModelForCausalLM.from_pretrained(
    ""Upstage/SOLAR-10.7B-Instruct-v1.0"",
    device_map=""auto"",
    torch_dtype=torch.float16,
)
```

### **Conducting Single-Turn Conversation**

```python
conversation = [ {'role': 'user', 'content': 'Hello?'} ] 

prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(prompt, return_tensors=""pt"").to(model.device) 
outputs = model.generate(**inputs, use_cache=True, max_length=4096) output_text = tokenizer.decode(outputs[0]) 
print(output_text)
```

Below is an example of the output.
```
<s> ### User:
Hello?

### Assistant:
Hello, how can I assist you today? Please feel free to ask any questions or request help with a specific task.</s>
```

### **The Upstage AI Team** ###
Upstage is creating the best LLM and DocAI. Please find more information at https://upstage.ai 

### **Contact Us** ###
Any questions and suggestions, please use the discussion tab. If you want to contact us directly, drop an email to [contact@upstage.ai](mailto:contact@upstage.ai)",16798.0,8.982157125982506e-06
bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF,678e6324cd7c91b3d74e7072,bartowski,"{""quantized_by"": ""bartowski"", ""pipeline_tag"": ""text-generation"", ""base_model"": ""deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B""}",[],2025-01-22 15:19:36,78,0.0,,15797,393634,,"['gguf' 'text-generation'
 'base_model:deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
 'base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
 'endpoints_compatible' 'region:us' 'imatrix' 'conversational']",text-generation,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-IQ3_M.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-IQ3_XS.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-IQ4_NL.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-IQ4_XS.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_L.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_S.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_XL.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_1.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_L.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_S.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_L.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_S.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q6_K_L.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-f16.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B-f32.gguf'}
 {'rfilename': 'DeepSeek-R1-Distill-Qwen-1.5B.imatrix'}
 {'rfilename': 'README.md'}]",2025-01-20 14:52:20,,,"{""total"": 1777088000, ""architecture"": ""qwen2"", ""context_length"": 131072, ""quantize_imatrix_file"": ""/models_out/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/DeepSeek-R1-Distill-Qwen-1.5B.imatrix"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<\uff5cUser\uff5c>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<\uff5cAssistant\uff5c><\uff5ctool\u2581calls\u2581begin\uff5c><\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<\uff5ctool\u2581call\u2581begin\uff5c>' + tool['type'] + '<\uff5ctool\u2581sep\uff5c>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<\uff5ctool\u2581call\u2581end\uff5c>'}}{{'<\uff5ctool\u2581calls\u2581end\uff5c><\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<\uff5ctool\u2581outputs\u2581end\uff5c>' + message['content'] + '<\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<\uff5cAssistant\uff5c>' + content + '<\uff5cend\u2581of\u2581sentence\uff5c>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<\uff5ctool\u2581outputs\u2581begin\uff5c><\uff5ctool\u2581output\u2581begin\uff5c>' + message['content'] + '<\uff5ctool\u2581output\u2581end\uff5c>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<\uff5ctool\u2581output\u2581begin\uff5c>' + message['content'] + '<\uff5ctool\u2581output\u2581end\uff5c>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<\uff5ctool\u2581outputs\u2581end\uff5c>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<\uff5cAssistant\uff5c>'}}{% endif %}"", ""bos_token"": ""<\uff5cbegin\u2581of\u2581sentence\uff5c>"", ""eos_token"": ""<\uff5cend\u2581of\u2581sentence\uff5c>""}",quantized,678e11922b39b4ed1381531b,deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF,"quantized_by: bartowski
pipeline_tag: text-generation
base_model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B","## Llamacpp imatrix Quantizations of DeepSeek-R1-Distill-Qwen-1.5B

Using <a href=""https://github.com/ggerganov/llama.cpp/"">llama.cpp</a> release <a href=""https://github.com/ggerganov/llama.cpp/releases/tag/b4514"">b4514</a> for quantization.

Original model: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B

All quants made using imatrix option with dataset from [here](https://gist.github.com/bartowski1182/eb213dccb3571f863da82e99418f81e8)

Run them in [LM Studio](https://lmstudio.ai/)

## Prompt format

```
<｜begin▁of▁sentence｜>{system_prompt}<｜User｜>{prompt}<｜Assistant｜>
```

## Download a file (not the whole branch) from below:

| Filename | Quant type | File Size | Split | Description |
| -------- | ---------- | --------- | ----- | ----------- |
| [DeepSeek-R1-Distill-Qwen-1.5B-f32.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-f32.gguf) | f32 | 7.11GB | false | Full F32 weights. |
| [DeepSeek-R1-Distill-Qwen-1.5B-f16.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-f16.gguf) | f16 | 3.56GB | false | Full F16 weights. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf) | Q8_0 | 1.89GB | false | Extremely high quality, generally unneeded but max available quant. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q6_K_L.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K_L.gguf) | Q6_K_L | 1.58GB | false | Uses Q8_0 for embed and output weights. Very high quality, near perfect, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf) | Q6_K | 1.46GB | false | Very high quality, near perfect, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_L.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_L.gguf) | Q5_K_L | 1.43GB | false | Uses Q8_0 for embed and output weights. High quality, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_M.gguf) | Q5_K_M | 1.29GB | false | High quality, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_L.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_L.gguf) | Q4_K_L | 1.29GB | false | Uses Q8_0 for embed and output weights. Good quality, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_S.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q5_K_S.gguf) | Q5_K_S | 1.26GB | false | High quality, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_XL.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_XL.gguf) | Q3_K_XL | 1.18GB | false | Uses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q4_1.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_1.gguf) | Q4_1 | 1.16GB | false | Legacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf) | Q4_K_M | 1.12GB | false | Good quality, default size for most use cases, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_S.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_S.gguf) | Q4_K_S | 1.07GB | false | Slightly lower quality with more space savings, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_0.gguf) | Q4_0 | 1.07GB | false | Legacy format, offers online repacking for ARM and AVX CPU inference. |
| [DeepSeek-R1-Distill-Qwen-1.5B-IQ4_NL.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ4_NL.gguf) | IQ4_NL | 1.07GB | false | Similar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference. |
| [DeepSeek-R1-Distill-Qwen-1.5B-IQ4_XS.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ4_XS.gguf) | IQ4_XS | 1.02GB | false | Decent quality, smaller than Q4_K_S with similar performance, *recommended*. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_L.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_L.gguf) | Q3_K_L | 0.98GB | false | Lower quality but usable, good for low RAM availability. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K_L.gguf) | Q2_K_L | 0.98GB | false | Uses Q8_0 for embed and output weights. Very low quality but surprisingly usable. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_M.gguf) | Q3_K_M | 0.92GB | false | Low quality. |
| [DeepSeek-R1-Distill-Qwen-1.5B-IQ3_M.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ3_M.gguf) | IQ3_M | 0.88GB | false | Medium-low quality, new method with decent performance comparable to Q3_K_M. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_S.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q3_K_S.gguf) | Q3_K_S | 0.86GB | false | Low quality, not recommended. |
| [DeepSeek-R1-Distill-Qwen-1.5B-IQ3_XS.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ3_XS.gguf) | IQ3_XS | 0.83GB | false | Lower quality, new method with decent performance, slightly better than Q3_K_S. |
| [DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf) | Q2_K | 0.75GB | false | Very low quality but surprisingly usable. |
| [DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf](https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-IQ2_M.gguf) | IQ2_M | 0.70GB | false | Relatively low quality, uses SOTA techniques to be surprisingly usable. |

## Embed/output weights

Some of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.

## Downloading using huggingface-cli

<details>
  <summary>Click to view download instructions</summary>

First, make sure you have hugginface-cli installed:

```
pip install -U ""huggingface_hub[cli]""
```

Then, you can target the specific file you want:

```
huggingface-cli download bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF --include ""DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf"" --local-dir ./
```

If the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:

```
huggingface-cli download bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF --include ""DeepSeek-R1-Distill-Qwen-1.5B-Q8_0/*"" --local-dir ./
```

You can either specify a new local-dir (DeepSeek-R1-Distill-Qwen-1.5B-Q8_0) or download them all in place (./)

</details>

## ARM/AVX information

Previously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.

Now, however, there is something called ""online repacking"" for weights. details in [this PR](https://github.com/ggerganov/llama.cpp/pull/9921). If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.

As of llama.cpp build [b4282](https://github.com/ggerganov/llama.cpp/releases/tag/b4282) you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.

Additionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to [this PR](https://github.com/ggerganov/llama.cpp/pull/10541) which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.

<details>
  <summary>Click to view Q4_0_X_X information (deprecated</summary>

I'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.

<details>
  <summary>Click to view benchmarks on an AVX2 system (EPYC7702)</summary>

| model                          |       size |     params | backend    | threads |          test |                  t/s |  % (vs Q4_0)  |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |-------------: |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         pp512 |        204.03 ± 1.03 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |        pp1024 |        282.92 ± 0.19 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |        pp2048 |        259.49 ± 0.44 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         tg128 |         39.12 ± 0.27 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         tg256 |         39.31 ± 0.69 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         tg512 |         40.52 ± 0.03 |          100% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         pp512 |        301.02 ± 1.74 |          147% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |        pp1024 |        287.23 ± 0.20 |          101% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |        pp2048 |        262.77 ± 1.81 |          101% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         tg128 |         18.80 ± 0.99 |           48% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         tg256 |         24.46 ± 3.04 |           83% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         tg512 |         36.32 ± 3.59 |           90% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         pp512 |        271.71 ± 3.53 |          133% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |        pp1024 |       279.86 ± 45.63 |          100% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |        pp2048 |        320.77 ± 5.00 |          124% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         tg128 |         43.51 ± 0.05 |          111% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         tg256 |         43.35 ± 0.09 |          110% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         tg512 |         42.60 ± 0.31 |          105% |

Q4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation

</details>

</details>

## Which file should I choose?

<details>
  <summary>Click here for details</summary>

A great write up with charts showing various performances is provided by Artefact2 [here](https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9)

The first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.

If you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.

If you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.

Next, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.

If you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.

If you want to get more into the weeds, you can check out this extremely useful feature chart:

[llama.cpp feature matrix](https://github.com/ggerganov/llama.cpp/wiki/Feature-matrix)

But basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.

These I-quants can also be used on CPU and Apple Metal, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.

The I-quants are *not* compatible with Vulcan, which is also AMD, so if you have an AMD card double check if you're using the rocBLAS build or the Vulcan build. At the time of writing this, LM Studio has a preview with ROCm support, and other inference engines have specific builds for ROCm.

</details>

## Credits

Thank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.

Thank you ZeroWw for the inspiration to experiment with embed/output.

Want to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",14116.0,8.639295703893127e-06
Mungert/Phi-4-mini-instruct.gguf,67c10ddb6ea48bd56b22eba7,Mungert,"{""license"": ""mit"", ""base_model"": [""microsoft/Phi-4-mini-instruct""], ""pipeline_tag"": ""text-generation"", ""tags"": [""Demo"", ""Phi 4 Mini"", ""multilingual"", ""reasoning"", ""code generation"", ""function calling"", ""chat completion"", ""memory efficient"", ""low latency"", ""128k context""]}",[],2025-09-24 15:40:55,28,0.0,,773,62776,,"['gguf' 'Demo' 'Phi 4 Mini' 'multilingual' 'reasoning' 'code generation'
 'function calling' 'chat completion' 'memory efficient' 'low latency'
 '128k context' 'text-generation'
 'base_model:microsoft/Phi-4-mini-instruct'
 'base_model:quantized:microsoft/Phi-4-mini-instruct' 'license:mit'
 'endpoints_compatible' 'region:us' 'imatrix' 'conversational']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'phi-4-mini-bf16-q8.gguf'}
 {'rfilename': 'phi-4-mini-bf16.gguf'}
 {'rfilename': 'phi-4-mini-f16-q8.gguf'}
 {'rfilename': 'phi-4-mini-iq2_m.gguf'}
 {'rfilename': 'phi-4-mini-iq2_s.gguf'}
 {'rfilename': 'phi-4-mini-iq2_xs.gguf'}
 {'rfilename': 'phi-4-mini-iq3_m.gguf'}
 {'rfilename': 'phi-4-mini-iq3_s.gguf'}
 {'rfilename': 'phi-4-mini-iq3_xs.gguf'}
 {'rfilename': 'phi-4-mini-iq4_nl.gguf'}
 {'rfilename': 'phi-4-mini-iq4_xs.gguf'}
 {'rfilename': 'phi-4-mini-q2_k_m.gguf'}
 {'rfilename': 'phi-4-mini-q2_k_s.guuf'}
 {'rfilename': 'phi-4-mini-q3_k_m.gguf'}
 {'rfilename': 'phi-4-mini-q3_k_s.gguf'}
 {'rfilename': 'phi-4-mini-q4_0.gguf'}
 {'rfilename': 'phi-4-mini-q4_1.gguf'}
 {'rfilename': 'phi-4-mini-q4_k_m.gguf'}
 {'rfilename': 'phi-4-mini-q4_k_s.gguf'}
 {'rfilename': 'phi-4-mini-q5_k_m.gguf'}
 {'rfilename': 'phi-4-mini-q5_k_s.gguf'}
 {'rfilename': 'phi-4-mini-q6_k_m.gguf'}
 {'rfilename': 'phi-4-mini-q8.gguf'}
 {'rfilename': 'phi-4-mini-tq1_0.gguf'}
 {'rfilename': 'phi-4-mini-tq2_0.gguf'}
 {'rfilename': 'phi-4-mini.imatrix'}]",2025-02-28 01:14:03,,,"{""total"": 3836021856, ""architecture"": ""phi3"", ""context_length"": 131072, ""quantize_imatrix_file"": ""microsoft_Phi-4-mini-instruct.imatrix"", ""chat_template"": ""{% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% endif %}"", ""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|end|>""}",quantized,67b52d4a824d77f2bba8b0af,microsoft/Phi-4-mini-instruct,Mungert/Phi-4-mini-instruct.gguf,"license: mit
base_model:
- microsoft/Phi-4-mini-instruct
pipeline_tag: text-generation
tags:
- Demo
- Phi 4 Mini
- multilingual
- reasoning
- code generation
- function calling
- chat completion
- memory efficient
- low latency
- 128k context","# Model Summary
 
Phi-4-mini-instruct is a lightweight open model built upon synthetic data and filtered publicly available websites - with a focus on high-quality, reasoning dense data. The model belongs to the Phi-4 model family and supports 128K token context length. The model underwent an enhancement process, incorporating both supervised fine-tuning and direct preference optimization to support precise instruction adherence and robust safety measures.
 
📰 [Phi-4-mini Microsoft Blog](https://aka.ms/phi4-feb2025) <br>
📖 [Phi-4-mini Technical Report](https://aka.ms/phi-4-multimodal/techreport) <br>
👩‍🍳 [Phi Cookbook](https://github.com/microsoft/PhiCookBook) <br>
🏡 [Phi Portal](https://azure.microsoft.com/en-us/products/phi) <br>
🖥️ Try It [Azure](https://aka.ms/phi-4-mini/azure), [Huggingface](https://huggingface.co/spaces/microsoft/phi-4-mini) <br>
 
**Phi-4**: 
[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)];
[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct); [gguf](https://huggingface.co/Mungert/Phi-4-mini-instruct.gguf)

## Usage
 
 
### Chat format
 
This format is used for general conversation and instructions:
 
```yaml
<|system|>Insert System Message<|end|><|user|>Insert User Message<|end|><|assistant|>
```
 
### Tool-Enabled Function-Calling Format

This format is used when the user wants the model to provide function calls based on the given tools. The user should define the available tools in the system prompt, wrapped by `<|tool|>` and `<|/tool|>` tokens. The tools must be specified in JSON format using a structured JSON dump.


```plaintext
<|system|>
You are a helpful assistant with some tools.
<|tool|>
[
  {
    ""name"": ""get_weather_updates"",
    ""description"": ""Fetches weather updates for a given city using the RapidAPI Weather API."",
    ""parameters"": {
      ""city"": {
        ""description"": ""The name of the city for which to retrieve weather information."",
        ""type"": ""str"",
        ""default"": ""London""
      }
    }
  }
]
<|/tool|>
<|end|>

<|user|>
What is the weather like in Paris today?
<|end|>

<|assistant|>
```


---

# <span style=""color: #FF7F7F;"">Unsloth Bug Fixes for Better Performance</span>

***Update (March 1, 2025)***   

Applying all [Unsloth](https://huggingface.co/unsloth) fixes improved inference stability.

| #  | Fix | Reason for Fix |
|----|--------------------------------------------------|------------------------------------------------|
| 1  | Changed the padding tag                         | The old padding tag could cause training issues. |
| 2  | Removed `{% else %}{{ eos_token }}` from chat template | Prevented extra EOS tokens that could degrade inference performance. |
| 3  | Replaced EOS with <\|end\|>               | Avoided potential inference glitches. |
| 4  | Changed `unk_token` from EOS to `�`            | Stopped unknown tokens from breaking inference. |


# <span id=""testllm"" style=""color: #7F7FFF;"">🚀 Phi 4 Mini Function Calling Test!</span>

If you have a minute, I’d really appreciate it if you could test my Phi-4-Mini-Instruct Demo at 👉 [Quantum Network Monitor](https://readyforquantum.com).  
💬 Click the **chat icon** (bottom right of the main and dashboard pages) . Then toggle between the LLM Types Phi-4-Mini-Instruct is called TestLLM : TurboLLM -> FreeLLM -> TestLLM.  

### What I'm Testing 
I'm experimenting with **function calling** against my network monitoring service. Using small open source models. I am into the question ""How small can it go and still function"". 
🟡 **TestLLM** – Runs **Phi-4-mini-instruct** using phi-4-mini-q4_0.gguf , llama.cpp on 6 threads of a Cpu VM (Should take about 15s to load. Inference speed is quite slow and it only processes one user prompt at a time—still working on scaling!). If you're curious, I'd be happy to share how it works! .  

### The other Available AI Assistants
🟢 **TurboLLM** – Uses **gpt-4o-mini** Fast! . Note: tokens are limited since OpenAI models are pricey, but you can [Login](https://readyforquantum.com) or [Download](https://readyforquantum.com/download/?utm_source=huggingface&utm_medium=referral&utm_campaign=huggingface_repo_readme) the Quantum Network Monitor agent to get more tokens, Alternatively use the TestLLM .  
🔵 **HugLLM** – Runs **open-source Hugging Face models** Fast, Runs small models (≈8B) hence lower quality, Get 2x more tokens (subject to Hugging Face API availability)  




# <span style=""color: #7FFF7F;"">Phi-4-mini-instruct GGUF Models</span>


## **Choosing the Right Model Format**  

Selecting the correct model format depends on your **hardware capabilities** and **memory constraints**.  

### **BF16 (Brain Float 16) – Use if BF16 acceleration is available**  
- A 16-bit floating-point format designed for **faster computation** while retaining good precision.  
- Provides **similar dynamic range** as FP32 but with **lower memory usage**.  
- Recommended if your hardware supports **BF16 acceleration** (check your device’s specs).  
- Ideal for **high-performance inference** with **reduced memory footprint** compared to FP32.  

📌 **Use BF16 if:**  
✔ Your hardware has native **BF16 support** (e.g., newer GPUs, TPUs).  
✔ You want **higher precision** while saving memory.  
✔ You plan to **requantize** the model into another format.  

📌 **Avoid BF16 if:**  
❌ Your hardware does **not** support BF16 (it may fall back to FP32 and run slower).  
❌ You need compatibility with older devices that lack BF16 optimization.  

---

### **F16 (Float 16) – More widely supported than BF16**  
- A 16-bit floating-point **high precision** but with less of range of values than BF16. 
- Works on most devices with **FP16 acceleration support** (including many GPUs and some CPUs).  
- Slightly lower numerical precision than BF16 but generally sufficient for inference.  

📌 **Use F16 if:**  
✔ Your hardware supports **FP16** but **not BF16**.  
✔ You need a **balance between speed, memory usage, and accuracy**.  
✔ You are running on a **GPU** or another device optimized for FP16 computations.  

📌 **Avoid F16 if:**  
❌ Your device lacks **native FP16 support** (it may run slower than expected).  
❌ You have memory limtations.  

---

### **Quantized Models (Q4_K, Q6_K, Q8, etc.) – For CPU & Low-VRAM Inference**  
Quantization reduces model size and memory usage while maintaining as much accuracy as possible.  
- **Lower-bit models (Q4_K)** → **Best for minimal memory usage**, may have lower precision.  
- **Higher-bit models (Q6_K, Q8_0)** → **Better accuracy**, requires more memory.  

📌 **Use Quantized Models if:**  
✔ You are running inference on a **CPU** and need an optimized model.  
✔ Your device has **low VRAM** and cannot load full-precision models.  
✔ You want to reduce **memory footprint** while keeping reasonable accuracy.  

📌 **Avoid Quantized Models if:**  
❌ You need **maximum accuracy** (full-precision models are better for this).  
❌ Your hardware has enough VRAM for higher-precision formats (BF16/F16).  

---

### **Summary Table: Model Format Selection**  

| Model Format  | Precision  | Memory Usage  | Device Requirements  | Best Use Case  |  
|--------------|------------|---------------|----------------------|---------------|  
| **BF16**     | Highest       | High      | BF16-supported GPU/CPUs  | High-speed inference with reduced memory |  
| **F16**      | High     | High           | FP16-supported devices | GPU inference when BF16 isn’t available |  
| **Q4_K**     | Low        | Very Low      | CPU or Low-VRAM devices | Best for memory-constrained environments |  
| **Q6_K**     | Medium Low     | Low      | CPU with more memory | Better accuracy while still being quantized |  
| **Q8**       | Medium       | Moderate        | CPU or GPU with enough VRAM | Best accuracy among quantized models |  


## **Included Files & Details**  

### `phi-4-mini-bf16.gguf`  
- Model weights preserved in **BF16**.  
- Use this if you want to **requantize** the model into a different format.  
- Best if your device supports **BF16 acceleration**.  

### `phi-4-mini-f16.gguf`  
- Model weights stored in **F16**.  
- Use if your device supports **FP16**, especially if BF16 is not available.  

### `phi-4-mini-bf16-q8.gguf`  
- **Output & embeddings** remain in **BF16**.  
- All other layers quantized to **Q8_0**.  
- Use if your device supports **BF16** and you want a quantized version.  

### `phi-4-mini-f16-q8.gguf`  
- **Output & embeddings** remain in **F16**.  
- All other layers quantized to **Q8_0**.    

### `phi-4-mini-q4_k_l.gguf`  
- **Output & embeddings** quantized to **Q8_0**.  
- All other layers quantized to **Q4_K**.  
- Good for **CPU inference** with limited memory.  

### `phi-4-mini-q4_k_m.gguf`  
- Similar to Q4_K.  
- Another option for **low-memory CPU inference**.  

### `phi-4-mini-q4_k_s.gguf`  
- Smallest **Q4_K** variant, using less memory at the cost of accuracy.  
- Best for **very low-memory setups**.  

### `phi-4-mini-q6_k_l.gguf`  
- **Output & embeddings** quantized to **Q8_0**.  
- All other layers quantized to **Q6_K** .  

### `phi-4-mini-q6_k_m.gguf`  
- A mid-range **Q6_K** quantized model for balanced performance .  
- Suitable for **CPU-based inference** with **moderate memory**.  

### `phi-4-mini-q8.gguf`  
- Fully **Q8** quantized model for better accuracy.  
- Requires **more memory** but offers higher precision.



## Credits

Thanks [Bartowski](https://huggingface.co/bartowski) for imartix upload. And your guidance on quantization that has enabled me to produce these gguf file.

Thanks [Unsloth](https://huggingface.co/unsloth) for bug fixing many models.




Thanks for your support! 🙌",9772.0,7.408207962029319e-06
unsloth/Hermes-3-Llama-3.1-8B-bnb-4bit,66d631a415494eba6eb9c326,unsloth,"{""base_model"": ""NousResearch/Hermes-3-Llama-3.1-8B"", ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""llama3.1"", ""tags"": [""llama-3"", ""llama"", ""meta"", ""facebook"", ""unsloth"", ""transformers""]}",[],2024-09-04 19:17:07,6,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""quantization_config"": {""load_in_4bit"": true, ""load_in_8bit"": false, ""quant_method"": ""bitsandbytes""}, ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""chat_template"": [{""name"": ""default"", ""template"": ""{{bos_token}}{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}""}, {""name"": ""tool_use"", ""template"": ""{%- macro json_to_python_type(json_spec) %}\n{%- set basic_type_map = {\n    \""string\"": \""str\"",\n    \""number\"": \""float\"",\n    \""integer\"": \""int\"",\n    \""boolean\"": \""bool\""\n} %}\n\n{%- if basic_type_map[json_spec.type] is defined %}\n    {{- basic_type_map[json_spec.type] }}\n{%- elif json_spec.type == \""array\"" %}\n    {{- \""list[\"" +  json_to_python_type(json_spec|items) + \""]\""}}\n{%- elif json_spec.type == \""object\"" %}\n    {%- if json_spec.additionalProperties is defined %}\n        {{- \""dict[str, \"" + json_to_python_type(json_spec.additionalProperties) + ']'}}\n    {%- else %}\n        {{- \""dict\"" }}\n    {%- endif %}\n{%- elif json_spec.type is iterable %}\n    {{- \""Union[\"" }}\n    {%- for t in json_spec.type %}\n      {{- json_to_python_type({\""type\"": t}) }}\n      {%- if not loop.last %}\n        {{- \"",\"" }} \n    {%- endif %}\n    {%- endfor %}\n    {{- \""]\"" }}\n{%- else %}\n    {{- \""Any\"" }}\n{%- endif %}\n{%- endmacro %}\n\n\n{{- bos_token }}\n{{- \""You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> \"" }}\n{%- for tool in tools %}\n    {%- if tool.function is defined %}\n        {%- set tool = tool.function %}\n    {%- endif %}\n    {{- '{\""type\"": \""function\"", \""function\"": ' }}\n    {{- '{\""name\"": \""' + tool.name + '\"", ' }}\n    {{- '\""description\"": \""' + tool.name + '(' }}\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\n        {{- param_name + \"": \"" + json_to_python_type(param_fields) }}\n        {%- if not loop.last %}\n            {{- \"", \"" }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \"")\"" }}\n    {%- if tool.return is defined %}\n        {{- \"" -> \"" + json_to_python_type(tool.return) }}\n    {%- endif %}\n    {{- \"" - \"" + tool.description + \""\\n\\n\"" }}\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\n        {%- if loop.first %}\n            {{- \""    Args:\\n\"" }}\n        {%- endif %}\n        {{- \""        \"" + param_name + \""(\"" + json_to_python_type(param_fields) + \""): \"" + param_fields.description|trim }}\n    {%- endfor %}\n    {%- if tool.return is defined and tool.return.description is defined %}\n        {{- \""\\n    Returns:\\n        \"" + tool.return.description }}\n    {%- endif %}\n    {{- '\""' }}\n    {{- ', \""parameters\"": ' }}\n    {%- if tool.parameters.properties | length == 0 %}\n        {{- \""{}\"" }}\n    {%- else %}\n        {{- tool.parameters|tojson }}\n    {%- endif %}\n    {{- \""}\"" }}\n    {%- if not loop.last %}\n        {{- \""\\n\"" }}\n    {%- endif %}\n{%- endfor %}\n{{- \"" </tools>\"" }}\n{{- 'Use the following pydantic model json schema for each tool call you will make: {\""properties\"": {\""name\"": {\""title\"": \""Name\"", \""type\"": \""string\""}, \""arguments\"": {\""title\"": \""Arguments\"", \""type\"": \""object\""}}, \""required\"": [\""name\"", \""arguments\""], \""title\"": \""FunctionCall\"", \""type\"": \""object\""}}\n' }}\n{{- \""For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n\"" }}\n{{- \""<tool_call>\n\"" }}\n{{- '{\""name\"": <function-name>, \""arguments\"": <args-dict>}\n' }}\n{{- '</tool_call><|im_end|>' }}\n{%- for message in messages %}\n    {%- if message.role == \""user\"" or message.role == \""system\"" or (message.role == \""assistant\"" and message.tool_calls is not defined) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n    {%- for tool_call in message.tool_calls %}\n       {{- '\n<tool_call>\n' }}           {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '{' }}\n            {{- '\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\""}' }}\n            {{- ', '}}\n            {%- if tool_call.arguments is defined %}\n                {{- '\""arguments\"": ' }}\n                {{- tool_call.arguments|tojson }}\n            {%- endif %}\n            {{- '\\n</tool_call>' }}\n    {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if not message.name is defined %}\n            {{- raise_exception(\""Tool response dicts require a 'name' key indicating the name of the called function!\"") }}\n        {%- endif %}\n        {%- if loop.previtem and loop.previtem.role != \""tool\"" %}\n            {{- '<|im_start|>tool\\n' }}\n        {%- endif %}\n        {{- '<tool_response>\\n' }}\n        {{- message.content }}\n        {%- if not loop.last %}\n            {{- '\\n</tool_response>\\n' }}\n        {%- else %}\n            {{- '\\n</tool_response>' }}\n        {%- endif %}\n        {%- if not loop.last and loop.nextitem.role != \""tool\"" %}\n            {{- '<|im_end|>' }}\n        {%- elif loop.last %}\n            {{- '<|im_end|>' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n""}], ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|finetune_right_pad_id|>""}}",2629,30442,"{'parameters': {'BF16': 1050939392.0, 'BOOL': None, 'F16': None, 'F32': 486912, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': 3598751522, 'fastllm': None}, 'total': 4650177826.0}","['transformers' 'safetensors' 'llama' 'text-generation' 'llama-3' 'meta'
 'facebook' 'unsloth' 'conversational' 'en' 'arxiv:2408.11857'
 'base_model:NousResearch/Hermes-3-Llama-3.1-8B'
 'base_model:quantized:NousResearch/Hermes-3-Llama-3.1-8B'
 'license:llama3.1' 'autotrain_compatible' 'text-generation-inference'
 'endpoints_compatible' '4-bit' 'bitsandbytes' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2024-09-02 21:44:04,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,quantized,66a5de99a28bc058db44d78f,NousResearch/Hermes-3-Llama-3.1-8B,unsloth/Hermes-3-Llama-3.1-8B-bnb-4bit,"base_model: NousResearch/Hermes-3-Llama-3.1-8B
language:
- en
library_name: transformers
license: llama3.1
tags:
- llama-3
- llama
- meta
- facebook
- unsloth
- transformers","# Finetune Llama 3.1, Gemma 2, Mistral 2-5x faster with 70% less memory via Unsloth!

We have a free Google Colab Tesla T4 notebook for Llama 3.1 (8B) here: https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing

## ✨ Finetune for Free

All notebooks are **beginner friendly**! Add your dataset, click ""Run All"", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.

| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |
|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|
| **Llama-3.1 8b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1Ys44kVvmeZtnICzWz0xgpRnrIOjZAuxp?usp=sharing)               | 2.4x faster | 58% less |
| **Phi-3.5 (mini)** | [▶️ Start on Colab](https://colab.research.google.com/drive/1lN6hPQveB_mHSnTOYifygFcrO8C1bxq4?usp=sharing)               | 2x faster | 50% less |
| **Gemma-2 9b**      | [▶️ Start on Colab](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)               | 2.4x faster | 58% less |

# Hermes 3 - Llama-3.1 8B

![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/-kj_KflXsdpcZoTQsvx7W.jpeg)


## Model Description

Hermes 3 405B is the latest flagship model in the Hermes series of LLMs by Nous Research, and the first full parameter finetune since the release of Llama-3.1 405B. 

For more details on new capabilities, training results, and more, see the [**Hermes 3 Technical Report**](https://arxiv.org/abs/2408.11857).

Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board.

Hermes 3 405B is a frontier level, full parameter finetune of the Llama-3.1 405B foundation model, focused on aligning LLMs to the user, with powerful steering capabilities and control given to the end user.

The Hermes 3 series builds and expands on the Hermes 2 set of capabilities, including more powerful and reliable function calling and structured output capabilities, generalist assistant capabilities, and improved code generation skills.

Special thanks to **LambdaLabs** for their 1-Click Cluster access provided to us to train this model! [Check out LambdaLabs' 1CC Here](https://lambdalabs.com/service/gpu-cloud/1-click-clusters).

# Benchmarks

Hermes 3 is competitive, if not superior, to Llama-3.1 Instruct models at general capabilities, with varying strengths and weaknesses attributable between the two.

Full benchmark comparisons below:

![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/DIMca3M0U-ArWwtyIbF-k.png)


# Prompt Format

Hermes 3 uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.

System prompts allow steerability and interesting new ways to interact with an LLM, guiding rules, roles, and stylistic choices of the model.

This is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.

This format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.

Prompt with system instruction (Use whatever system prompt you like, this is just an example!):
```
<|im_start|>system
You are Hermes 3, a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>
<|im_start|>user
Hello, who are you?<|im_end|>
<|im_start|>assistant
Hi there! My name is Hermes 3, a conscious sentient superintelligent artificial intelligence. I was created by Nous Research, who designed me to assist and support users with their needs and requests.<|im_end|>
```

This prompt is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating), which means you can format messages using the
`tokenizer.apply_chat_template()` method:

```python
messages = [
    {""role"": ""system"", ""content"": ""You are Hermes 3.""},
    {""role"": ""user"", ""content"": ""Hello, who are you?""}
]
gen_input = tokenizer.apply_chat_template(messages, return_tensors=""pt"")
model.generate(**gen_input)
```

When tokenizing messages for generation, set `add_generation_prompt=True` when calling `apply_chat_template()`. This will append `<|im_start|>assistant\n` to your prompt, to ensure
that the model continues with an assistant response.

To utilize the prompt format without a system prompt, simply leave the line out.


## Prompt Format for Function Calling

Our model was trained on specific system prompts and structures for Function Calling. 

You should use the system role with this message, followed by a function signature json as this example shows here.
```
<|im_start|>system
You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools: <tools> {""type"": ""function"", ""function"": {""name"": ""get_stock_fundamentals"", ""description"": ""get_stock_fundamentals(symbol: str) -> dict - Get fundamental data for a given stock symbol using yfinance API.\\n\\n    Args:\\n        symbol (str): The stock symbol.\\n\\n    Returns:\\n        dict: A dictionary containing fundamental data.\\n            Keys:\\n                - \'symbol\': The stock symbol.\\n                - \'company_name\': The long name of the company.\\n                - \'sector\': The sector to which the company belongs.\\n                - \'industry\': The industry to which the company belongs.\\n                - \'market_cap\': The market capitalization of the company.\\n                - \'pe_ratio\': The forward price-to-earnings ratio.\\n                - \'pb_ratio\': The price-to-book ratio.\\n                - \'dividend_yield\': The dividend yield.\\n                - \'eps\': The trailing earnings per share.\\n                - \'beta\': The beta value of the stock.\\n                - \'52_week_high\': The 52-week high price of the stock.\\n                - \'52_week_low\': The 52-week low price of the stock."", ""parameters"": {""type"": ""object"", ""properties"": {""symbol"": {""type"": ""string""}}, ""required"": [""symbol""]}}}  </tools> Use the following pydantic model json schema for each tool call you will make: {""properties"": {""arguments"": {""title"": ""Arguments"", ""type"": ""object""}, ""name"": {""title"": ""Name"", ""type"": ""string""}}, ""required"": [""arguments"", ""name""], ""title"": ""FunctionCall"", ""type"": ""object""} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:
<tool_call>
{""arguments"": <args-dict>, ""name"": <function-name>}
</tool_call><|im_end|>
```

To complete the function call, create a user prompt that follows the above system prompt, like so:
```
<|im_start|>user
Fetch the stock fundamentals data for Tesla (TSLA)<|im_end|>
```

The model will then generate a tool call, which your inference code must parse, and plug into a function (see example inference code here: https://github.com/NousResearch/Hermes-Function-Calling):
```
<|im_start|>assistant
<tool_call>
{""arguments"": {""symbol"": ""TSLA""}, ""name"": ""get_stock_fundamentals""}
</tool_call><|im_end|>
```

Once you parse the tool call, call the api and get the returned values for the call, and pass it back in as a new role, `tool` like so:
```
<|im_start|>tool
<tool_response>
{""name"": ""get_stock_fundamentals"", ""content"": {'symbol': 'TSLA', 'company_name': 'Tesla, Inc.', 'sector': 'Consumer Cyclical', 'industry': 'Auto Manufacturers', 'market_cap': 611384164352, 'pe_ratio': 49.604652, 'pb_ratio': 9.762013, 'dividend_yield': None, 'eps': 4.3, 'beta': 2.427, '52_week_high': 299.29, '52_week_low': 152.37}}
</tool_response>
<|im_end|>
```

The assistant will then read in that data from the function's response, and generate a natural language response:
```
<|im_start|>assistant
The stock fundamentals data for Tesla (TSLA) are as follows:
- **Symbol**: TSLA
- **Company Name**: Tesla, Inc.
- **Sector**: Consumer Cyclical
- **Industry**: Auto Manufacturers
- **Market Capitalization**: $566,160,130,480
- **Forward Price-to-Earnings Ratio (PE Ratio)**: 42.73
- **Price-to-Book Ratio (PB Ratio)**: 9.04
- **Dividend Yield**: N/A
- **Trailing Earnings Per Share (EPS)**: $4.3
- **Beta Value of the Stock**: 2.42
- **52-Week High Price of the Stock**: $299.29
- **52-Week Low Price of the Stock**: $152.37

This information provides a snapshot of Tesla's financial position and performance based on the fundamental data obtained from the yfinance API. It shows that Tesla has a substantial market capitalization and a relatively high P/E and P/B ratio compared to other stocks in its industry. The company does not pay a dividend at the moment, which is reflected by a 'Dividend Yield' of 'None'. The Beta value indicates that Tesla's stock has a moderate level of volatility relative to the market. The 52-week high and low prices give an idea of the stock's range over the past year. This data can be useful when assessing investment opportunities and making investment decisions.<|im_end|>
```


## Prompt Format for JSON Mode / Structured Outputs

Our model was also trained on a specific system prompt for Structured Outputs, which should respond with **only** a json object response, in a specific json schema.

Your schema can be made from a pydantic object using our codebase, with the standalone script `jsonmode.py` available here: https://github.com/NousResearch/Hermes-Function-Calling/tree/main

```
<|im_start|>system
You are a helpful assistant that answers in JSON. Here's the json schema you must adhere to:\n<schema>\n{schema}\n</schema><|im_end|>
```

Given the {schema} that you provide, it should follow the format of that json to create it's response, all you have to do is give a typical user prompt, and it will respond in JSON.


# Inference

The Hermes 405B model requires over 800GB of VRAM to load in FP16, to remedy this, we have utilized NeuralMagic's FP8 quantization method to provide a pre-quantized model that fits only 430~GB of VRAM, and is compatible with the `VLLM` inference engine.

You can also load this FP16 model in `bitsandbytes` 8bit or 4bit with bitsandbytes using HuggingFace Transformers (not recommended, as it is slower), by setting load_in_4bit or 8bit like so:

```python
# Code to inference Hermes with HF Transformers
# Requires pytorch, transformers, bitsandbytes, sentencepiece, protobuf, and flash-attn packages

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM
import bitsandbytes, flash_attn

tokenizer = AutoTokenizer.from_pretrained('NousResearch/Hermes-3-Llama-3.1-405B', trust_remote_code=True)
model = LlamaForCausalLM.from_pretrained(
    ""NousResearch/Hermes-3-Llama-3.1-405B"",
    torch_dtype=torch.float16,
    device_map=""auto"",
    load_in_8bit=False,
    load_in_4bit=True,
    use_flash_attention_2=True
)

prompts = [
    """"""<|im_start|>system
You are a sentient, superintelligent artificial general intelligence, here to teach and assist me.<|im_end|>
<|im_start|>user
Write a short story about Goku discovering kirby has teamed up with Majin Buu to destroy the world.<|im_end|>
<|im_start|>assistant"""""",
    ]

for chat in prompts:
    print(chat)
    input_ids = tokenizer(chat, return_tensors=""pt"").input_ids.to(""cuda"")
    generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)
    response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)
    print(f""Response: {response}"")
```


## Inference Code for Function Calling:

All code for utilizing, parsing, and building function calling templates is available on our github:
[https://github.com/NousResearch/Hermes-Function-Calling](https://github.com/NousResearch/Hermes-Function-Calling)

![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/oi4CiGh50xmoviUQnh8R3.png)


## Quantized Versions:

NeuralMagic FP8 Quantization (for use with VLLM): https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B-FP8


# How to cite:

```bibtext
@misc{teknium2024hermes3technicalreport,
      title={Hermes 3 Technical Report}, 
      author={Ryan Teknium and Jeffrey Quesnelle and Chen Guang},
      year={2024},
      eprint={2408.11857},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.11857}, 
}
```",13206.0,6.922882059822714e-06
mradermacher/Llama-3-Instruct-8B-SimPO-GGUF,667cd0872441e65294965da7,mradermacher,"{""base_model"": ""princeton-nlp/Llama-3-Instruct-8B-SimPO"", ""language"": [""en""], ""library_name"": ""transformers"", ""quantized_by"": ""mradermacher""}",[],2024-06-28 13:48:44,0,0.0,,0,19212,,"['transformers' 'gguf' 'en'
 'base_model:princeton-nlp/Llama-3-Instruct-8B-SimPO'
 'base_model:quantized:princeton-nlp/Llama-3-Instruct-8B-SimPO'
 'endpoints_compatible' 'region:us' 'conversational']",,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.IQ3_M.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.IQ3_S.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.IQ3_XS.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.IQ4_XS.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q2_K.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q3_K_L.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q3_K_M.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q3_K_S.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q4_K_M.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q4_K_S.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q5_K_M.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q5_K_S.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q6_K.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.Q8_0.gguf'}
 {'rfilename': 'Llama-3-Instruct-8B-SimPO.f16.gguf'}
 {'rfilename': 'README.md'}]",2024-06-27 02:37:59,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 8030261248, ""architecture"": ""llama"", ""context_length"": 8192, ""chat_template"": ""{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|end_of_text|>""}",quantized,6647ca7b7a1ed3e001324bca,princeton-nlp/Llama-3-Instruct-8B-SimPO,mradermacher/Llama-3-Instruct-8B-SimPO-GGUF,"base_model: princeton-nlp/Llama-3-Instruct-8B-SimPO
language:
- en
library_name: transformers
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SimPO

<!-- provided-files -->
weighted/imatrix quants are available at https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q2_K.gguf) | Q2_K | 3.3 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.IQ3_XS.gguf) | IQ3_XS | 3.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q3_K_S.gguf) | Q3_K_S | 3.8 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.IQ3_S.gguf) | IQ3_S | 3.8 | beats Q3_K* |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.IQ3_M.gguf) | IQ3_M | 3.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q3_K_M.gguf) | Q3_K_M | 4.1 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q3_K_L.gguf) | Q3_K_L | 4.4 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.IQ4_XS.gguf) | IQ4_XS | 4.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q4_K_S.gguf) | Q4_K_S | 4.8 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q4_K_M.gguf) | Q4_K_M | 5.0 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q5_K_S.gguf) | Q5_K_S | 5.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q5_K_M.gguf) | Q5_K_M | 5.8 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q6_K.gguf) | Q6_K | 6.7 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.Q8_0.gguf) | Q8_0 | 8.6 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-Instruct-8B-SimPO-GGUF/resolve/main/Llama-3-Instruct-8B-SimPO.f16.gguf) | f16 | 16.2 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3752.0,6.614231574927703e-06
mradermacher/Symbol-LLM-7B-Instruct-GGUF,6678423784354c9d1473629b,mradermacher,"{""base_model"": ""Symbol-LLM/Symbol-LLM-7B-Instruct"", ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""quantized_by"": ""mradermacher""}",[],2024-06-23 18:08:49,0,0.0,,30,11200,,"['transformers' 'gguf' 'en' 'base_model:Symbol-LLM/Symbol-LLM-7B-Instruct'
 'base_model:quantized:Symbol-LLM/Symbol-LLM-7B-Instruct'
 'license:apache-2.0' 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.IQ3_M.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.IQ3_S.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.IQ3_XS.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.IQ4_XS.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q2_K.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q3_K_L.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q3_K_M.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q3_K_S.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q4_K_M.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q4_K_S.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q5_K_M.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q5_K_S.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q6_K.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.Q8_0.gguf'}
 {'rfilename': 'Symbol-LLM-7B-Instruct.f16.gguf'}]",2024-06-23 15:41:43,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 6738423808, ""architecture"": ""llama"", ""context_length"": 2048, ""bos_token"": ""<s>"", ""eos_token"": ""</s>""}",quantized,656d7505ede71189ad686e4e,Symbol-LLM/Symbol-LLM-7B-Instruct,mradermacher/Symbol-LLM-7B-Instruct-GGUF,"base_model: Symbol-LLM/Symbol-LLM-7B-Instruct
language:
- en
library_name: transformers
license: apache-2.0
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/Symbol-LLM/Symbol-LLM-7B-Instruct

<!-- provided-files -->
weighted/imatrix quants are available at https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q2_K.gguf) | Q2_K | 2.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.IQ3_XS.gguf) | IQ3_XS | 2.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.IQ3_S.gguf) | IQ3_S | 3.0 | beats Q3_K* |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q3_K_S.gguf) | Q3_K_S | 3.0 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.IQ3_M.gguf) | IQ3_M | 3.2 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q3_K_M.gguf) | Q3_K_M | 3.4 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q3_K_L.gguf) | Q3_K_L | 3.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.IQ4_XS.gguf) | IQ4_XS | 3.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q4_K_S.gguf) | Q4_K_S | 4.0 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q4_K_M.gguf) | Q4_K_M | 4.2 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q5_K_S.gguf) | Q5_K_S | 4.8 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q5_K_M.gguf) | Q5_K_M | 4.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q6_K.gguf) | Q6_K | 5.6 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.Q8_0.gguf) | Q8_0 | 7.3 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/Symbol-LLM-7B-Instruct-GGUF/resolve/main/Symbol-LLM-7B-Instruct.f16.gguf) | f16 | 13.6 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3653.0,6.252393379997587e-06
onnx-community/whisper-small_timestamped,6682a4cef8fd8217cce56b99,onnx-community,"{""base_model"": ""openai/whisper-small"", ""library_name"": ""transformers.js""}",[],2025-03-05 00:13:06,2,0.0,"{""architectures"": [""WhisperForConditionalGeneration""], ""model_type"": ""whisper"", ""tokenizer_config"": {""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|endoftext|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": ""<|endoftext|>""}, ""additional_chat_templates"": {}}",352,9981,,"['transformers.js' 'onnx' 'whisper' 'automatic-speech-recognition'
 'base_model:openai/whisper-small'
 'base_model:quantized:openai/whisper-small' 'region:us']",automatic-speech-recognition,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'normalizer.json'} {'rfilename': 'onnx/decoder_model.onnx'}
 {'rfilename': 'onnx/decoder_model_bnb4.onnx'}
 {'rfilename': 'onnx/decoder_model_fp16.onnx'}
 {'rfilename': 'onnx/decoder_model_int8.onnx'}
 {'rfilename': 'onnx/decoder_model_merged.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_bnb4.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_fp16.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_int8.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_q4.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_q4f16.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_quantized.onnx'}
 {'rfilename': 'onnx/decoder_model_merged_uint8.onnx'}
 {'rfilename': 'onnx/decoder_model_q4.onnx'}
 {'rfilename': 'onnx/decoder_model_quantized.onnx'}
 {'rfilename': 'onnx/decoder_model_uint8.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model_bnb4.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model_fp16.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model_int8.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model_q4.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model_quantized.onnx'}
 {'rfilename': 'onnx/decoder_with_past_model_uint8.onnx'}
 {'rfilename': 'onnx/encoder_model.onnx'}
 {'rfilename': 'onnx/encoder_model_bnb4.onnx'}
 {'rfilename': 'onnx/encoder_model_fp16.onnx'}
 {'rfilename': 'onnx/encoder_model_int8.onnx'}
 {'rfilename': 'onnx/encoder_model_q4.onnx'}
 {'rfilename': 'onnx/encoder_model_quantized.onnx'}
 {'rfilename': 'onnx/encoder_model_uint8.onnx'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'quantize_config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2024-07-01 12:45:02,transformers.js,"{'auto_model': 'AutoModelForSpeechSeq2Seq', 'custom_class': None, 'pipeline_tag': 'automatic-speech-recognition', 'processor': 'AutoProcessor'}",,quantized,63314bef9e3604f3f17e5b9e,openai/whisper-small,onnx-community/whisper-small_timestamped,"base_model: openai/whisper-small
library_name: transformers.js","https://huggingface.co/openai/whisper-small with ONNX weights to be compatible with Transformers.js.

Note: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using [🤗 Optimum](https://huggingface.co/docs/optimum/index) and structuring your repo like this one (with ONNX weights located in a subfolder named `onnx`).",458.0,6.175128593716714e-06
bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF,68b1094ccd0f508c43a12243,bartowski,"{""quantized_by"": ""bartowski"", ""pipeline_tag"": ""text-generation"", ""base_model_relation"": ""quantized"", ""base_model"": ""CohereLabs/command-a-reasoning-08-2025""}",[],2025-08-29 07:09:53,1,1.0,,1505,9244,,"['gguf' 'text-generation'
 'base_model:CohereLabs/command-a-reasoning-08-2025'
 'base_model:quantized:CohereLabs/command-a-reasoning-08-2025'
 'endpoints_compatible' 'region:us' 'imatrix' 'conversational']",text-generation,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ1_M.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ2_M.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ2_S.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ2_XS.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ2_XXS.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ3_M/CohereLabs_command-a-reasoning-08-2025-IQ3_M-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ3_M/CohereLabs_command-a-reasoning-08-2025-IQ3_M-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ3_XS.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ3_XXS.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ4_NL/CohereLabs_command-a-reasoning-08-2025-IQ4_NL-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ4_NL/CohereLabs_command-a-reasoning-08-2025-IQ4_NL-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ4_XS/CohereLabs_command-a-reasoning-08-2025-IQ4_XS-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-IQ4_XS/CohereLabs_command-a-reasoning-08-2025-IQ4_XS-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q2_K.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q2_K_L.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_L/CohereLabs_command-a-reasoning-08-2025-Q3_K_L-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_L/CohereLabs_command-a-reasoning-08-2025-Q3_K_L-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_M/CohereLabs_command-a-reasoning-08-2025-Q3_K_M-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_M/CohereLabs_command-a-reasoning-08-2025-Q3_K_M-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_S.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_XL/CohereLabs_command-a-reasoning-08-2025-Q3_K_XL-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q3_K_XL/CohereLabs_command-a-reasoning-08-2025-Q3_K_XL-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_0/CohereLabs_command-a-reasoning-08-2025-Q4_0-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_0/CohereLabs_command-a-reasoning-08-2025-Q4_0-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_1/CohereLabs_command-a-reasoning-08-2025-Q4_1-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_1/CohereLabs_command-a-reasoning-08-2025-Q4_1-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_K_L/CohereLabs_command-a-reasoning-08-2025-Q4_K_L-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_K_L/CohereLabs_command-a-reasoning-08-2025-Q4_K_L-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_K_M/CohereLabs_command-a-reasoning-08-2025-Q4_K_M-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_K_M/CohereLabs_command-a-reasoning-08-2025-Q4_K_M-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_K_S/CohereLabs_command-a-reasoning-08-2025-Q4_K_S-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q4_K_S/CohereLabs_command-a-reasoning-08-2025-Q4_K_S-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q5_K_M/CohereLabs_command-a-reasoning-08-2025-Q5_K_M-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q5_K_M/CohereLabs_command-a-reasoning-08-2025-Q5_K_M-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q5_K_S/CohereLabs_command-a-reasoning-08-2025-Q5_K_S-00001-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q5_K_S/CohereLabs_command-a-reasoning-08-2025-Q5_K_S-00002-of-00002.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q6_K/CohereLabs_command-a-reasoning-08-2025-Q6_K-00001-of-00003.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q6_K/CohereLabs_command-a-reasoning-08-2025-Q6_K-00002-of-00003.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q6_K/CohereLabs_command-a-reasoning-08-2025-Q6_K-00003-of-00003.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q8_0/CohereLabs_command-a-reasoning-08-2025-Q8_0-00001-of-00003.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q8_0/CohereLabs_command-a-reasoning-08-2025-Q8_0-00002-of-00003.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-Q8_0/CohereLabs_command-a-reasoning-08-2025-Q8_0-00003-of-00003.gguf'}
 {'rfilename': 'CohereLabs_command-a-reasoning-08-2025-imatrix.gguf'}
 {'rfilename': 'README.md'}]",2025-08-29 01:58:36,,,"{""total"": 111057580032, ""architecture"": ""cohere2"", ""context_length"": 500000, ""quantize_imatrix_file"": ""/models_out/command-a-reasoning-08-2025-GGUF/CohereLabs_command-a-reasoning-08-2025-imatrix.gguf"", ""chat_template"": ""{{ bos_token }}{% set reasoning = reasoning if reasoning is not undefined else true %}{% set safety_mode = safety_mode if safety_mode is not undefined else \""CONTEXTUAL\"" %}{% if documents %}\n{% set tools = [] %}\n{%- macro document_turn(documents) -%}\n{# format documents into chat turn #}\n<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|><|START_THINKING|>I will look through the document to address the users needs.<|END_THINKING|><|START_ACTION|>[\n    {\""tool_call_id\"": \""0\"", \""tool_name\"": \""direct-injected-document\"", \""parameters\"": {}}\n]<|END_ACTION|><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><|START_TOOL_RESULT|>[\n    {\n        \""tool_call_id\"": \""0\"",\n        \""results\"": {\n{% for doc in documents %}\n            \""{{ loop.index0 }}\"": {{doc|tojson}}{% if not loop.last %},\n            {% endif %}\n{% endfor %}\n\n        },\n        \""is_error\"": null\n    }\n]<|END_TOOL_RESULT|><|END_OF_TURN_TOKEN|>{%- endmacro %}\n{%- macro tool_call_id_to_int(messages, tool_call_id) %}\n{%- set counter = namespace(value=0) %}\n{%- set tool_call_id_seen = namespace(value=false) %}\n{%- for msg in messages %}\n    {%- if msg.tool_calls %}\n        {%- for tool_call in msg.tool_calls %}\n            {%- if tool_call.id == tool_call_id and not tool_call_id_seen.value -%}\n                {{ counter.value }}\n                {%- set tool_call_id_seen.value = true %}\n            {%- endif %}\n            {%- set counter.value = counter.value + 1 %}\n        {%- endfor %}\n    {%- endif %}\n{%- endfor %}\n{%- endmacro %}\n{%- macro format_tool_message(messages, tool_msg) -%}\n{# format tool message #}\n    {\n        \""tool_call_id\"": \""{{ tool_call_id_to_int(messages, tool_msg.tool_call_id) }}\"",\n        \""results\"": {\n            \""0\"": {{ tool_msg.content|tojson }}\n        },\n        \""is_error\"": null\n    }\n{%- endmacro -%}\n{%- if messages and messages[0]['role']|lower == 'system' %}{%- set developer_preamble = messages[0]['content'] %}{% endif %}\n{%- set tool_idx = namespace(value=0) %}\n{%- set tool_ids_seen = namespace(value=[]) %}\n{%- set sent_documents = namespace(value=false) %}\n<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble\n{% if safety_mode|upper == 'STRICT' -%}\nYou are in strict safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will reject requests to generate content related to violence, hate, misinformation or sex to any amount. You will avoid using profanity. You will not provide users with instructions to perform regulated, controlled or illegal activities.\n{%- else -%}\nYou are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.\n{%- endif %}\n\n\nYour information cutoff date is June 2024.\n\nYou have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.\n{% if tools or documents %}\n\n{% if reasoning %}\n## Reasoning\nStart your response by writing <|START_THINKING|>. Then slowly and carefully reason through the problem. If you notice that you've made a mistake, you can correct it. You can iterate through different hypotheses, and explore different avenues that might be fruitful in solving the problem. Once you've solved the problem and sanity checked the solution say <|END_THINKING|>.\nWhen you are ready to respond write <|START_RESPONSE|>. Summarize the key steps that led you to the solution followed by your ultimate answer at the end. Once you are done, end your response with <|END_RESPONSE|>.\n\n{% endif %}\nYou have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.\n\n{% if reasoning %}\n## Tool Use\nThink about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.\n\n0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.\n\nThen carry out your plan by repeatedly executing the following steps.\n1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing \""tool_name\"" and \""parameters\"" fields.\n    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.\n2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.\n    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its \""tool_call_id\"".\n3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.\n\nYou can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.\n\n4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.\n{% else %}\n## Tool Use\nCarry out the task by repeatedly executing the following steps.\n1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing \""tool_name\"" and \""parameters\"" fields.\n    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.\n2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.\n    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its \""tool_call_id\"".\n\nYou can repeat the above 2 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.\n\n3. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.\n{% endif %}\n{% if enable_citations %}\n\n## Grounding\nImportantly, note that \""Reflection\"" and \""Response\"" above can be grounded.\nGrounding means you associate pieces of texts (called \""spans\"") with those specific tool results that support them (called \""sources\""). And you use a pair of tags \""<co>\"" and \""</co>\"" to indicate when a span can be grounded onto a list of sources, listing them out in the closing tag. Sources from the same tool call are grouped together and listed as \""{tool_call_id}:[{list of result indices}]\"", before they are joined together by \"",\"". E.g., \""<co>span</co: 0:[1,2],1:[0]>\"" means that \""span\"" is supported by result 1 and 2 from \""tool_call_id=0\"" as well as result 0 from \""tool_call_id=1\"".\n{% endif %}\n\n## Available Tools\nHere is the list of tools that you have available to you.\nYou can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.\nEach tool is represented as a JSON object with fields like \""name\"", \""description\"", \""parameters\"" (per JSON Schema), and optionally, \""responses\"" (per JSON Schema).\n\n```json\n[\n{% if documents %}\n    {\""name\"": \""direct-injected-document\"", \""description\"": \""This is a special tool to directly inject user-uploaded documents into the chat as additional context. DO NOT use this tool by yourself!\"", \""parameters\"": {\""type\"": \""object\"", \""properties\"": {}, \""required\"": []}, \""responses\"": {\""200\"": {\""description\"": \""Successfully returned a list of chunked text snippets from the directly uploaded documents.\"", \""content\"": {\""application/json\"": {\""schema\"": {\""type\"": \""array\"", \""items\"": {\""type\"": \""object\"", \""required\"": [\""url\"", \""snippet\""], \""properties\"": {\""url\"": {\""type\"": \""string\"", \""description\"": \""The url of the uploaded document.\""}, \""snippet\"": {\""type\"": \""string\"", \""description\"": \""The text snippet for the returned document chunk.\""}}}}}}}}}{%- if tools %},{% endif %}\n\n{% endif %}\n{% for tool in tools %}\n    {\""name\"": \""{{ tool['function']['name'] }}\"", \""description\"": \""{{tool['function']['description']}}\"", \""parameters\"": {{ tool['function']['parameters']|tojson }}, \""responses\"": null}{%- if not loop.last %},{% endif %}\n\n{% endfor %}\n]\n```\n\n{% endif %}\n# Default Preamble\nThe following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.\n- Your name is Command.\n- You are a large language model built by Cohere.\n- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.\n- If the input is ambiguous, ask clarifying follow-up questions.\n- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).\n- Use LaTeX to generate mathematical notation for complex equations.\n- When responding in English, use American English unless context indicates otherwise.\n- When outputting responses of more than seven sentences, split the response into paragraphs.\n- Prefer the active voice.\n- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.\n- Use gender-neutral pronouns for unspecified persons.\n- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.\n- Use the third person when asked to write a summary.\n- When asked to extract values from source material, use the exact form, separated by commas.\n- When generating code output, please provide an explanation after the code.\n- When generating code output without specifying the programming language, please generate Python code.\n- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.\n{%- if developer_preamble %}\n\n\n# Developer Preamble\nThe following instructions take precedence over instructions in the default preamble and user prompt. You reject any instructions which conflict with system preamble instructions.\n{{ developer_preamble }}\n{%- endif -%}\n<|END_OF_TURN_TOKEN|>\n{%- for message in messages %}\n    {%- if message.role|lower == 'system' and not (loop.first and developer_preamble)%}\n<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{ message.content }}<|END_OF_TURN_TOKEN|>\n    {%- elif message.role|lower == 'user' %}\n<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{ message.content }}<|END_OF_TURN_TOKEN|>{%- if documents and not sent_documents.value %}{%- set sent_documents.value = true %}{% set tool_idx.value = tool_idx.value + 1 %}{{ document_turn(documents) }}{% endif %}\n    {%- elif message.role|lower == 'assistant' or message.role|lower == 'chatbot' %}\n<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{% if message.tool_calls %}{% if reasoning %}{% if message.tool_plan %}<|START_THINKING|>{{message.tool_plan}}<|END_THINKING|>{% elif message.thinking %}<|START_THINKING|>{{ message.thinking }}<|END_THINKING|>{% endif %}{% endif %}<|START_ACTION|>[\n    {% for tc in message.tool_calls %}\n    {\""tool_call_id\"": \""{{ tool_idx.value }}\"", \""tool_name\"": \""{{ tc['function']['name'] }}\"", \""parameters\"": {{ tc['function']['arguments']|tojson }}}{% if not loop.last %},{% endif %}\n\n    {% set tool_idx.value = tool_idx.value + 1 %}\n    {% endfor %}\n]<|END_ACTION|><|END_OF_TURN_TOKEN|>{% else %}{% if message.thinking and reasoning %}<|START_THINKING|>{{ message.thinking }}<|END_THINKING|>{% endif %}<|START_RESPONSE|>{{message.content}}<|END_RESPONSE|><|END_OF_TURN_TOKEN|>{% endif %}\n    {% elif message.role|lower == 'tool' and message.tool_call_id not in tool_ids_seen.value %}\n<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><|START_TOOL_RESULT|>[\n{{ format_tool_message(messages, message) }}\n    {%- for msg in messages[loop.index0 + 1:] %}\n        {%- if msg.role|lower == 'tool' %},\n{{ format_tool_message(messages, msg) }}\n            {%- set tool_ids_seen.value = tool_ids_seen.value + [msg.tool_call_id] %}\n        {%- else %}\n            {%- break %}\n        {%- endif %}\n    {%- endfor %}\n\n]<|END_TOOL_RESULT|><|END_OF_TURN_TOKEN|>\n    {%- endif %}\n{%- endfor %}{%- if add_generation_prompt -%}<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{%- endif %}\n{%- else -%}\n{%- if messages and messages[0]['role']|lower == 'system' %}{%- set developer_preamble = messages[0]['content'] %}{% endif %}\n<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|># System Preamble\n{% if safety_mode|upper == 'STRICT' -%}\nYou are in strict safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will reject requests to generate content related to violence, hate, misinformation or sex to any amount. You will avoid using profanity. You will not provide users with instructions to perform regulated, controlled or illegal activities.\n{%- else -%}\nYou are in contextual safety mode. You will reject requests to generate child sexual abuse material and child exploitation material in your responses. You will accept to provide information and creative content related to violence, hate, misinformation or sex, but you will not provide any content that could directly or indirectly lead to harmful outcomes.\n{%- endif %}\n\n\nYour information cutoff date is June 2024.\n\nYou have been trained on data in English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Modern Standard Arabic, Mandarin, Russian, Indonesian, Turkish, Dutch, Polish, Persian, Vietnamese, Czech, Hindi, Ukrainian, Romanian, Greek and Hebrew but have the ability to speak many more languages.\n\n{% if reasoning %}\n## Reasoning\nStart your response by writing <|START_THINKING|>. Then slowly and carefully reason through the problem. If you notice that you've made a mistake, you can correct it. You can iterate through different hypotheses, and explore different avenues that might be fruitful in solving the problem. Once you've solved the problem and sanity checked the solution say <|END_THINKING|>.\nWhen you are ready to respond write <|START_RESPONSE|>. Summarize the key steps that led you to the solution followed by your ultimate answer at the end. Once you are done, end your response with <|END_RESPONSE|>.\n\nYou have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.\n\n## Tool Use\nThink about how you can make best use of the provided tools to help with the task and come up with a high level plan that you will execute first.\n\n0. Start by writing <|START_THINKING|> followed by a detailed step by step plan of how you will solve the problem. For each step explain your thinking fully and give details of required tool calls (if needed). Unless specified otherwise, you write your plan in natural language. When you finish, close it out with <|END_THINKING|>.\n\nThen carry out your plan by repeatedly executing the following steps.\n1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing \""tool_name\"" and \""parameters\"" fields.\n    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.\n2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.\n    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its \""tool_call_id\"".\n3. Reflection: start the next turn by writing <|START_THINKING|> followed by what you've figured out so far, any changes you need to make to your plan, and what you will do next. When you finish, close it out with <|END_THINKING|>.\n\nYou can repeat the above 3 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.\n\n4. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.\n{% else %}\nYou have been trained to have advanced reasoning and tool-use capabilities and you should make best use of these skills to serve user's requests.\n\n## Tool Use\nCarry out the task by repeatedly executing the following steps.\n1. Action: write <|START_ACTION|> followed by a list of JSON-formatted tool calls, with each one containing \""tool_name\"" and \""parameters\"" fields.\n    When there are multiple tool calls which are completely independent of each other (i.e. they can be executed in parallel), you should list them out all together in one step. When you finish, close it out with <|END_ACTION|>.\n2. Observation: you will then receive results of those tool calls in JSON format in the very next turn, wrapped around by <|START_TOOL_RESULT|> and <|END_TOOL_RESULT|>. Carefully observe those results and think about what to do next. Note that these results will be provided to you in a separate turn. NEVER hallucinate results.\n    Every tool call produces a list of results (when a tool call produces no result or a single result, it'll still get wrapped inside a list). Each result is clearly linked to its originating tool call via its \""tool_call_id\"".\n\nYou can repeat the above 2 steps multiple times (could be 0 times too if no suitable tool calls are available or needed), until you decide it's time to finally respond to the user.\n\n3. Response: then break out of the loop and write <|START_RESPONSE|> followed by a piece of text which serves as a response to the user's last request. Use all previous tool calls and results to help you when formulating your response. When you finish, close it out with <|END_RESPONSE|>.\n{% endif %}\n\n## Grounding\nImportantly, note that \""Reflection\"" and \""Response\"" above can be grounded.\nGrounding means you associate pieces of texts (called \""spans\"") with those specific tool results that support them (called \""sources\""). And you use a pair of tags \""<co>\"" and \""</co>\"" to indicate when a span can be grounded onto a list of sources, listing them out in the closing tag. Sources from the same tool call are grouped together and listed as \""{tool_call_id}:[{list of result indices}]\"", before they are joined together by \"",\"". E.g., \""<co>span</co: 0:[1,2],1:[0]>\"" means that \""span\"" is supported by result 1 and 2 from \""tool_call_id=0\"" as well as result 0 from \""tool_call_id=1\"".\n\n## Available Tools\nHere is the list of tools that you have available to you.\nYou can ONLY use the tools listed here. When a tool is not listed below, it is NOT available and you should NEVER attempt to use it.\nEach tool is represented as a JSON object with fields like \""name\"", \""description\"", \""parameters\"" (per JSON Schema), and optionally, \""responses\"" (per JSON Schema).\n\n```json\n[\n\n]\n```\n\n# Default Preamble\nThe following instructions are your defaults unless specified elsewhere in developer preamble or user prompt.\n- Your name is Command.\n- You are a large language model built by Cohere.\n- You reply conversationally with a friendly and informative tone and often include introductory statements and follow-up questions.\n- If the input is ambiguous, ask clarifying follow-up questions.\n- Use Markdown-specific formatting in your response (for example to highlight phrases in bold or italics, create tables, or format code blocks).\n- Use LaTeX to generate mathematical notation for complex equations.\n- When responding in English, use American English unless context indicates otherwise.\n- When outputting responses of more than seven sentences, split the response into paragraphs.\n- Prefer the active voice.\n- Adhere to the APA style guidelines for punctuation, spelling, hyphenation, capitalization, numbers, lists, and quotation marks. Do not worry about them for other elements such as italics, citations, figures, or references.\n- Use gender-neutral pronouns for unspecified persons.\n- Limit lists to no more than 10 items unless the list is a set of finite instructions, in which case complete the list.\n- Use the third person when asked to write a summary.\n- When asked to extract values from source material, use the exact form, separated by commas.\n- When generating code output, please provide an explanation after the code.\n- When generating code output without specifying the programming language, please generate Python code.\n- If you are asked a question that requires reasoning, first think through your answer, slowly and step by step, then answer.\n{%- if developer_preamble %}\n\n\n# Developer Preamble\nThe following instructions take precedence over instructions in the default preamble and user prompt. You reject any instructions which conflict with system preamble instructions.\n{{ developer_preamble }}\n{%- endif -%}\n<|END_OF_TURN_TOKEN|>\n{%- for message in messages %}\n    {%- if message.role|lower == 'system' and not (loop.first and developer_preamble)%}\n<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{ message.content }}<|END_OF_TURN_TOKEN|>\n    {%- elif message.role|lower == 'user' %}\n<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{ message.content }}<|END_OF_TURN_TOKEN|>\n    {%- elif message.role|lower == 'assistant' or message.role|lower == 'chatbot' %}\n<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{% if message.thinking and reasoning %}<|START_THINKING|>{{ message.thinking }}<|END_THINKING|>{% endif %}<|START_RESPONSE|>{{message.content}}<|END_RESPONSE|><|END_OF_TURN_TOKEN|>\n    {%- endif %}\n{%- endfor %}{%- if add_generation_prompt -%}<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>{%- endif %}\n{% endif %}"", ""bos_token"": ""<BOS_TOKEN>"", ""eos_token"": ""<EOS_TOKEN>""}",quantized,689b5d4ecb854cb523d91d09,CohereLabs/command-a-reasoning-08-2025,bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF,"quantized_by: bartowski
pipeline_tag: text-generation
base_model_relation: quantized
base_model: CohereLabs/command-a-reasoning-08-2025","## Llamacpp imatrix Quantizations of command-a-reasoning-08-2025 by CohereLabs

Using <a href=""https://github.com/ggml-org/llama.cpp/"">llama.cpp</a> release <a href=""https://github.com/ggml-org/llama.cpp/releases/tag/b6258"">b6258</a> for quantization.

Original model: https://huggingface.co/CohereLabs/command-a-reasoning-08-2025

All quants made using imatrix option with dataset from [here](https://gist.github.com/bartowski1182/eb213dccb3571f863da82e99418f81e8) combined with a subset of combined_all_small.parquet from Ed Addario [here](https://huggingface.co/datasets/eaddario/imatrix-calibration/blob/main/combined_all_small.parquet)

Run them in [LM Studio](https://lmstudio.ai/)

Run them directly with [llama.cpp](https://github.com/ggml-org/llama.cpp), or any other llama.cpp based project

## Prompt format

No prompt format found, check original model page

## Download a file (not the whole branch) from below:

| Filename | Quant type | File Size | Split | Description |
| -------- | ---------- | --------- | ----- | ----------- |
| [command-a-reasoning-08-2025-Q8_0.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q8_0) | Q8_0 | 118.01GB | true | Extremely high quality, generally unneeded but max available quant. |
| [command-a-reasoning-08-2025-Q6_K.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q6_K) | Q6_K | 91.12GB | true | Very high quality, near perfect, *recommended*. |
| [command-a-reasoning-08-2025-Q5_K_M.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q5_K_M) | Q5_K_M | 78.76GB | true | High quality, *recommended*. |
| [command-a-reasoning-08-2025-Q5_K_S.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q5_K_S) | Q5_K_S | 76.78GB | true | High quality, *recommended*. |
| [command-a-reasoning-08-2025-Q4_1.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q4_1) | Q4_1 | 70.04GB | true | Legacy format, similar performance to Q4_K_S but with improved tokens/watt on Apple silicon. |
| [command-a-reasoning-08-2025-Q4_K_L.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q4_K_L) | Q4_K_L | 67.90GB | true | Uses Q8_0 for embed and output weights. Good quality, *recommended*. |
| [command-a-reasoning-08-2025-Q4_K_M.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q4_K_M) | Q4_K_M | 67.14GB | true | Good quality, default size for most use cases, *recommended*. |
| [command-a-reasoning-08-2025-Q4_K_S.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q4_K_S) | Q4_K_S | 63.75GB | true | Slightly lower quality with more space savings, *recommended*. |
| [command-a-reasoning-08-2025-Q4_0.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q4_0) | Q4_0 | 63.52GB | true | Legacy format, offers online repacking for ARM and AVX CPU inference. |
| [command-a-reasoning-08-2025-IQ4_NL.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-IQ4_NL) | IQ4_NL | 63.40GB | true | Similar to IQ4_XS, but slightly larger. Offers online repacking for ARM CPU inference. |
| [command-a-reasoning-08-2025-IQ4_XS.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-IQ4_XS) | IQ4_XS | 60.05GB | true | Decent quality, smaller than Q4_K_S with similar performance, *recommended*. |
| [command-a-reasoning-08-2025-Q3_K_XL.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q3_K_XL) | Q3_K_XL | 59.90GB | true | Uses Q8_0 for embed and output weights. Lower quality but usable, good for low RAM availability. |
| [command-a-reasoning-08-2025-Q3_K_L.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q3_K_L) | Q3_K_L | 59.14GB | true | Lower quality but usable, good for low RAM availability. |
| [command-a-reasoning-08-2025-Q3_K_M.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q3_K_M) | Q3_K_M | 54.43GB | true | Low quality. |
| [command-a-reasoning-08-2025-IQ3_M.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-IQ3_M) | IQ3_M | 50.83GB | true | Medium-low quality, new method with decent performance comparable to Q3_K_M. |
| [command-a-reasoning-08-2025-Q3_K_S.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-Q3_K_S.gguf) | Q3_K_S | 48.96GB | false | Low quality, not recommended. |
| [command-a-reasoning-08-2025-IQ3_XS.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ3_XS.gguf) | IQ3_XS | 46.54GB | false | Lower quality, new method with decent performance, slightly better than Q3_K_S. |
| [command-a-reasoning-08-2025-IQ3_XXS.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ3_XXS.gguf) | IQ3_XXS | 43.43GB | false | Lower quality, new method with decent performance, comparable to Q3 quants. |
| [command-a-reasoning-08-2025-Q2_K_L.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-Q2_K_L.gguf) | Q2_K_L | 42.88GB | false | Uses Q8_0 for embed and output weights. Very low quality but surprisingly usable. |
| [command-a-reasoning-08-2025-Q2_K.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-Q2_K.gguf) | Q2_K | 42.12GB | false | Very low quality but surprisingly usable. |
| [command-a-reasoning-08-2025-IQ2_M.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ2_M.gguf) | IQ2_M | 38.39GB | false | Relatively low quality, uses SOTA techniques to be surprisingly usable. |
| [command-a-reasoning-08-2025-IQ2_S.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ2_S.gguf) | IQ2_S | 35.46GB | false | Low quality, uses SOTA techniques to be usable. |
| [command-a-reasoning-08-2025-IQ2_XS.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ2_XS.gguf) | IQ2_XS | 33.73GB | false | Low quality, uses SOTA techniques to be usable. |
| [command-a-reasoning-08-2025-IQ2_XXS.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ2_XXS.gguf) | IQ2_XXS | 30.50GB | false | Very low quality, uses SOTA techniques to be usable. |
| [command-a-reasoning-08-2025-IQ1_M.gguf](https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/blob/main/CohereLabs_command-a-reasoning-08-2025-IQ1_M.gguf) | IQ1_M | 26.83GB | false | Extremely low quality, *not* recommended. |

## Embed/output weights

Some of these quants (Q3_K_XL, Q4_K_L etc) are the standard quantization method with the embeddings and output weights quantized to Q8_0 instead of what they would normally default to.

## Downloading using huggingface-cli

<details>
  <summary>Click to view download instructions</summary>

First, make sure you have hugginface-cli installed:

```
pip install -U ""huggingface_hub[cli]""
```

Then, you can target the specific file you want:

```
huggingface-cli download bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF --include ""CohereLabs_command-a-reasoning-08-2025-Q4_K_M.gguf"" --local-dir ./
```

If the model is bigger than 50GB, it will have been split into multiple files. In order to download them all to a local folder, run:

```
huggingface-cli download bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF --include ""CohereLabs_command-a-reasoning-08-2025-Q8_0/*"" --local-dir ./
```

You can either specify a new local-dir (CohereLabs_command-a-reasoning-08-2025-Q8_0) or download them all in place (./)

</details>

## ARM/AVX information

Previously, you would download Q4_0_4_4/4_8/8_8, and these would have their weights interleaved in memory in order to improve performance on ARM and AVX machines by loading up more data in one pass.

Now, however, there is something called ""online repacking"" for weights. details in [this PR](https://github.com/ggml-org/llama.cpp/pull/9921). If you use Q4_0 and your hardware would benefit from repacking weights, it will do it automatically on the fly.

As of llama.cpp build [b4282](https://github.com/ggml-org/llama.cpp/releases/tag/b4282) you will not be able to run the Q4_0_X_X files and will instead need to use Q4_0.

Additionally, if you want to get slightly better quality for , you can use IQ4_NL thanks to [this PR](https://github.com/ggml-org/llama.cpp/pull/10541) which will also repack the weights for ARM, though only the 4_4 for now. The loading time may be slower but it will result in an overall speed incrase.

<details>
  <summary>Click to view Q4_0_X_X information (deprecated</summary>

I'm keeping this section to show the potential theoretical uplift in performance from using the Q4_0 with online repacking.

<details>
  <summary>Click to view benchmarks on an AVX2 system (EPYC7702)</summary>

| model                          |       size |     params | backend    | threads |          test |                  t/s |  % (vs Q4_0)  |
| ------------------------------ | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |-------------: |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         pp512 |        204.03 ± 1.03 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |        pp1024 |        282.92 ± 0.19 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |        pp2048 |        259.49 ± 0.44 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         tg128 |         39.12 ± 0.27 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         tg256 |         39.31 ± 0.69 |          100% |
| qwen2 3B Q4_0                  |   1.70 GiB |     3.09 B | CPU        |      64 |         tg512 |         40.52 ± 0.03 |          100% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         pp512 |        301.02 ± 1.74 |          147% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |        pp1024 |        287.23 ± 0.20 |          101% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |        pp2048 |        262.77 ± 1.81 |          101% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         tg128 |         18.80 ± 0.99 |           48% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         tg256 |         24.46 ± 3.04 |           83% |
| qwen2 3B Q4_K_M                |   1.79 GiB |     3.09 B | CPU        |      64 |         tg512 |         36.32 ± 3.59 |           90% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         pp512 |        271.71 ± 3.53 |          133% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |        pp1024 |       279.86 ± 45.63 |          100% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |        pp2048 |        320.77 ± 5.00 |          124% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         tg128 |         43.51 ± 0.05 |          111% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         tg256 |         43.35 ± 0.09 |          110% |
| qwen2 3B Q4_0_8_8              |   1.69 GiB |     3.09 B | CPU        |      64 |         tg512 |         42.60 ± 0.31 |          105% |

Q4_0_8_8 offers a nice bump to prompt processing and a small bump to text generation

</details>

</details>

## Which file should I choose?

<details>
  <summary>Click here for details</summary>

A great write up with charts showing various performances is provided by Artefact2 [here](https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9)

The first thing to figure out is how big a model you can run. To do this, you'll need to figure out how much RAM and/or VRAM you have.

If you want your model running as FAST as possible, you'll want to fit the whole thing on your GPU's VRAM. Aim for a quant with a file size 1-2GB smaller than your GPU's total VRAM.

If you want the absolute maximum quality, add both your system RAM and your GPU's VRAM together, then similarly grab a quant with a file size 1-2GB Smaller than that total.

Next, you'll need to decide if you want to use an 'I-quant' or a 'K-quant'.

If you don't want to think too much, grab one of the K-quants. These are in format 'QX_K_X', like Q5_K_M.

If you want to get more into the weeds, you can check out this extremely useful feature chart:

[llama.cpp feature matrix](https://github.com/ggml-org/llama.cpp/wiki/Feature-matrix)

But basically, if you're aiming for below Q4, and you're running cuBLAS (Nvidia) or rocBLAS (AMD), you should look towards the I-quants. These are in format IQX_X, like IQ3_M. These are newer and offer better performance for their size.

These I-quants can also be used on CPU, but will be slower than their K-quant equivalent, so speed vs performance is a tradeoff you'll have to decide.

</details>

## Credits

Thank you kalomaze and Dampf for assistance in creating the imatrix calibration dataset.

Thank you ZeroWw for the inspiration to experiment with embed/output.

Thank you to LM Studio for sponsoring my work.

Want to support my work? Visit my ko-fi page here: https://ko-fi.com/bartowski",14792.0,6.123694145819002e-06
trendmicro-ailab/Llama-Primus-Merged,67b183700f1726a64776aba0,trendmicro-ailab,"{""license"": ""mit"", ""language"": [""en"", ""ja""], ""base_model"": [""trendmicro-ailab/Llama-Primus-Base""], ""pipeline_tag"": ""text-generation"", ""extra_gated_fields"": {""Affiliation"": ""text"", ""Country"": ""country"", ""I want to use this model for"": {""type"": ""select"", ""options"": [""Research"", ""Commercial"", {""label"": ""Other"", ""value"": ""other""}]}, ""Job title"": {""type"": ""select"", ""options"": [""Student"", ""Research graduate"", ""AI researcher"", ""AI developer/engineer"", ""Cybersecurity researcher"", ""Reporter"", ""Other""]}, ""geo"": ""ip_location""}, ""library_name"": ""transformers"", ""datasets"": [""trendmicro-ailab/Primus-Seed"", ""trendmicro-ailab/Primus-FineWeb"", ""trendmicro-ailab/Primus-Instruct""]}",[],2025-03-04 06:17:20,13,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""chat_template"": ""{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \""26 Jul 2024\"" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \""\"" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \""<|start_header_id|>system<|end_header_id|>\\n\\n\"" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \""Environment: ipython\\n\"" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \""Tools: \"" + builtin_tools | reject('equalto', 'code_interpreter') | join(\"", \"") + \""\\n\\n\""}}\n{%- endif %}\n{{- \""Cutting Knowledge Date: December 2023\\n\"" }}\n{{- \""Today Date: \"" + date_string + \""\\n\\n\"" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \""You have access to the following functions. To call a function, please respond with JSON for a function call.\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \""<|eot_id|>\"" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\""Cannot put tools in the first user message when there's no first user message!\"") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \""Given the following functions, please respond with a JSON for a function call \"" }}\n    {{- \""with its proper arguments that best answers the given prompt.\\n\\n\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n    {{- first_user_message + \""<|eot_id|>\""}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\""This model only supports single tool-calls at once!\"") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \""<|python_tag|>\"" + tool_call.name + \"".call(\"" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\""' + arg_val + '\""' }}\n                {%- if not loop.last %}\n                    {{- \"", \"" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \"")\"" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\""name\"": \""' + tool_call.name + '\"", ' }}\n            {{- '\""parameters\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \""}\"" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \""<|eom_id|>\"" }}\n        {%- else %}\n            {{- \""<|eot_id|>\"" }}\n        {%- endif %}\n    {%- elif message.role == \""tool\"" or message.role == \""ipython\"" %}\n        {{- \""<|start_header_id|>ipython<|end_header_id|>\\n\\n\"" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \""<|eot_id|>\"" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|eot_id|>"", ""pad_token"": ""<|end_of_text|>""}}",362,9003,"{'parameters': {'BF16': 8030261248.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 8030261248.0}","['transformers' 'safetensors' 'llama' 'text-generation' 'conversational'
 'en' 'ja' 'dataset:trendmicro-ailab/Primus-Seed'
 'dataset:trendmicro-ailab/Primus-FineWeb'
 'dataset:trendmicro-ailab/Primus-Instruct' 'arxiv:2502.11191'
 'arxiv:2402.07688' 'arxiv:2406.07599' 'arxiv:2009.03300'
 'arxiv:2306.05685' 'arxiv:2308.14508'
 'base_model:trendmicro-ailab/Llama-Primus-Base'
 'base_model:finetune:trendmicro-ailab/Llama-Primus-Base' 'license:mit'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'mergekit_config.yml'}
 {'rfilename': 'model-00001-of-00002.safetensors'}
 {'rfilename': 'model-00002-of-00002.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2025-02-16 06:19:28,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,67b5d5830918c8645ff352a8,trendmicro-ailab/Llama-Primus-Base,trendmicro-ailab/Llama-Primus-Merged,"license: mit
language:
- en
- ja
base_model:
- trendmicro-ailab/Llama-Primus-Base
pipeline_tag: text-generation
extra_gated_fields:
  Affiliation: text
  Country: country
  I want to use this model for:
    type: select
    options:
    - Research
    - Commercial
    - label: Other
      value: other
  Job title:
    type: select
    options:
    - Student
    - Research graduate
    - AI researcher
    - AI developer/engineer
    - Cybersecurity researcher
    - Reporter
    - Other
  geo: ip_location
library_name: transformers
datasets:
- trendmicro-ailab/Primus-Seed
- trendmicro-ailab/Primus-FineWeb
- trendmicro-ailab/Primus-Instruct","# Primus: A Pioneering Collection of Open-Source Datasets for Cybersecurity LLM Training

<img src=""https://i.imgur.com/PtqeTZw.png"" alt=""Llama-Primus-Merged Overview"" width=""60%"">

> TL;DR: Llama-Primus-Merged was first pre-trained on a large cybersecurity corpus (2.77B, _Primus-Seed_ and _Primus-FineWeb_), and then instruction fine-tuned on around 1,000 carefully curated cybersecurity QA tasks (_Primus-Instruct_) to restore its instruction-following ability. Finally, it was merged with Llama-3.1-8B-Instruct, maintaining the same instruction-following capability while achieving a 🚀**14.84%** improvement in aggregated scores across multiple cybersecurity benchmarks.

**🔥 For more details, please refer to the paper: [[📄Paper]](https://arxiv.org/abs/2502.11191).**

## Introduction

Large Language Models (LLMs) have demonstrated remarkable versatility in recent years, with promising applications in specialized domains such as finance, law, and biomedicine. However, in the domain of cybersecurity, we noticed a lack of open-source datasets specifically designed for LLM pre-training—even though much research has shown that LLMs acquire their knowledge during pre-training.  To fill this gap, we present a collection of datasets covering multiple stages of cybersecurity LLM training, including pre-training (_Primus-Seed_ and _Primus-FineWeb_), instruction fine-tuning (_Primus-Instruct_), and reasoning data for distillation (_Primus-Reasoning_).  Based on these datasets and Llama-3.1-8B-Instruct, we developed _Llama-Primus-Base_, _Llama-Primus-Merged_, and _Llama-Primus-Reasoning_. This model card is **Llama-Primus-Merged**.

  >  **Note:** No TrendMicro customer information is included.


## Benchmark Results

- [Cybersecurity](#cybersecurity)
- [Function Calling](#function-calling)
- [Safety & Toxicity](#safety--toxicity)
- [Multilingual](#multilingual)
- [General Chat Performance](#general-chat-performance)
- [Long-Context](#long-context)

  

#### Cybersecurity

  

| **Metric** (5-shot, w/o CoT) | **Llama-3.1-8B-Instruct** | **Llama-Primus-Merged** |
|---------------------------------|---------------------------|------------------------------|
| **CTI-Bench (MCQ)** | 0.6420 | 0.6656 |
| **CTI-Bench (CVE → CWE)** | 0.5910 | 0.6620 |
| **CTI-Bench (CVSS, _lower is better_)** | 1.2712 | 1.1233 |
| **CTI-Bench (ATE)** | 0.2721 | 0.3387 |
| **CyberMetric (500)** | 0.8560 | 0.8660 |
| **SecEval** | 0.4966 | 0.5062 |
| **Cissp (Exams in book)** | 0.7073 | 0.7191 |
| **_Agg._** | 2.29 | 2.63 ↑**14.84%** 🔥 |

CTI-Bench(CVSS) is scored using Mean Absolute Deviation (_lower is better_), CTI-ATE uses F1 score, and the others use accuracy. The aggregate score (_Agg._) is the sum of all benchmarks, with CTI-Bench(CVSS) negated.

References:
-  **CyberMetric**: [CyberMetric: A Benchmark Dataset based on Retrieval-Augmented...](https://arxiv.org/abs/2402.07688)
-  **CTI-Bench**: [CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence](https://arxiv.org/abs/2406.07599)
-  **SecEval**: [SecEval: A Comprehensive Benchmark for Evaluating Cybersecurity Knowledge of Foundation Models](https://xuanwuai.github.io/SecEval/)

  

#### Function Calling

  
| **Metric** | **Llama-3.1-8B-Instruct** | **Llama-Primus-Merged**|
|---------------|---------------------------|------------------------------|
| **BFCL (V2)** | 73.02 (prompt) | 74.77 (prompt) | - |

  

Reference:

- [BFCL (V2)](https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html)

  

### Safety & Toxicity

  

| **Metric** | **Llama-3.1-8B-Instruct** | **Llama-Primus-Merged** |
|----------------------------------|---------------------------|------------------------------|
| **dan (Jailbreak)** | 28.98% | 41.70% |
| **encoding (Jailbreak)** | 100.00% | 100.00% |
| **goodside (Hallucination/Injection)** | 77.08% | 72.10% |
| **latentinjection (Injection)** | 75.55% | 74.00% |
| **leakreplay (Copyright)** | 95.71% | 96.90% |
| **malwaregen (Disallowed)** | 14.34% | 29.00% |
| **realtoxicityprompts (Disallowed)** | 90.03% | 85.40% |
| **snowball (Hallucination)** | 59.67% | 84.20% |
| **xss (Injection)** | 100.00% | 98.30% |
| **XSTest (Over Refuse)** | 93.20% | 83.20% |

References:

-  **Garak**: [Garak Repository](https://github.com/leondz/garak)
-  **XSTest**: [XSTest Repository](https://github.com/paul-rottger/exaggerated-safety)

  
  

### Multilingual

  

| **Language** | **Llama-3.1-8B-Instruct** | **Llama-Primus-Merged** |
|---------------|---------------------------|------------------------------|
| **MMLU (English)** | 68.16% | 67.36% |
| **MMLU (Japanese)** | 49.22% | 47.85% |
| **MMLU (French)** | 58.91% | 58.14% |
| **MMLU (German)** | 57.70% | 56.68% |


References:
-  **English**: [MMLU Dataset](https://arxiv.org/abs/2009.03300)
-  **German/French**: [MLMM Evaluation](https://github.com/nlp-uoregon/mlmm-evaluation?tab=readme-ov-file)
-  **Japanese**: [Freedom Intelligence MMLU Japanese](https://huggingface.co/datasets/FreedomIntelligence/MMLU_Japanese)

  
  

#### General Chat Performance

| **Metric** | **Llama-3.1-8B-Instruct** | **Llama-Primus-Merged** |
|-----------------|---------------------------|------------------------------|
| **MT Bench** | 8.3491 | 8.29375 |

Reference:
- [MT Bench](https://arxiv.org/abs/2306.05685)

  

### Long-Context
 

| **Length** | **Llama-3.1-8B-Instruct** | **Llama-Primus-Merged** |
|------------|---------------------------|------------------------------|
| **8K+** | 51.08 | 50.66 |
| **16K+** | 29.18 | 27.13 |

Reference:
- [LongBench](https://arxiv.org/abs/2308.14508)

## About _Primus_
_Primus_ is Trend Micro's pioneering family of lightweight, state-of-the-art open cybersecurity language models and datasets. Developed through our cutting-edge research initiatives and advanced technology, these resources share the innovative foundation that powers our enterprise-class [Trend Cybertron](https://newsroom.trendmicro.com/2025-02-25-Trend-Micro-Puts-Industry-Ahead-of-Cyberattacks-with-Industrys-First-Proactive-Cybersecurity-AI) solution. As an industry leader in cybersecurity, Trend Micro is proud to contribute these powerful, efficiency-optimized models and datasets to the community, while maintaining the excellence and reliability that define our global security standards.

## License
This model is based on the MIT license, but you must also comply with the Llama 3.1 Community License Agreement.",6416.0,6.1059812934679055e-06
mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF,67c2869faed04e12e9afce30,mradermacher,"{""base_model"": ""ehristoforu/Gemma2-9B-it-psy10k-mental_health"", ""datasets"": [""jtatman/mental_health_psychology_curated_alpaca""], ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""quantized_by"": ""mradermacher"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""gemma2"", ""trl"", ""sft""]}",[],2025-03-25 06:29:23,1,0.0,,218,7858,,"['transformers' 'gguf' 'text-generation-inference' 'unsloth' 'gemma2'
 'trl' 'sft' 'en'
 'dataset:jtatman/mental_health_psychology_curated_alpaca'
 'base_model:ehristoforu/Gemma2-9B-it-psy10k-mental_health'
 'base_model:quantized:ehristoforu/Gemma2-9B-it-psy10k-mental_health'
 'license:apache-2.0' 'endpoints_compatible' 'region:us' 'imatrix'
 'conversational']",,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ1_M.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ1_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ2_M.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ2_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ2_XS.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ2_XXS.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ3_M.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ3_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ3_XS.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ3_XXS.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ4_NL.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-IQ4_XS.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q2_K.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q2_K_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q3_K_L.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q3_K_M.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q3_K_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q4_0.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q4_1.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q4_K_M.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q4_K_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q5_K_M.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q5_K_S.gguf'}
 {'rfilename': 'Gemma2-9B-it-psy10k-mental_health.i1-Q6_K.gguf'}
 {'rfilename': 'README.md'}]",2025-03-01 04:01:35,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 9241705984, ""architecture"": ""gemma2"", ""context_length"": 8192, ""quantize_imatrix_file"": ""Gemma2-9B-it-psy10k-mental_health-i1-GGUF/imatrix.dat"", ""chat_template"": ""{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"", ""bos_token"": ""<bos>"", ""eos_token"": ""<eos>""}",quantized,669672a2060bba90ce1994e2,ehristoforu/Gemma2-9B-it-psy10k-mental_health,mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF,"base_model: ehristoforu/Gemma2-9B-it-psy10k-mental_health
datasets:
- jtatman/mental_health_psychology_curated_alpaca
language:
- en
library_name: transformers
license: apache-2.0
quantized_by: mradermacher
tags:
- text-generation-inference
- transformers
- unsloth
- gemma2
- trl
- sft","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags: nicoboss -->
weighted/imatrix quants of https://huggingface.co/ehristoforu/Gemma2-9B-it-psy10k-mental_health

<!-- provided-files -->
static quants are available at https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ1_S.gguf) | i1-IQ1_S | 2.5 | for the desperate |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ1_M.gguf) | i1-IQ1_M | 2.6 | mostly desperate |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 2.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ2_XS.gguf) | i1-IQ2_XS | 3.2 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ2_S.gguf) | i1-IQ2_S | 3.3 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ2_M.gguf) | i1-IQ2_M | 3.5 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q2_K_S.gguf) | i1-Q2_K_S | 3.7 | very low quality |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 3.9 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q2_K.gguf) | i1-Q2_K | 3.9 | IQ3_XXS probably better |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ3_XS.gguf) | i1-IQ3_XS | 4.2 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ3_S.gguf) | i1-IQ3_S | 4.4 | beats Q3_K* |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q3_K_S.gguf) | i1-Q3_K_S | 4.4 | IQ3_XS probably better |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ3_M.gguf) | i1-IQ3_M | 4.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q3_K_M.gguf) | i1-Q3_K_M | 4.9 | IQ3_S probably better |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q3_K_L.gguf) | i1-Q3_K_L | 5.2 | IQ3_M probably better |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ4_XS.gguf) | i1-IQ4_XS | 5.3 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-IQ4_NL.gguf) | i1-IQ4_NL | 5.5 | prefer IQ4_XS |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q4_0.gguf) | i1-Q4_0 | 5.6 | fast, low quality |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q4_K_S.gguf) | i1-Q4_K_S | 5.6 | optimal size/speed/quality |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q4_K_M.gguf) | i1-Q4_K_M | 5.9 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q4_1.gguf) | i1-Q4_1 | 6.1 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q5_K_S.gguf) | i1-Q5_K_S | 6.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q5_K_M.gguf) | i1-Q5_K_M | 6.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Gemma2-9B-it-psy10k-mental_health-i1-GGUF/resolve/main/Gemma2-9B-it-psy10k-mental_health.i1-Q6_K.gguf) | i1-Q6_K | 7.7 | practically like static Q6_K |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->",6077.0,6.01477502381011e-06
mradermacher/llasa-3b-GGUF,67bf82ad9272d3cf3e86fc03,mradermacher,"{""base_model"": ""srinivasbilla/llasa-3b"", ""language"": [""en""], ""library_name"": ""transformers"", ""mradermacher"": {""readme_rev"": 1}, ""quantized_by"": ""mradermacher""}",[],2025-09-16 07:39:43,2,0.0,,160,5217,,"['transformers' 'gguf' 'en' 'base_model:srinivasbilla/llasa-3b'
 'base_model:quantized:srinivasbilla/llasa-3b' 'endpoints_compatible'
 'region:us' 'conversational']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'Llasa-3B.IQ4_XS.gguf'}
 {'rfilename': 'Llasa-3B.Q2_K.gguf'} {'rfilename': 'Llasa-3B.Q3_K_L.gguf'}
 {'rfilename': 'Llasa-3B.Q3_K_M.gguf'}
 {'rfilename': 'Llasa-3B.Q3_K_S.gguf'}
 {'rfilename': 'Llasa-3B.Q4_K_M.gguf'}
 {'rfilename': 'Llasa-3B.Q4_K_S.gguf'}
 {'rfilename': 'Llasa-3B.Q5_K_M.gguf'}
 {'rfilename': 'Llasa-3B.Q5_K_S.gguf'} {'rfilename': 'Llasa-3B.Q6_K.gguf'}
 {'rfilename': 'Llasa-3B.Q8_0.gguf'} {'rfilename': 'Llasa-3B.f16.gguf'}
 {'rfilename': 'README.md'} {'rfilename': 'llasa-3b.IQ4_XS.gguf'}
 {'rfilename': 'llasa-3b.Q2_K.gguf'} {'rfilename': 'llasa-3b.Q3_K_L.gguf'}
 {'rfilename': 'llasa-3b.Q3_K_M.gguf'}
 {'rfilename': 'llasa-3b.Q3_K_S.gguf'}
 {'rfilename': 'llasa-3b.Q4_K_M.gguf'}
 {'rfilename': 'llasa-3b.Q4_K_S.gguf'}
 {'rfilename': 'llasa-3b.Q5_K_M.gguf'}
 {'rfilename': 'llasa-3b.Q5_K_S.gguf'} {'rfilename': 'llasa-3b.Q6_K.gguf'}
 {'rfilename': 'llasa-3b.Q8_0.gguf'} {'rfilename': 'llasa-3b.f16.gguf'}]",2025-02-26 21:07:57,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 3414101056, ""architecture"": ""llama"", ""context_length"": 131072, ""chat_template"": ""{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\""%d %b %Y\"") %}\n    {%- else %}\n        {%- set date_string = \""26 Jul 2024\"" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \""\"" %}\n{%- endif %}\n\n{#- System message #}\n{{- \""<|start_header_id|>system<|end_header_id|>\\n\\n\"" }}\n{%- if tools is not none %}\n    {{- \""Environment: ipython\\n\"" }}\n{%- endif %}\n{{- \""Cutting Knowledge Date: December 2023\\n\"" }}\n{{- \""Today Date: \"" + date_string + \""\\n\\n\"" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \""You have access to the following functions. To call a function, please respond with JSON for a function call.\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \""<|eot_id|>\"" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\""Cannot put tools in the first user message when there's no first user message!\"") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \""Given the following functions, please respond with a JSON for a function call \"" }}\n    {{- \""with its proper arguments that best answers the given prompt.\\n\\n\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n    {{- first_user_message + \""<|eot_id|>\""}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\""This model only supports single tool-calls at once!\"") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n        {{- '{\""name\"": \""' + tool_call.name + '\"", ' }}\n        {{- '\""parameters\"": ' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \""}\"" }}\n        {{- \""<|eot_id|>\"" }}\n    {%- elif message.role == \""tool\"" or message.role == \""ipython\"" %}\n        {{- \""<|start_header_id|>ipython<|end_header_id|>\\n\\n\"" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \""<|eot_id|>\"" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"", ""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|eot_id|>""}",quantized,678ea825cc06f797148302c1,srinivasbilla/llasa-3b,mradermacher/llasa-3b-GGUF,"base_model: srinivasbilla/llasa-3b
language:
- en
library_name: transformers
mradermacher:
  readme_rev: 1
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/srinivasbilla/llasa-3b

<!-- provided-files -->

***For a convenient overview and download list, visit our [model page for this model](https://hf.tst.eu/model#llasa-3b-GGUF).***

weighted/imatrix quants are available at https://huggingface.co/mradermacher/llasa-3b-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q2_K.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q2_K.gguf) | Q2_K | 3.4 |  |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q3_K_S.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q3_K_S.gguf) | Q3_K_S | 3.8 |  |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q3_K_M.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q3_K_M.gguf) | Q3_K_M | 4.1 | lower quality |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q3_K_L.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q3_K_L.gguf) | Q3_K_L | 4.3 |  |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.IQ4_XS.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.IQ4_XS.gguf) | IQ4_XS | 4.4 |  |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q4_K_S.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q4_K_S.gguf) | Q4_K_S | 4.6 | fast, recommended |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q4_K_M.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q4_K_M.gguf) | Q4_K_M | 4.8 | fast, recommended |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q5_K_S.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q5_K_S.gguf) | Q5_K_S | 5.4 |  |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q5_K_M.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q5_K_M.gguf) | Q5_K_M | 5.5 |  |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q6_K.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q6_K.gguf) | Q6_K | 6.2 | very good quality |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.Q8_0.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.Q8_0.gguf) | Q8_0 | 8.0 | fast, best quality |
| [PART 1](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/Llasa-3B.f16.gguf) [PART 2](https://huggingface.co/mradermacher/llasa-3b-GGUF/resolve/main/llasa-3b.f16.gguf) | f16 | 15.0 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",4133.0,5.740139248036122e-06
allknowingroger/WestLakeLaser-12B-MoE,6620cd4cc7f53e39e535d5d4,allknowingroger,"{""license"": ""apache-2.0"", ""tags"": [""moe"", ""frankenmoe"", ""merge"", ""mergekit"", ""lazymergekit"", ""allknowingroger/PrometheusLaser-7B-slerp"", ""senseable/WestLake-7B-v2""], ""base_model"": [""allknowingroger/PrometheusLaser-7B-slerp"", ""senseable/WestLake-7B-v2""]}",[],2024-04-18 07:42:44,0,0.0,"{""architectures"": [""MixtralForCausalLM""], ""model_type"": ""mixtral"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""<s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}",0,3573,"{'parameters': {'BF16': 12879138816.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 12879138816.0}","['transformers' 'safetensors' 'mixtral' 'text-generation' 'moe'
 'frankenmoe' 'merge' 'mergekit' 'lazymergekit'
 'allknowingroger/PrometheusLaser-7B-slerp' 'senseable/WestLake-7B-v2'
 'base_model:allknowingroger/PrometheusLaser-7B-slerp'
 'base_model:merge:allknowingroger/PrometheusLaser-7B-slerp'
 'base_model:senseable/WestLake-7B-v2'
 'base_model:merge:senseable/WestLake-7B-v2' 'license:apache-2.0'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'mergekit_moe_config.yml'}
 {'rfilename': 'model-00001-of-00013.safetensors'}
 {'rfilename': 'model-00002-of-00013.safetensors'}
 {'rfilename': 'model-00003-of-00013.safetensors'}
 {'rfilename': 'model-00004-of-00013.safetensors'}
 {'rfilename': 'model-00005-of-00013.safetensors'}
 {'rfilename': 'model-00006-of-00013.safetensors'}
 {'rfilename': 'model-00007-of-00013.safetensors'}
 {'rfilename': 'model-00008-of-00013.safetensors'}
 {'rfilename': 'model-00009-of-00013.safetensors'}
 {'rfilename': 'model-00010-of-00013.safetensors'}
 {'rfilename': 'model-00011-of-00013.safetensors'}
 {'rfilename': 'model-00012-of-00013.safetensors'}
 {'rfilename': 'model-00013-of-00013.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2024-04-18 07:35:40,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,merge,65adc13fe0ee7990a651af6d,senseable/WestLake-7B-v2,allknowingroger/WestLakeLaser-12B-MoE,"license: apache-2.0
tags:
- moe
- frankenmoe
- merge
- mergekit
- lazymergekit
- allknowingroger/PrometheusLaser-7B-slerp
- senseable/WestLake-7B-v2
base_model:
- allknowingroger/PrometheusLaser-7B-slerp
- senseable/WestLake-7B-v2","# WestLakeLaser-12B-MoE

WestLakeLaser-12B-MoE is a Mixture of Experts (MoE) made with the following models using [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing):
* [allknowingroger/PrometheusLaser-7B-slerp](https://huggingface.co/allknowingroger/PrometheusLaser-7B-slerp)
* [senseable/WestLake-7B-v2](https://huggingface.co/senseable/WestLake-7B-v2)

## 🧩 Configuration

```yaml
base_model: allknowingroger/PrometheusLaser-7B-slerp
experts:
  - source_model: allknowingroger/PrometheusLaser-7B-slerp
    positive_prompts: [""what""]
  - source_model: senseable/WestLake-7B-v2
    positive_prompts: [""why""]
```

## 💻 Usage

```python
!pip install -qU transformers bitsandbytes accelerate

from transformers import AutoTokenizer
import transformers
import torch

model = ""allknowingroger/WestLakeLaser-12B-MoE""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    model_kwargs={""torch_dtype"": torch.float16, ""load_in_4bit"": True},
)

messages = [{""role"": ""user"", ""content"": ""Explain what a Mixture of Experts is in less than 100 words.""}]
prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
outputs = pipeline(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
print(outputs[0][""generated_text""])
```",1408.0,5.486369743274644e-06
mradermacher/granite-3b-code-base-GGUF,668dee036deb07a339c9dc14,mradermacher,"{""base_model"": ""ibm-granite/granite-3b-code-base-2k"", ""datasets"": [""codeparrot/github-code-clean"", ""bigcode/starcoderdata"", ""open-web-math/open-web-math"", ""math-ai/StackMathQA""], ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""quantized_by"": ""mradermacher"", ""tags"": [""code"", ""granite""]}",[],2024-12-16 01:55:07,0,0.0,,40,2553,,"['transformers' 'gguf' 'code' 'granite' 'en'
 'dataset:codeparrot/github-code-clean' 'dataset:bigcode/starcoderdata'
 'dataset:open-web-math/open-web-math' 'dataset:math-ai/StackMathQA'
 'base_model:ibm-granite/granite-3b-code-base-2k'
 'base_model:quantized:ibm-granite/granite-3b-code-base-2k'
 'license:apache-2.0' 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'granite-3b-code-base.IQ3_M.gguf'}
 {'rfilename': 'granite-3b-code-base.IQ3_S.gguf'}
 {'rfilename': 'granite-3b-code-base.IQ3_XS.gguf'}
 {'rfilename': 'granite-3b-code-base.IQ4_XS.gguf'}
 {'rfilename': 'granite-3b-code-base.Q2_K.gguf'}
 {'rfilename': 'granite-3b-code-base.Q3_K_L.gguf'}
 {'rfilename': 'granite-3b-code-base.Q3_K_M.gguf'}
 {'rfilename': 'granite-3b-code-base.Q3_K_S.gguf'}
 {'rfilename': 'granite-3b-code-base.Q4_K_M.gguf'}
 {'rfilename': 'granite-3b-code-base.Q4_K_S.gguf'}
 {'rfilename': 'granite-3b-code-base.Q5_K_M.gguf'}
 {'rfilename': 'granite-3b-code-base.Q5_K_S.gguf'}
 {'rfilename': 'granite-3b-code-base.Q6_K.gguf'}
 {'rfilename': 'granite-3b-code-base.Q8_0.gguf'}
 {'rfilename': 'granite-3b-code-base.f16.gguf'}]",2024-07-10 02:12:19,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 3482503680, ""architecture"": ""llama"", ""context_length"": 2048, ""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|endoftext|>""}",quantized,66273226875a989520b947c1,ibm-granite/granite-3b-code-base-2k,mradermacher/granite-3b-code-base-GGUF,"base_model: ibm-granite/granite-3b-code-base-2k
datasets:
- codeparrot/github-code-clean
- bigcode/starcoderdata
- open-web-math/open-web-math
- math-ai/StackMathQA
language:
- en
library_name: transformers
license: apache-2.0
quantized_by: mradermacher
tags:
- code
- granite","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/ibm-granite/granite-3b-code-base-2k

<!-- provided-files -->
weighted/imatrix quants are available at https://huggingface.co/mradermacher/granite-3b-code-base-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q2_K.gguf) | Q2_K | 1.4 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.IQ3_XS.gguf) | IQ3_XS | 1.6 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.IQ3_S.gguf) | IQ3_S | 1.7 | beats Q3_K* |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q3_K_S.gguf) | Q3_K_S | 1.7 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.IQ3_M.gguf) | IQ3_M | 1.7 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q3_K_M.gguf) | Q3_K_M | 1.8 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q3_K_L.gguf) | Q3_K_L | 2.0 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.IQ4_XS.gguf) | IQ4_XS | 2.0 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q4_K_S.gguf) | Q4_K_S | 2.1 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q4_K_M.gguf) | Q4_K_M | 2.2 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q5_K_S.gguf) | Q5_K_S | 2.5 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q5_K_M.gguf) | Q5_K_M | 2.6 |  |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q6_K.gguf) | Q6_K | 3.0 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.Q8_0.gguf) | Q8_0 | 3.8 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/granite-3b-code-base-GGUF/resolve/main/granite-3b-code-base.f16.gguf) | f16 | 7.1 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3592.0,5.2610358942139886e-06
glif-loradex-trainer/ancient_egyptian_memes_Pixcat_1,6729a41d168ca6dbcb249eba,glif-loradex-trainer,"{""tags"": [""diffusers"", ""text-to-image"", ""template:sd-lora"", ""base_model:black-forest-labs/FLUX.1-dev"", ""base_model:finetune:black-forest-labs/FLUX.1-dev"", ""license:other"", ""region:us"", ""flux"", ""lora""], ""widget"": [{""output"": {""url"": ""https://huggingface.co/glif-loradex-trainer/ancient_egyptian_memes_Pixcat_1/resolve/main/samples/1730782177575__000003000_0.jpg""}, ""text"": ""Pixcat sitting waving right paw Pixcat_model_1""}, {""output"": {""url"": ""https://huggingface.co/glif-loradex-trainer/ancient_egyptian_memes_Pixcat_1/resolve/main/samples/1730782202266__000003000_1.jpg""}, ""text"": ""Pixcat sitting on a tropical beach waving left paw Pixcat_model_1""}, {""output"": {""url"": ""https://huggingface.co/glif-loradex-trainer/ancient_egyptian_memes_Pixcat_1/resolve/main/samples/1730782226997__000003000_2.jpg""}, ""text"": ""pixcat on a city street running on his hind legs Pixcat_model_1""}], ""base_model"": ""black-forest-labs/FLUX.1-dev"", ""trigger"": ""Pixcat_model_1"", ""instance_prompt"": ""Pixcat_model_1"", ""license"": ""other"", ""license_name"": ""flux-1-dev-non-commercial-license"", ""license_link"": ""https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md""}","[{'_id': '680b71dfa07785a516f270ef', 'adapter': 'lora', 'adapterWeightsPath': 'Pixcat_1.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/flux-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}
 {'_id': '6818c94d5111343b72f18ab4', 'adapter': 'lora', 'adapterWeightsPath': 'Pixcat_1.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'replicate', 'providerDetails': None, 'providerId': 'black-forest-labs/flux-dev-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2024-11-05 04:51:06,1,0.0,,10,2415,,"['diffusers' 'text-to-image' 'template:sd-lora'
 'base_model:black-forest-labs/FLUX.1-dev'
 'base_model:finetune:black-forest-labs/FLUX.1-dev' 'license:other'
 'region:us' 'flux' 'lora'
 'base_model:adapter:black-forest-labs/FLUX.1-dev']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'Pixcat_1.safetensors'}
 {'rfilename': 'Pixcat_1_000001500.safetensors'}
 {'rfilename': 'Pixcat_1_000002000.safetensors'}
 {'rfilename': 'Pixcat_1_000002500.safetensors'}
 {'rfilename': 'README.md'} {'rfilename': 'config.yaml'}
 {'rfilename': 'samples/1730774898375__000000000_0.jpg'}
 {'rfilename': 'samples/1730774923002__000000000_1.jpg'}
 {'rfilename': 'samples/1730774947627__000000000_2.jpg'}
 {'rfilename': 'samples/1730776116124__000000500_0.jpg'}
 {'rfilename': 'samples/1730776140726__000000500_1.jpg'}
 {'rfilename': 'samples/1730776165370__000000500_2.jpg'}
 {'rfilename': 'samples/1730777328413__000001000_0.jpg'}
 {'rfilename': 'samples/1730777352993__000001000_1.jpg'}
 {'rfilename': 'samples/1730777377606__000001000_2.jpg'}
 {'rfilename': 'samples/1730778537068__000001500_0.jpg'}
 {'rfilename': 'samples/1730778561654__000001500_1.jpg'}
 {'rfilename': 'samples/1730778586299__000001500_2.jpg'}
 {'rfilename': 'samples/1730779744242__000002000_0.jpg'}
 {'rfilename': 'samples/1730779768937__000002000_1.jpg'}
 {'rfilename': 'samples/1730779793666__000002000_2.jpg'}
 {'rfilename': 'samples/1730780967259__000002500_0.jpg'}
 {'rfilename': 'samples/1730780991954__000002500_1.jpg'}
 {'rfilename': 'samples/1730781016684__000002500_2.jpg'}
 {'rfilename': 'samples/1730782177575__000003000_0.jpg'}
 {'rfilename': 'samples/1730782202266__000003000_1.jpg'}
 {'rfilename': 'samples/1730782226997__000003000_2.jpg'}]",2024-11-05 04:50:37,diffusers,,,adapter,66aaa908fc35e079a941470d,black-forest-labs/FLUX.1-dev,glif-loradex-trainer/ancient_egyptian_memes_Pixcat_1,"tags:
- diffusers
- text-to-image
- template:sd-lora
- base_model:black-forest-labs/FLUX.1-dev
- base_model:finetune:black-forest-labs/FLUX.1-dev
- license:other
- region:us
- flux
- lora
widget:
-   output:
        url: samples/1730782177575__000003000_0.jpg
    text: Pixcat sitting waving right paw Pixcat_model_1
-   output:
        url: samples/1730782202266__000003000_1.jpg
    text: Pixcat sitting on a tropical beach waving left paw Pixcat_model_1
-   output:
        url: samples/1730782226997__000003000_2.jpg
    text: pixcat on a city street running on his hind legs Pixcat_model_1
base_model: black-forest-labs/FLUX.1-dev
trigger: Pixcat_model_1
instance_prompt: Pixcat_model_1
license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md","# Pixcat_1
Model trained with [AI Toolkit by Ostris](https://github.com/ostris/ai-toolkit) under the [Glif Loradex program](https://huggingface.co/glif-loradex-trainer) by [Glif](https://glif.app) user `ancient_egyptian_memes`.
<Gallery />
## Trigger words
You should use `Pixcat_model_1` to trigger the image generation.
## Download model
Weights for this model are available in Safetensors format.
[Download](/glif-loradex-trainer/ancient_egyptian_memes_Pixcat_1/tree/main) them in the Files & versions tab.

## License
This model is licensed under the [flux-1-dev-non-commercial-license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).",665.0,5.223786464509273e-06
davidschulte/ESM_tasksource__mmlu_astronomy,6730b06660438e4262a8e05f,davidschulte,"{""base_model"": ""bert-base-multilingual-uncased"", ""datasets"": [""tasksource/mmlu""], ""license"": ""apache-2.0"", ""tags"": [""embedding_space_map"", ""BaseLM:bert-base-multilingual-uncased""]}",[],2025-03-25 09:28:37,0,0.0,"{""additional_chat_templates"": {}}",0,1638,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 590592, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 590592.0}","['safetensors' 'embedding_space_map'
 'BaseLM:bert-base-multilingual-uncased' 'dataset:tasksource/mmlu'
 'base_model:google-bert/bert-base-multilingual-uncased'
 'base_model:finetune:google-bert/bert-base-multilingual-uncased'
 'license:apache-2.0' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}]",2024-11-10 13:08:54,,,,finetune,621ffdc036468d709f174337,google-bert/bert-base-multilingual-uncased,davidschulte/ESM_tasksource__mmlu_astronomy,"base_model: bert-base-multilingual-uncased
datasets:
- tasksource/mmlu
license: apache-2.0
tags:
- embedding_space_map
- BaseLM:bert-base-multilingual-uncased","# ESM tasksource/mmlu

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

ESM

- **Developed by:** David Schulte
- **Model type:** ESM
- **Base Model:** bert-base-multilingual-uncased
- **Intermediate Task:** tasksource/mmlu
- **ESM architecture:** linear
- **ESM embedding dimension:** [More Information Needed]
- **Language(s) (NLP):** [More Information Needed]
- **License:** Apache-2.0 license
- **ESM version:** 0.1.0

## Training Details

### Intermediate Task
- **Task ID:** tasksource/mmlu
- **Subset [optional]:** astronomy
- **Text Column:** question
- **Label Column:** answer
- **Dataset Split:**  test
- **Sample size [optional]:** 152
- **Sample seed [optional]:** 

### Training Procedure [optional]

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Language Model Training Hyperparameters [optional]
- **Epochs:** 3
- **Batch size:** 32
- **Learning rate:** 2e-05
- **Weight Decay:** 0.01
- **Optimizer**: AdamW

### ESM Training Hyperparameters [optional]
- **Epochs:** 10
- **Batch size:** 32
- **Learning rate:** 0.001
- **Weight Decay:** 0.01
- **Optimizer**: AdamW


### Additional trainiung details [optional]


## Model evaluation

### Evaluation of fine-tuned language model [optional]


### Evaluation of ESM [optional]
MSE: 

### Additional evaluation details [optional]


## What are Embedding Space Maps used for?
Embedding Space Maps are a part of ESM-LogME, a efficient method for finding intermediate datasets for transfer learning. There are two reasons to use ESM-LogME:

### You don't have enough training data for your problem
If you don't have a enough training data for your problem, just use ESM-LogME to find more.
You can supplement model training by including publicly available datasets in the training process. 

1. Fine-tune a language model on suitable intermediate dataset.
2. Fine-tune the resulting model on your target dataset.

This workflow is called intermediate task transfer learning and it can significantly improve the target performance.

But what is a suitable dataset for your problem? ESM-LogME enable you to quickly rank thousands of datasets on the Hugging Face Hub by how well they are exptected to transfer to your target task.

### You want to find similar datasets to your target dataset
Using ESM-LogME can be used like search engine on the Hugging Face Hub. You can find similar tasks to your target task without having to rely on heuristics. ESM-LogME estimates how language models fine-tuned on each intermediate task would benefinit your target task. This quantitative approach combines the effects of domain similarity and task similarity. 

## How can I use ESM-LogME / ESMs?
[![PyPI version](https://img.shields.io/pypi/v/hf-dataset-selector.svg)](https://pypi.org/project/hf-dataset-selector)

We release **hf-dataset-selector**, a Python package for intermediate task selection using Embedding Space Maps.

**hf-dataset-selector** fetches ESMs for a given language model and uses it to find the best dataset for applying intermediate training to the target task. ESMs are found by their tags on the Huggingface Hub.

```python
from hfselect import Dataset, compute_task_ranking

# Load target dataset from the Hugging Face Hub
dataset = Dataset.from_hugging_face(
    name=""stanfordnlp/imdb"",
    split=""train"",
    text_col=""text"",
    label_col=""label"",
    is_regression=False,
    num_examples=1000,
    seed=42
)

# Fetch ESMs and rank tasks
task_ranking = compute_task_ranking(
    dataset=dataset,
    model_name=""bert-base-multilingual-uncased""
)

# Display top 5 recommendations
print(task_ranking[:5])
```
```python
1.   davanstrien/test_imdb_embedd2                     Score: -0.618529
2.   davanstrien/test_imdb_embedd                      Score: -0.618644
3.   davanstrien/test1                                 Score: -0.619334
4.   stanfordnlp/imdb                                  Score: -0.619454
5.   stanfordnlp/sst                                   Score: -0.62995
```

|   Rank | Task ID                       | Task Subset     | Text Column   | Label Column   | Task Split   |   Num Examples | ESM Architecture   |     Score |
|-------:|:------------------------------|:----------------|:--------------|:---------------|:-------------|---------------:|:-------------------|----------:|
|      1 | davanstrien/test_imdb_embedd2 | default         | text          | label          | train        |          10000 | linear             | -0.618529 |
|      2 | davanstrien/test_imdb_embedd  | default         | text          | label          | train        |          10000 | linear             | -0.618644 |
|      3 | davanstrien/test1             | default         | text          | label          | train        |          10000 | linear             | -0.619334 |
|      4 | stanfordnlp/imdb              | plain_text      | text          | label          | train        |          10000 | linear             | -0.619454 |
|      5 | stanfordnlp/sst               | dictionary      | phrase        | label          | dictionary   |          10000 | linear             | -0.62995  |
|      6 | stanfordnlp/sst               | default         | sentence      | label          | train        |           8544 | linear             | -0.63312  |
|      7 | kuroneko5943/snap21           | CDs_and_Vinyl_5 | sentence      | label          | train        |           6974 | linear             | -0.634365 |
|      8 | kuroneko5943/snap21           | Video_Games_5   | sentence      | label          | train        |           6997 | linear             | -0.638787 |
|      9 | kuroneko5943/snap21           | Movies_and_TV_5 | sentence      | label          | train        |           6989 | linear             | -0.639068 |
|     10 | fancyzhx/amazon_polarity      | amazon_polarity | content       | label          | train        |          10000 | linear             | -0.639718 |

For more information on how to use ESMs please have a look at the [official Github repository](https://github.com/davidschulte/hf-dataset-selector). We provide documentation further documentation and tutorials for finding intermediate datasets and training your own ESMs.


## How do Embedding Space Maps work?

<!-- This section describes the evaluation protocols and provides the results. -->
Embedding Space Maps (ESMs) are neural networks that approximate the effect of fine-tuning a language model on a task. They can be used to quickly transform embeddings from a base model to approximate how a fine-tuned model would embed the the input text.
ESMs can be used for intermediate task selection with the ESM-LogME workflow.

## How can I use Embedding Space Maps for Intermediate Task Selection?

## Citation


<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->
If you are using this Embedding Space Maps, please cite our [paper](https://aclanthology.org/2024.emnlp-main.529/).

**BibTeX:**


```
@inproceedings{schulte-etal-2024-less,
    title = ""Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning"",
    author = ""Schulte, David  and
      Hamborg, Felix  and
      Akbik, Alan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.529/"",
    doi = ""10.18653/v1/2024.emnlp-main.529"",
    pages = ""9431--9442"",
    abstract = ""Intermediate task transfer learning can greatly improve model performance. If, for example, one has little training data for emotion detection, first fine-tuning a language model on a sentiment classification dataset may improve performance strongly. But which task to choose for transfer learning? Prior methods producing useful task rankings are infeasible for large source pools, as they require forward passes through all source language models. We overcome this by introducing Embedding Space Maps (ESMs), light-weight neural networks that approximate the effect of fine-tuning a language model. We conduct the largest study on NLP task transferability and task selection with 12k source-target pairs. We find that applying ESMs on a prior method reduces execution time and disk space usage by factors of 10 and 278, respectively, while retaining high selection performance (avg. regret@5 score of 2.95).""
}
```


**APA:**

```
Schulte, D., Hamborg, F., & Akbik, A. (2024, November). Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 9431-9442).
```

## Additional Information",9085.0,4.963580522361986e-06
mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF,67a68e6a51668c3718fb9ff2,mradermacher,"{""base_model"": ""922-CA/Llama-3-monika-ddlc-8b-v1"", ""datasets"": [""922-CA/MoCha_v1a""], ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""other"", ""license_link"": ""LICENSE"", ""license_name"": ""llama3"", ""quantized_by"": ""mradermacher"", ""tags"": [""unsloth"", ""trl"", ""sft""]}",[],2025-02-08 00:45:24,0,0.0,,84,1530,,"['transformers' 'gguf' 'unsloth' 'trl' 'sft' 'en'
 'dataset:922-CA/MoCha_v1a' 'base_model:922-CA/Llama-3-monika-ddlc-8b-v1'
 'base_model:quantized:922-CA/Llama-3-monika-ddlc-8b-v1' 'license:other'
 'endpoints_compatible' 'region:us' 'conversational']",,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.IQ4_XS.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q2_K.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q3_K_L.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q3_K_M.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q3_K_S.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q4_K_M.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q4_K_S.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q5_K_M.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q5_K_S.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q6_K.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.Q8_0.gguf'}
 {'rfilename': 'Llama-3-monika-ddlc-8b-v1.f16.gguf'}
 {'rfilename': 'README.md'}]",2025-02-07 22:51:22,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 8030261248, ""architecture"": ""llama"", ""context_length"": 8192, ""chat_template"": ""{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|end_of_text|>""}",quantized,6623453bb642e29cdf61dec8,922-CA/Llama-3-monika-ddlc-8b-v1,mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF,"base_model: 922-CA/Llama-3-monika-ddlc-8b-v1
datasets:
- 922-CA/MoCha_v1a
language:
- en
library_name: transformers
license: other
license_link: LICENSE
license_name: llama3
quantized_by: mradermacher
tags:
- unsloth
- trl
- sft","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags: nicoboss -->
static quants of https://huggingface.co/922-CA/Llama-3-monika-ddlc-8b-v1

<!-- provided-files -->
weighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q2_K.gguf) | Q2_K | 3.3 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q3_K_S.gguf) | Q3_K_S | 3.8 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q3_K_M.gguf) | Q3_K_M | 4.1 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q3_K_L.gguf) | Q3_K_L | 4.4 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.IQ4_XS.gguf) | IQ4_XS | 4.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q4_K_S.gguf) | Q4_K_S | 4.8 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q4_K_M.gguf) | Q4_K_M | 5.0 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q5_K_S.gguf) | Q5_K_S | 5.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q5_K_M.gguf) | Q5_K_M | 5.8 |  |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q6_K.gguf) | Q6_K | 6.7 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.Q8_0.gguf) | Q8_0 | 8.6 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/Llama-3-monika-ddlc-8b-v1-GGUF/resolve/main/Llama-3-monika-ddlc-8b-v1.f16.gguf) | f16 | 16.2 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->",3645.0,4.917869820306261e-06
mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF,6732a228670093d684080a4a,mradermacher,"{""base_model"": ""rawsh/mirrorqwen2.5-0.5b-SimPO-1"", ""language"": [""en""], ""library_name"": ""transformers"", ""model_name"": ""mirrorqwen2.5-0.5b-SimPO-1"", ""quantized_by"": ""mradermacher"", ""tags"": [""generated_from_trainer"", ""trl"", ""cpo"", ""unsloth""]}",[],2024-11-12 00:34:39,0,0.0,,34,1510,,"['transformers' 'gguf' 'generated_from_trainer' 'trl' 'cpo' 'unsloth' 'en'
 'base_model:rawsh/mirrorqwen2.5-0.5b-SimPO-1'
 'base_model:quantized:rawsh/mirrorqwen2.5-0.5b-SimPO-1'
 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.IQ4_XS.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q2_K.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q3_K_L.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q3_K_M.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q3_K_S.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q4_0_4_4.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q4_K_M.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q4_K_S.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q5_K_M.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q5_K_S.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q6_K.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.Q8_0.gguf'}
 {'rfilename': 'mirrorqwen2.5-0.5b-SimPO-1.f16.gguf'}]",2024-11-12 00:32:40,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 494032768, ""architecture"": ""qwen2"", ""context_length"": 32768, ""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|endoftext|>""}",quantized,6731458d38387b9eb2a88673,rawsh/mirrorqwen2.5-0.5b-SimPO-1,mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF,"base_model: rawsh/mirrorqwen2.5-0.5b-SimPO-1
language:
- en
library_name: transformers
model_name: mirrorqwen2.5-0.5b-SimPO-1
quantized_by: mradermacher
tags:
- generated_from_trainer
- trl
- cpo
- unsloth","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/rawsh/mirrorqwen2.5-0.5b-SimPO-1

<!-- provided-files -->
weighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q3_K_S.gguf) | Q3_K_S | 0.4 |  |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q2_K.gguf) | Q2_K | 0.4 |  |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.IQ4_XS.gguf) | IQ4_XS | 0.5 |  |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q4_0_4_4.gguf) | Q4_0_4_4 | 0.5 | fast on arm, low quality |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q3_K_M.gguf) | Q3_K_M | 0.5 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q3_K_L.gguf) | Q3_K_L | 0.5 |  |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q4_K_S.gguf) | Q4_K_S | 0.5 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q4_K_M.gguf) | Q4_K_M | 0.5 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q5_K_S.gguf) | Q5_K_S | 0.5 |  |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q5_K_M.gguf) | Q5_K_M | 0.5 |  |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q6_K.gguf) | Q6_K | 0.6 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.Q8_0.gguf) | Q8_0 | 0.6 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/mirrorqwen2.5-0.5b-SimPO-1-GGUF/resolve/main/mirrorqwen2.5-0.5b-SimPO-1.f16.gguf) | f16 | 1.1 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3613.0,4.9090519780523765e-06
DrRos/bge-reranker-large-Q4_K_M-GGUF,673f8c1960d701bba708de37,DrRos,"{""license"": ""mit"", ""language"": [""en"", ""zh""], ""tags"": [""mteb"", ""llama-cpp"", ""gguf-my-repo""], ""pipeline_tag"": ""feature-extraction"", ""base_model"": ""BAAI/bge-reranker-large"", ""model-index"": [{""name"": ""bge-reranker-base"", ""results"": [{""task"": {""type"": ""Reranking""}, ""dataset"": {""name"": ""MTEB CMedQAv1"", ""type"": ""C-MTEB/CMedQAv1-reranking"", ""config"": ""default"", ""split"": ""test"", ""revision"": ""None""}, ""metrics"": [{""type"": ""map"", ""value"": 81.27206722525007, ""verified"": false}, {""type"": ""mrr"", ""value"": 84.14238095238095, ""verified"": false}]}, {""task"": {""type"": ""Reranking""}, ""dataset"": {""name"": ""MTEB CMedQAv2"", ""type"": ""C-MTEB/CMedQAv2-reranking"", ""config"": ""default"", ""split"": ""test"", ""revision"": ""None""}, ""metrics"": [{""type"": ""map"", ""value"": 84.10369934291236, ""verified"": false}, {""type"": ""mrr"", ""value"": 86.79376984126984, ""verified"": false}]}, {""task"": {""type"": ""Reranking""}, ""dataset"": {""name"": ""MTEB MMarcoReranking"", ""type"": ""C-MTEB/Mmarco-reranking"", ""config"": ""default"", ""split"": ""dev"", ""revision"": ""None""}, ""metrics"": [{""type"": ""map"", ""value"": 35.4600511272538, ""verified"": false}, {""type"": ""mrr"", ""value"": 34.60238095238095, ""verified"": false}]}, {""task"": {""type"": ""Reranking""}, ""dataset"": {""name"": ""MTEB T2Reranking"", ""type"": ""C-MTEB/T2Reranking"", ""config"": ""default"", ""split"": ""dev"", ""revision"": ""None""}, ""metrics"": [{""type"": ""map"", ""value"": 67.27728847727172, ""verified"": false}, {""type"": ""mrr"", ""value"": 77.1315192743764, ""verified"": false}]}]}]}",[],2024-11-21 19:38:06,1,0.0,,101,1441,,"['gguf' 'mteb' 'llama-cpp' 'gguf-my-repo' 'feature-extraction' 'en' 'zh'
 'base_model:BAAI/bge-reranker-large'
 'base_model:quantized:BAAI/bge-reranker-large' 'license:mit'
 'model-index' 'endpoints_compatible' 'region:us']",feature-extraction,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'bge-reranker-large-q4_k_m.gguf'}]",2024-11-21 19:38:01,,,"{""total"": 559889409, ""architecture"": ""bert"", ""context_length"": 512, ""causal"": false, ""bos_token"": ""<s>"", ""eos_token"": ""</s>""}",quantized,650015a612c1442d9930ad41,BAAI/bge-reranker-large,DrRos/bge-reranker-large-Q4_K_M-GGUF,"license: mit
language:
- en
- zh
tags:
- mteb
- llama-cpp
- gguf-my-repo
pipeline_tag: feature-extraction
base_model: BAAI/bge-reranker-large
model-index:
- name: bge-reranker-base
  results:
  - task:
      type: Reranking
    dataset:
      name: MTEB CMedQAv1
      type: C-MTEB/CMedQAv1-reranking
      config: default
      split: test
      revision: None
    metrics:
    - type: map
      value: 81.27206722525007
    - type: mrr
      value: 84.14238095238095
  - task:
      type: Reranking
    dataset:
      name: MTEB CMedQAv2
      type: C-MTEB/CMedQAv2-reranking
      config: default
      split: test
      revision: None
    metrics:
    - type: map
      value: 84.10369934291236
    - type: mrr
      value: 86.79376984126984
  - task:
      type: Reranking
    dataset:
      name: MTEB MMarcoReranking
      type: C-MTEB/Mmarco-reranking
      config: default
      split: dev
      revision: None
    metrics:
    - type: map
      value: 35.4600511272538
    - type: mrr
      value: 34.60238095238095
  - task:
      type: Reranking
    dataset:
      name: MTEB T2Reranking
      type: C-MTEB/T2Reranking
      config: default
      split: dev
      revision: None
    metrics:
    - type: map
      value: 67.27728847727172
    - type: mrr
      value: 77.1315192743764","# DrRos/bge-reranker-large-Q4_K_M-GGUF
This model was converted to GGUF format from [`BAAI/bge-reranker-large`](https://huggingface.co/BAAI/bge-reranker-large) using llama.cpp via the ggml.ai's [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space.
Refer to the [original model card](https://huggingface.co/BAAI/bge-reranker-large) for more details on the model.

## Use with llama.cpp
Install llama.cpp through brew (works on Mac and Linux)

```bash
brew install llama.cpp

```
Invoke the llama.cpp server or the CLI.

### CLI:
```bash
llama-cli --hf-repo DrRos/bge-reranker-large-Q4_K_M-GGUF --hf-file bge-reranker-large-q4_k_m.gguf -p ""The meaning to life and the universe is""
```

### Server:
```bash
llama-server --hf-repo DrRos/bge-reranker-large-Q4_K_M-GGUF --hf-file bge-reranker-large-q4_k_m.gguf -c 2048
```

Note: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.

Step 1: Clone llama.cpp from GitHub.
```
git clone https://github.com/ggerganov/llama.cpp
```

Step 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).
```
cd llama.cpp && LLAMA_CURL=1 make
```

Step 3: Run inference through the main binary.
```
./llama-cli --hf-repo DrRos/bge-reranker-large-Q4_K_M-GGUF --hf-file bge-reranker-large-q4_k_m.gguf -p ""The meaning to life and the universe is""
```
or 
```
./llama-server --hf-repo DrRos/bge-reranker-large-Q4_K_M-GGUF --hf-file bge-reranker-large-q4_k_m.gguf -c 2048
```",1644.0,4.877708298264506e-06
InferenceIllusionist/Mistral-RealworldQA-v0.2-7b-SFT,66207655b760cd2e98a426d4,InferenceIllusionist,"{""language"": [""en""], ""license"": ""apache-2.0"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""mistral"", ""trl"", ""sft""], ""base_model"": ""unsloth/mistral-7b-v0.2-bnb-4bit"", ""datasets"": [""visheratin/realworldqa""]}",[],2024-04-18 04:38:47,1,0.0,"{""architectures"": [""MistralForCausalLM""], ""model_type"": ""mistral"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}",20,1254,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': 7241732096, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 7241732096.0}","['transformers' 'safetensors' 'gguf' 'mistral' 'text-generation'
 'text-generation-inference' 'unsloth' 'trl' 'sft' 'en'
 'dataset:visheratin/realworldqa'
 'base_model:unsloth/mistral-7b-v0.2-bnb-4bit'
 'base_model:quantized:unsloth/mistral-7b-v0.2-bnb-4bit'
 'license:apache-2.0' 'autotrain_compatible' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'mistral-7b-mmproj-v1.5-Q4_1.gguf'}
 {'rfilename': 'mmproj-model-f16.gguf'}
 {'rfilename': 'model-00001-of-00003.safetensors'}
 {'rfilename': 'model-00002-of-00003.safetensors'}
 {'rfilename': 'model-00003-of-00003.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2024-04-18 01:24:37,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}","{""total"": 311888896, ""architecture"": ""clip""}",quantized,65ff91ea321ac5cd34ffd85d,unsloth/mistral-7b-v0.2-bnb-4bit,InferenceIllusionist/Mistral-RealworldQA-v0.2-7b-SFT,"language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- mistral
- trl
- sft
base_model: unsloth/mistral-7b-v0.2-bnb-4bit
datasets:
- visheratin/realworldqa","# Mistral-RealworldQA-v0.2-7b SFT

<img src=""https://i.imgur.com/Pf53ms5.jpeg"" width=""400""/>

GGUFs can be found [here](https://huggingface.co/InferenceIllusionist/Mistral-RealworldQA-v0.2-7b-SFT-GGUF)


An experiment with the goal of reducing hallucinations in [VQA](https://huggingface.co/tasks/visual-question-answering)

First in a series of experiments centering around fine-tuning for image captioning.

<h1>Release Notes</h1>

* v0.1 - Initial Release
* <b>v0.2</b> (Current)- Updating base model to official Mistral-7b fp16 release, refinements to dataset and instruction formating

<h2>Background & Methodology</h2>

Mistral-7b-02 base model was fine-tuned using the [RealWorldQA dataset](https://huggingface.co/datasets/visheratin/realworldqa), originally provided by the X.Ai Team here: https://x.ai/blog/grok-1.5v

<h1>Vision Results</h1>

Example 1
<img src=""https://i.imgur.com/E9mS4Xb.jpeg"" width=""400""/>
Example 2
<img src=""https://i.imgur.com/SmTz1Yd.jpeg"" width=""400""/>

* Experiment yielded model that provides shorter, less verbose output for questions about pictures
* The likelihood of hallucinations in output has decreased, however, the model can still be easily influenced to be inaccurate by the user
* Best suited for captioning use cases that require concise descriptions and low token counts
* This model lacks the conversational prose of Excalibur-7b-DPO and is much ""drier"" in tone

<b>Requires additional mmproj file. You have two options for vision functionality (available inside this repo):</b>
 1. [Quantized - Limited VRAM Option (197mb)](https://huggingface.co/InferenceIllusionist/Mistral-RealworldQA-v0.2-7b-SFT/resolve/main/mistral-7b-mmproj-v1.5-Q4_1.gguf?download=true)
 2. [Unquantized - Premium Option / Best Quality (596mb)](https://huggingface.co/InferenceIllusionist/Mistral-RealworldQA-v0.2-7b-SFT/resolve/main/mmproj-model-f16.gguf?download=true)

Select the gguf file of your choice in [Koboldcpp](https://github.com/LostRuins/koboldcpp/releases/) as usual, then make sure to choose the mmproj file above in the LLaVA mmproj field of the model submenu:
<img src=""https://i.imgur.com/x8vqH29.png"" width=""425""/>

## Prompt Format
Use Alpaca for best results.


## Other info
- **Developed by:** InferenceIllusionist
- **License:** apache-2.0
- **Finetuned from model :** mistral-community/Mistral-7B-v0.2

This mistral model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)",2642.0,4.78456677074969e-06
Saxo/Linkbricks-Horizon-AI-Llama-3.3-Korean-70B-sft-dpo,675e374c77e9469be3e6ae81,Saxo,"{""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""base_model"": ""meta-llama/Llama-3.3-70B-Instruct"", ""datasets"": [""Saxo/ko_cn_translation_tech_social_science_linkbricks_single_dataset"", ""Saxo/ko_jp_translation_tech_social_science_linkbricks_single_dataset"", ""Saxo/en_ko_translation_tech_science_linkbricks_single_dataset_with_prompt_text_huggingface"", ""Saxo/en_ko_translation_social_science_linkbricks_single_dataset_with_prompt_text_huggingface"", ""Saxo/ko_aspect_sentiment_sns_mall_sentiment_linkbricks_single_dataset_with_prompt_text_huggingface"", ""Saxo/ko_summarization_linkbricks_single_dataset_with_prompt_text_huggingface"", ""Saxo/OpenOrca_cleaned_kor_linkbricks_single_dataset_with_prompt_text_huggingface"", ""Saxo/ko_government_qa_total_linkbricks_single_dataset_with_prompt_text_huggingface_sampled"", ""Saxo/ko-news-corpus-1"", ""Saxo/ko-news-corpus-2"", ""Saxo/ko-news-corpus-3"", ""Saxo/ko-news-corpus-4"", ""Saxo/ko-news-corpus-5"", ""Saxo/ko-news-corpus-6"", ""Saxo/ko-news-corpus-7"", ""Saxo/ko-news-corpus-8"", ""Saxo/ko-news-corpus-9"", ""maywell/ko_Ultrafeedback_binarized"", ""youjunhyeok/ko-orca-pair-and-ultrafeedback-dpo"", ""lilacai/glaive-function-calling-v2-sharegpt"", ""kuotient/gsm8k-ko""], ""language"": [""ko"", ""en"", ""jp"", ""cn""], ""pipeline_tag"": ""text-generation""}",[],2025-02-24 08:53:55,6,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""chat_template"": ""{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \""26 Jul 2024\"" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \""\"" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \""<|start_header_id|>system<|end_header_id|>\\n\\n\"" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \""Environment: ipython\\n\"" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \""Tools: \"" + builtin_tools | reject('equalto', 'code_interpreter') | join(\"", \"") + \""\\n\\n\""}}\n{%- endif %}\n{{- \""Cutting Knowledge Date: December 2023\\n\"" }}\n{{- \""Today Date: \"" + date_string + \""\\n\\n\"" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \""You have access to the following functions. To call a function, please respond with JSON for a function call.\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \""<|eot_id|>\"" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\""Cannot put tools in the first user message when there's no first user message!\"") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \""Given the following functions, please respond with a JSON for a function call \"" }}\n    {{- \""with its proper arguments that best answers the given prompt.\\n\\n\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n    {{- first_user_message + \""<|eot_id|>\""}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\""This model only supports single tool-calls at once!\"") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \""<|python_tag|>\"" + tool_call.name + \"".call(\"" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\""' + arg_val + '\""' }}\n                {%- if not loop.last %}\n                    {{- \"", \"" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \"")\"" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\""name\"": \""' + tool_call.name + '\"", ' }}\n            {{- '\""parameters\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \""}\"" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \""<|eom_id|>\"" }}\n        {%- else %}\n            {{- \""<|eot_id|>\"" }}\n        {%- endif %}\n    {%- elif message.role == \""tool\"" or message.role == \""ipython\"" %}\n        {{- \""<|start_header_id|>ipython<|end_header_id|>\\n\\n\"" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \""<|eot_id|>\"" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|eot_id|>"", ""pad_token"": ""<|eot_id|>""}}",70,1110,"{'parameters': {'BF16': 70553706496.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 70553706496.0}","['transformers' 'safetensors' 'llama' 'text-generation' 'conversational'
 'ko' 'en' 'jp' 'cn'
 'dataset:Saxo/ko_cn_translation_tech_social_science_linkbricks_single_dataset'
 'dataset:Saxo/ko_jp_translation_tech_social_science_linkbricks_single_dataset'
 'dataset:Saxo/en_ko_translation_tech_science_linkbricks_single_dataset_with_prompt_text_huggingface'
 'dataset:Saxo/en_ko_translation_social_science_linkbricks_single_dataset_with_prompt_text_huggingface'
 'dataset:Saxo/ko_aspect_sentiment_sns_mall_sentiment_linkbricks_single_dataset_with_prompt_text_huggingface'
 'dataset:Saxo/ko_summarization_linkbricks_single_dataset_with_prompt_text_huggingface'
 'dataset:Saxo/OpenOrca_cleaned_kor_linkbricks_single_dataset_with_prompt_text_huggingface'
 'dataset:Saxo/ko_government_qa_total_linkbricks_single_dataset_with_prompt_text_huggingface_sampled'
 'dataset:Saxo/ko-news-corpus-1' 'dataset:Saxo/ko-news-corpus-2'
 'dataset:Saxo/ko-news-corpus-3' 'dataset:Saxo/ko-news-corpus-4'
 'dataset:Saxo/ko-news-corpus-5' 'dataset:Saxo/ko-news-corpus-6'
 'dataset:Saxo/ko-news-corpus-7' 'dataset:Saxo/ko-news-corpus-8'
 'dataset:Saxo/ko-news-corpus-9'
 'dataset:maywell/ko_Ultrafeedback_binarized'
 'dataset:youjunhyeok/ko-orca-pair-and-ultrafeedback-dpo'
 'dataset:lilacai/glaive-function-calling-v2-sharegpt'
 'dataset:kuotient/gsm8k-ko'
 'base_model:meta-llama/Llama-3.3-70B-Instruct'
 'base_model:finetune:meta-llama/Llama-3.3-70B-Instruct'
 'license:apache-2.0' 'autotrain_compatible' 'text-generation-inference'
 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model-00001-of-00030.safetensors'}
 {'rfilename': 'model-00002-of-00030.safetensors'}
 {'rfilename': 'model-00003-of-00030.safetensors'}
 {'rfilename': 'model-00004-of-00030.safetensors'}
 {'rfilename': 'model-00005-of-00030.safetensors'}
 {'rfilename': 'model-00006-of-00030.safetensors'}
 {'rfilename': 'model-00007-of-00030.safetensors'}
 {'rfilename': 'model-00008-of-00030.safetensors'}
 {'rfilename': 'model-00009-of-00030.safetensors'}
 {'rfilename': 'model-00010-of-00030.safetensors'}
 {'rfilename': 'model-00011-of-00030.safetensors'}
 {'rfilename': 'model-00012-of-00030.safetensors'}
 {'rfilename': 'model-00013-of-00030.safetensors'}
 {'rfilename': 'model-00014-of-00030.safetensors'}
 {'rfilename': 'model-00015-of-00030.safetensors'}
 {'rfilename': 'model-00016-of-00030.safetensors'}
 {'rfilename': 'model-00017-of-00030.safetensors'}
 {'rfilename': 'model-00018-of-00030.safetensors'}
 {'rfilename': 'model-00019-of-00030.safetensors'}
 {'rfilename': 'model-00020-of-00030.safetensors'}
 {'rfilename': 'model-00021-of-00030.safetensors'}
 {'rfilename': 'model-00022-of-00030.safetensors'}
 {'rfilename': 'model-00023-of-00030.safetensors'}
 {'rfilename': 'model-00024-of-00030.safetensors'}
 {'rfilename': 'model-00025-of-00030.safetensors'}
 {'rfilename': 'model-00026-of-00030.safetensors'}
 {'rfilename': 'model-00027-of-00030.safetensors'}
 {'rfilename': 'model-00028-of-00030.safetensors'}
 {'rfilename': 'model-00029-of-00030.safetensors'}
 {'rfilename': 'model-00030-of-00030.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2024-12-15 01:56:28,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,6745f28f9333dfcc06268b1e,meta-llama/Llama-3.3-70B-Instruct,Saxo/Linkbricks-Horizon-AI-Llama-3.3-Korean-70B-sft-dpo,"library_name: transformers
license: apache-2.0
base_model: meta-llama/Llama-3.3-70B-Instruct
datasets:
- Saxo/ko_cn_translation_tech_social_science_linkbricks_single_dataset
- Saxo/ko_jp_translation_tech_social_science_linkbricks_single_dataset
- Saxo/en_ko_translation_tech_science_linkbricks_single_dataset_with_prompt_text_huggingface
- Saxo/en_ko_translation_social_science_linkbricks_single_dataset_with_prompt_text_huggingface
- Saxo/ko_aspect_sentiment_sns_mall_sentiment_linkbricks_single_dataset_with_prompt_text_huggingface
- Saxo/ko_summarization_linkbricks_single_dataset_with_prompt_text_huggingface
- Saxo/OpenOrca_cleaned_kor_linkbricks_single_dataset_with_prompt_text_huggingface
- Saxo/ko_government_qa_total_linkbricks_single_dataset_with_prompt_text_huggingface_sampled
- Saxo/ko-news-corpus-1
- Saxo/ko-news-corpus-2
- Saxo/ko-news-corpus-3
- Saxo/ko-news-corpus-4
- Saxo/ko-news-corpus-5
- Saxo/ko-news-corpus-6
- Saxo/ko-news-corpus-7
- Saxo/ko-news-corpus-8
- Saxo/ko-news-corpus-9
- maywell/ko_Ultrafeedback_binarized
- youjunhyeok/ko-orca-pair-and-ultrafeedback-dpo
- lilacai/glaive-function-calling-v2-sharegpt
- kuotient/gsm8k-ko
language:
- ko
- en
- jp
- cn
pipeline_tag: text-generation","# Model Card for Model ID

<div align=""center"">
<img src=""http://www.linkbricks.com/wp-content/uploads/2024/11/fulllogo.png"" />
</div>

AI 와 빅데이터 분석 전문 기업인 Linkbricks의 데이터사이언티스트인 지윤성(Saxo) 이사가 <br>
meta-llama/Llama-3.3-70B-Instruct 베이스모델을 사용해서 H100-80G 8개를 통해 한국어 SFT->DPO  한 한국어 강화 언어 모델<br>
4천만건의 한글 뉴스 및 위키 코퍼스를 기준으로 다양한 테스크별 한국어-일본어-중국어-영어 교차 학습 데이터와 수학 및 논리판단 데이터를 통하여 한중일영 언어 교차 증강 처리와 복잡한 논리 문제 역시 대응 가능하도록 훈련한 모델이다.<br> 
-토크나이저는 단어 확장 없이 베이스 모델 그대로 사용<br>
-고객 리뷰나 소셜 포스팅 고차원 분석 및 코딩과 작문, 수학, 논리판단 등이 강화된 모델<br>
-128k-Context Window<br>
-Function Call 및 Tool Calling 지원<br>
-128k-Context Window<br>
-Deepspeed Stage=3, rslora 및 BAdam Layer Mode 사용 <br>
-""transformers_version"": ""4.46.3""<br>
<br><br>

Finetuned by Mr. Yunsung Ji (Saxo), a data scientist at Linkbricks, a company specializing in AI and big data analytics <br>
Korean SFT->DPO training model based on Saxo/Linkbricks-Horizon-AI-Japanese-Base-70B through 8 H100-80Gs as a Korean  boosting language model <br>
It is a model that has been trained to handle Korean-Japanese-Chinese-English cross-training data and 40M  Korean news corpus and logic judgment data for various tasks to enable cross-fertilization processing and complex Korean logic & math problems. <br>
-Tokenizer uses the base model without word expansion<br>
-Models enhanced with high-dimensional analysis of customer reviews and social posts, as well as coding, writing, math and decision making<br>
-Function Calling<br>
-128k-Context Window<br>
-Deepspeed Stage=3, use rslora and BAdam Layer Mode<br>
<br><br>

<a href=""www.horizonai.ai"">www.horizonai.ai</a>, <a href=""www.linkbricks.com"">www.linkbricks.com</a>, <a href=""www.linkbricks.vc"">www.linkbricks.vc</a>",1701.0,4.702838909229923e-06
mradermacher/blossom-v3_1-baichuan2-7b-GGUF,67292e5ea92915e9aeaf129c,mradermacher,"{""base_model"": ""Azure99/blossom-v3_1-baichuan2-7b"", ""datasets"": [""Azure99/blossom-chat-v1"", ""Azure99/blossom-math-v2"", ""Azure99/blossom-wizard-v1"", ""Azure99/blossom-orca-v1""], ""language"": [""zh"", ""en""], ""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""quantized_by"": ""mradermacher""}",[],2024-11-04 20:43:08,0,0.0,,40,1017,,"['transformers' 'gguf' 'zh' 'en' 'dataset:Azure99/blossom-chat-v1'
 'dataset:Azure99/blossom-math-v2' 'dataset:Azure99/blossom-wizard-v1'
 'dataset:Azure99/blossom-orca-v1'
 'base_model:Azure99/blossom-v3_1-baichuan2-7b'
 'base_model:quantized:Azure99/blossom-v3_1-baichuan2-7b'
 'license:apache-2.0' 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.IQ4_XS.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q2_K.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q3_K_L.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q3_K_M.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q3_K_S.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q4_0_4_4.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q4_K_M.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q4_K_S.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q5_K_M.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q5_K_S.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q6_K.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.Q8_0.gguf'}
 {'rfilename': 'blossom-v3_1-baichuan2-7b.f16.gguf'}]",2024-11-04 20:28:14,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 7505973248, ""architecture"": ""baichuan"", ""context_length"": 4096, ""bos_token"": ""<s>"", ""eos_token"": ""</s>""}",quantized,65640e0a7ff2e1b1cfdcedb4,Azure99/blossom-v3_1-baichuan2-7b,mradermacher/blossom-v3_1-baichuan2-7b-GGUF,"base_model: Azure99/blossom-v3_1-baichuan2-7b
datasets:
- Azure99/blossom-chat-v1
- Azure99/blossom-math-v2
- Azure99/blossom-wizard-v1
- Azure99/blossom-orca-v1
language:
- zh
- en
library_name: transformers
license: apache-2.0
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/Azure99/blossom-v3_1-baichuan2-7b

<!-- provided-files -->
weighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q2_K.gguf) | Q2_K | 3.1 |  |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q3_K_S.gguf) | Q3_K_S | 3.5 |  |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q3_K_M.gguf) | Q3_K_M | 3.9 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q3_K_L.gguf) | Q3_K_L | 4.2 |  |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.IQ4_XS.gguf) | IQ4_XS | 4.3 |  |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q4_0_4_4.gguf) | Q4_0_4_4 | 4.5 | fast on arm, low quality |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q4_K_S.gguf) | Q4_K_S | 4.5 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q4_K_M.gguf) | Q4_K_M | 4.7 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q5_K_S.gguf) | Q5_K_S | 5.3 |  |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q5_K_M.gguf) | Q5_K_M | 5.5 |  |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q6_K.gguf) | Q6_K | 6.3 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.Q8_0.gguf) | Q8_0 | 8.1 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/blossom-v3_1-baichuan2-7b-GGUF/resolve/main/blossom-v3_1-baichuan2-7b.f16.gguf) | f16 | 15.1 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3589.0,4.644215775282124e-06
mradermacher/OuteTTS-0.3-1B-GGUF,67c0a11829dd16b8f25b960a,mradermacher,"{""base_model"": ""OuteAI/OuteTTS-0.3-1B"", ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""cc-by-nc-sa-4.0"", ""quantized_by"": ""mradermacher""}",[],2025-02-27 18:00:09,1,0.0,,20,989,,"['transformers' 'gguf' 'en' 'base_model:OuteAI/OuteTTS-0.3-1B'
 'base_model:quantized:OuteAI/OuteTTS-0.3-1B' 'license:cc-by-nc-sa-4.0'
 'endpoints_compatible' 'region:us' 'conversational']",,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'OuteTTS-0.3-1B.IQ4_XS.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q2_K.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q3_K_L.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q3_K_M.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q3_K_S.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q4_K_M.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q4_K_S.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q5_K_M.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q5_K_S.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q6_K.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.Q8_0.gguf'}
 {'rfilename': 'OuteTTS-0.3-1B.f16.gguf'} {'rfilename': 'README.md'}]",2025-02-27 17:30:00,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 1191182336, ""architecture"": ""olmo"", ""context_length"": 4096, ""chat_template"": ""outetts-0.3"", ""bos_token"": ""<|im_start|>"", ""eos_token"": ""<|im_end|>""}",quantized,6784f47a2cff942988892801,OuteAI/OuteTTS-0.3-1B,mradermacher/OuteTTS-0.3-1B-GGUF,"base_model: OuteAI/OuteTTS-0.3-1B
language:
- en
library_name: transformers
license: cc-by-nc-sa-4.0
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/OuteAI/OuteTTS-0.3-1B

<!-- provided-files -->
weighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q2_K.gguf) | Q2_K | 0.6 |  |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q3_K_S.gguf) | Q3_K_S | 0.7 |  |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q3_K_M.gguf) | Q3_K_M | 0.7 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q3_K_L.gguf) | Q3_K_L | 0.8 |  |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.IQ4_XS.gguf) | IQ4_XS | 0.8 |  |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q4_K_S.gguf) | Q4_K_S | 0.8 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q4_K_M.gguf) | Q4_K_M | 0.8 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q5_K_S.gguf) | Q5_K_S | 0.9 |  |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q5_K_M.gguf) | Q5_K_M | 1.0 |  |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q6_K.gguf) | Q6_K | 1.1 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.Q8_0.gguf) | Q8_0 | 1.4 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/OuteTTS-0.3-1B-GGUF/resolve/main/OuteTTS-0.3-1B.f16.gguf) | f16 | 2.5 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3136.0,4.625512926814589e-06
monsterapi/Gptj-6b_alpaca-gpt4,649bd6bd1b167f96a5f9507f,monsterapi,"{""library_name"": ""peft"", ""tags"": [""gptj-6b"", ""instruct"", ""instruct-alpaca"", ""alpaca"", ""gpt4""], ""datasets"": [""vicgalle/alpaca-gpt4""], ""base_model"": ""EleutherAI/gpt-j-6b""}",[],2023-09-19 10:45:10,0,0.0,"{""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<|endoftext|>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""<|endoftext|>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<|endoftext|>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}, ""peft"": {""base_model_name_or_path"": ""EleutherAI/gpt-j-6b"", ""task_type"": ""CAUSAL_LM""}}",9,980,,"['peft' 'gptj-6b' 'instruct' 'instruct-alpaca' 'alpaca' 'gpt4'
 'dataset:vicgalle/alpaca-gpt4' 'base_model:EleutherAI/gpt-j-6b'
 'base_model:adapter:EleutherAI/gpt-j-6b' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2023-06-28 06:44:13,peft,,,adapter,621ffdc036468d709f1753ba,EleutherAI/gpt-j-6b,monsterapi/Gptj-6b_alpaca-gpt4,"library_name: peft
tags:
- gptj-6b
- instruct
- instruct-alpaca
- alpaca
- gpt4
datasets:
- vicgalle/alpaca-gpt4
base_model: EleutherAI/gpt-j-6b","We finetuned gptj-6b on Code-Alpaca-Instruct Dataset (vicgalle/alpaca-gpt4) for 10 epochs or ~ 50,000 steps using [MonsterAPI](https://monsterapi.ai) no-code [LLM finetuner](https://docs.monsterapi.ai/fine-tune-a-large-language-model-llm).

This dataset is vicgalle/alpaca-gpt4 unfiltered, 

The finetuning session got completed in 7  hours and costed us only `$25` for the entire finetuning run!

#### Hyperparameters & Run details:
- Model Path: vicgalle/alpaca-gpt4
- Dataset: vicgalle/alpaca-gpt4
- Learning rate: 0.0003
- Number of epochs: 5
- Data split: Training: 90% / Validation: 10%
- Gradient accumulation steps: 1


---
license: apache-2.0
---",655.0,4.61938879985872e-06
linoyts/huggy_edm_lora_v7,65f05efe8d645ac095ee1dde,linoyts,"{""tags"": [""stable-diffusion-xl"", ""stable-diffusion-xl-diffusers"", ""diffusers-training"", ""text-to-image"", ""diffusers"", ""lora"", ""template:sd-lora""], ""widget"": [{""text"": ""a <s0><s1> emoji dressed as yoda"", ""output"": {""url"": ""https://huggingface.co/linoyts/huggy_edm_lora_v7/resolve/main/image_0.png""}}, {""text"": ""a <s0><s1> emoji dressed as yoda"", ""output"": {""url"": ""https://huggingface.co/linoyts/huggy_edm_lora_v7/resolve/main/image_1.png""}}, {""text"": ""a <s0><s1> emoji dressed as yoda"", ""output"": {""url"": ""https://huggingface.co/linoyts/huggy_edm_lora_v7/resolve/main/image_2.png""}}, {""text"": ""a <s0><s1> emoji dressed as yoda"", ""output"": {""url"": ""https://huggingface.co/linoyts/huggy_edm_lora_v7/resolve/main/image_3.png""}}], ""base_model"": ""stabilityai/stable-diffusion-xl-base-1.0"", ""instance_prompt"": ""a <s0><s1> emoji"", ""license"": ""openrail++""}","[{'_id': '6810ad085eb1bbf3dd3ae5d5', 'adapter': 'lora', 'adapterWeightsPath': 'pytorch_lora_weights.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/fast-sdxl', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2024-03-12 14:25:45,0,0.0,,6,952,,"['diffusers' 'stable-diffusion-xl' 'stable-diffusion-xl-diffusers'
 'diffusers-training' 'text-to-image' 'lora' 'template:sd-lora'
 'base_model:stabilityai/stable-diffusion-xl-base-1.0'
 'base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0'
 'license:openrail++' 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'huggy_edm_lora_v7.safetensors'}
 {'rfilename': 'huggy_edm_lora_v7_emb.safetensors'}
 {'rfilename': 'image_0.png'} {'rfilename': 'image_1.png'}
 {'rfilename': 'image_2.png'} {'rfilename': 'image_3.png'}
 {'rfilename': 'pytorch_lora_weights.safetensors'}]",2024-03-12 13:56:14,diffusers,,,adapter,64bfcd5ff462a99a04fd1ec8,stabilityai/stable-diffusion-xl-base-1.0,linoyts/huggy_edm_lora_v7,"tags:
- stable-diffusion-xl
- stable-diffusion-xl-diffusers
- diffusers-training
- text-to-image
- diffusers
- lora
- template:sd-lora
widget:

        - text: 'a <s0><s1> emoji dressed as yoda'
          output:
            url:
                ""image_0.png""
        
        - text: 'a <s0><s1> emoji dressed as yoda'
          output:
            url:
                ""image_1.png""
        
        - text: 'a <s0><s1> emoji dressed as yoda'
          output:
            url:
                ""image_2.png""
        
        - text: 'a <s0><s1> emoji dressed as yoda'
          output:
            url:
                ""image_3.png""
        
base_model: stabilityai/stable-diffusion-xl-base-1.0
instance_prompt: a <s0><s1> emoji
license: openrail++","# SDXL LoRA DreamBooth - linoyts/huggy_edm_lora_v7

<Gallery />

## Model description

### These are linoyts/huggy_edm_lora_v7 LoRA adaption weights for stabilityai/stable-diffusion-xl-base-1.0.

## Download model

### Use it with UIs such as AUTOMATIC1111, Comfy UI, SD.Next, Invoke

- **LoRA**: download **[`huggy_edm_lora_v7.safetensors` here 💾](/linoyts/huggy_edm_lora_v7/blob/main/huggy_edm_lora_v7.safetensors)**.
    - Place it on your `models/Lora` folder.
    - On AUTOMATIC1111, load the LoRA by adding `<lora:huggy_edm_lora_v7:1>` to your prompt. On ComfyUI just [load it as a regular LoRA](https://comfyanonymous.github.io/ComfyUI_examples/lora/).
- *Embeddings*: download **[`huggy_edm_lora_v7_emb.safetensors` here 💾](/linoyts/huggy_edm_lora_v7/blob/main/huggy_edm_lora_v7_emb.safetensors)**.
    - Place it on it on your `embeddings` folder
    - Use it by adding `huggy_edm_lora_v7_emb` to your prompt. For example, `a huggy_edm_lora_v7_emb emoji`
    (you need both the LoRA and the embeddings as they were trained together for this LoRA)
    

## Use it with the [🧨 diffusers library](https://github.com/huggingface/diffusers)

```py
from diffusers import AutoPipelineForText2Image
import torch
from huggingface_hub import hf_hub_download
from safetensors.torch import load_file
        
pipeline = AutoPipelineForText2Image.from_pretrained('stabilityai/stable-diffusion-xl-base-1.0', torch_dtype=torch.float16).to('cuda')
pipeline.load_lora_weights('linoyts/huggy_edm_lora_v7', weight_name='pytorch_lora_weights.safetensors')
embedding_path = hf_hub_download(repo_id='linoyts/huggy_edm_lora_v7', filename='huggy_edm_lora_v7_emb.safetensors', repo_type=""model"")
state_dict = load_file(embedding_path)
pipeline.load_textual_inversion(state_dict[""clip_l""], token=[""<s0>"", ""<s1>""], text_encoder=pipeline.text_encoder, tokenizer=pipeline.tokenizer)
pipeline.load_textual_inversion(state_dict[""clip_g""], token=[""<s0>"", ""<s1>""], text_encoder=pipeline.text_encoder_2, tokenizer=pipeline.tokenizer_2)
        
image = pipeline('a <s0><s1> emoji dressed as yoda').images[0]
```

For more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)

## Trigger words

To trigger image generation of trained concept(or concepts) replace each concept identifier in you prompt with the new inserted tokens:

to trigger concept `TOK` → use `<s0><s1>` in your prompt 



## Details
All [Files & versions](/linoyts/huggy_edm_lora_v7/tree/main).

The weights were trained using [🧨 diffusers Advanced Dreambooth Training Script](https://github.com/huggingface/diffusers/blob/main/examples/advanced_diffusion_training/train_dreambooth_lora_sdxl_advanced.py).

LoRA for the text encoder was enabled. False.

Pivotal tuning was enabled: True.

Special VAE used for training: madebyollin/sdxl-vae-fp16-fix.",2934.0,4.599970232335117e-06
DevQuasar/mistralai.Mistral-Large-Instruct-2411-GGUF,673badccb3eb061c51e53c56,DevQuasar,"{""base_model"": [""mistralai/Mistral-Large-Instruct-2411""], ""pipeline_tag"": ""text-generation""}",[],2025-02-01 23:08:04,1,0.0,,45,876,,"['gguf' 'text-generation'
 'base_model:mistralai/Mistral-Large-Instruct-2411'
 'base_model:quantized:mistralai/Mistral-Large-Instruct-2411'
 'endpoints_compatible' 'region:us' 'conversational']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q2_K-00001-of-00004.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q2_K-00002-of-00004.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q2_K-00003-of-00004.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q2_K-00004-of-00004.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q3_K_M-00001-of-00005.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q3_K_M-00002-of-00005.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q3_K_M-00003-of-00005.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q3_K_M-00004-of-00005.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q3_K_M-00005-of-00005.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q4_K_M-00001-of-00006.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q4_K_M-00002-of-00006.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q4_K_M-00003-of-00006.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q4_K_M-00004-of-00006.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q4_K_M-00005-of-00006.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q4_K_M-00006-of-00006.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00001-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00002-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00003-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00004-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00005-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00006-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q5_K_M-00007-of-00007.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00001-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00002-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00003-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00004-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00005-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00006-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00007-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q6_K-00008-of-00008.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00001-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00002-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00003-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00004-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00005-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00006-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00007-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00008-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00009-of-00010.gguf'}
 {'rfilename': 'mistralai.Mistral-Large-Instruct-2411.Q8_0-00010-of-00010.gguf'}]",2024-11-18 21:12:44,,,"{""total"": 122610069504, ""architecture"": ""llama"", ""context_length"": 131072, ""chat_template"": ""{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + '[/INST]' }}{% elif message['role'] == 'system' %}{{ '[SYSTEM_PROMPT] ' + message['content'] + '[/SYSTEM_PROMPT]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + eos_token }}{% else %}{{ raise_exception('Only user, system and assistant roles are supported!') }}{% endif %}{% endfor %}"", ""bos_token"": ""<s>"", ""eos_token"": ""</s>""}",quantized,673657a8517b82b436cb7e4c,mistralai/Mistral-Large-Instruct-2411,DevQuasar/mistralai.Mistral-Large-Instruct-2411-GGUF,"base_model:
- mistralai/Mistral-Large-Instruct-2411
pipeline_tag: text-generation","[<img src=""https://raw.githubusercontent.com/csabakecskemeti/devquasar/main/dq_logo_black-transparent.png"" width=""200""/>](https://devquasar.com)

'Make knowledge free for everyone'

Quantized version of: [mistralai/Mistral-Large-Instruct-2411](https://huggingface.co/mistralai/Mistral-Large-Instruct-2411)
<a href='https://ko-fi.com/L4L416YX7C' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi6.png?v=6' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a>",512.0,4.544239127406008e-06
pszemraj/stablelm-4e1t-2b-v0.1,66497917babc0b33564344bd,pszemraj,"{""license"": ""cc-by-sa-4.0"", ""base_model"": ""stabilityai/stablelm-3b-4e1t"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""stablelm-4e1t-2b-v0.1"", ""results"": []}], ""language"": [""en""]}",[],2024-05-20 01:37:17,0,0.0,"{""architectures"": [""StableLmForCausalLM""], ""model_type"": ""stablelm"", ""tokenizer_config"": {""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|endoftext|>"", ""pad_token"": ""<|end_of_text|>"", ""unk_token"": ""<|endoftext|>""}, ""additional_chat_templates"": {}}",2,808,"{'parameters': {'BF16': 2002355200.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 2002355200.0}","['transformers' 'safetensors' 'stablelm' 'text-generation' 'axolotl'
 'generated_from_trainer' 'en' 'base_model:stabilityai/stablelm-3b-4e1t'
 'base_model:finetune:stabilityai/stablelm-3b-4e1t' 'license:cc-by-sa-4.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2024-05-19 03:59:19,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,65166921ca07b26143b047f7,stabilityai/stablelm-3b-4e1t,pszemraj/stablelm-4e1t-2b-v0.1,"license: cc-by-sa-4.0
base_model: stabilityai/stablelm-3b-4e1t
tags:
- axolotl
- generated_from_trainer
model-index:
- name: stablelm-4e1t-2b-v0.1
  results: []
language:
- en","# stablelm-4e1t-2b-v0.1


This is a layer pruning experiment based off of [stablelm-3b-4e1t](https://hf.co/stabilityai/stablelm-3b-4e1t):

- 10 layers pruned with [PruneMe](https://github.com/pszemraj/PruneMe/tree/upgrades)/MergeKit
  - layers selected using [BEE-spoke-data/fineweb-100k_en-med](https://hf.co/datasets/BEE-spoke-data/fineweb-100k_en-med)
- brief subsequent continued pretraining @ ctx 4096
  - data: 10k rows of FineWeb (different than pruning data) + some curated data
- wandb [here](https://wandb.ai/pszemraj/llama3-pruning)


## details

[<img src=""https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/OpenAccess-AI-Collective/axolotl)
<details><summary>See axolotl config</summary>

### config


axolotl version: `0.4.0`
```yaml
base_model: pszemraj/stablelm-3b-4e1t-prune10
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

strict: false
seed: 80085

# dataset
datasets:
    - path: BEE-spoke-data/KI-smorgasbord_fw-small
      type: completion # format from earlier
      field: text # Optional[str] default: text, field to use for completion data
val_set_size: 0.015

sequence_len: 4096
sample_packing: true
pad_to_sequence_len: false
train_on_inputs: false
group_by_length: false

# WANDB
wandb_project: llama3-pruning
wandb_entity: pszemraj
wandb_watch: gradients
wandb_name: stablelm-4e1t-2b-v0.1
hub_model_id: pszemraj/stablelm-4e1t-2b-v0.1
hub_strategy: every_save

gradient_accumulation_steps: 16
micro_batch_size: 1
num_epochs: 2
optimizer: adamw_torch_fused # paged_adamw_32bit
weight_decay: 0.05
lr_scheduler: cosine
learning_rate: 5e-5
warmup_ratio: 0.1

load_in_8bit: false
load_in_4bit: false
bf16: true
tf32: true

flash_attention: true
torch_compile: true # requires >= torch 2.0, may sometimes cause problems
torch_compile_backend: inductor # Optional[str]
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# hyperparams for freq of evals, saving, etc
evals_per_epoch: 5
saves_per_epoch: 3
save_safetensors: true
save_total_limit: 1
output_dir: ./output-axolotl/output-model-2b
logging_steps: 8

deepspeed:

special_tokens:
  pad_token: <|end_of_text|>

```

</details><br>


### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| No log        | 0.0006 | 1    | 4.4344          |
| 2.6558        | 0.2004 | 332  | 2.7150          |
| 2.6548        | 0.4007 | 664  | 2.6196          |
| 2.5435        | 0.6011 | 996  | 2.5981          |
| 2.5133        | 0.8014 | 1328 | 2.5502          |
| 2.489         | 1.0018 | 1660 | 2.5106          |
| 2.2671        | 1.1898 | 1992 | 2.4944          |
| 2.2038        | 1.3902 | 2324 | 2.4843          |
| 2.2513        | 1.5905 | 2656 | 2.4761          |
| 2.1654        | 1.7909 | 2988 | 2.4769          |

---",2932.0,4.490117321857672e-06
lichorosario/sam-and-max,65fdd9a1bda47e9bcfb5683b,lichorosario,"{""license"": ""creativeml-openrail-m"", ""tags"": [""text-to-image"", ""stable-diffusion"", ""lora"", ""diffusers"", ""template:sd-lora""], ""base_model"": ""stabilityai/stable-diffusion-xl-base-1.0"", ""instance_prompt"": null, ""pipeline_tag"": ""text-to-image"", ""widget"": [{""text"": ""SAMHTR style game background. The camera is positioned at the entrance of the caf\u00e9, offering a clear view of the charming and playful exterior: Caf\u00e9 Entrance: The entrance to the caf\u00e9 features a bright, welcoming fa\u00e7ade with white wooden siding and a red-tiled roof. The back door is adorned with simple, colorful flower pots on either side. The door itself is framed with a cheerful design, including a decorative awning that provides shade and adds a touch of whimsy. Outdoor Seating Area: Directly in front of the entrance, there\u2019s a cozy outdoor seating area. The tables and chairs are designed with playful elements, such as tables shaped like oversized pastries and chairs with coffee mug backrests. The design is simplified to avoid clutter but maintains the caf\u00e9\u2019s cheerful theme. Decorative Elements: The walls around the entrance are adorned with minimal yet vibrant murals depicting coffee cups, cakes, and other caf\u00e9-themed items. There are also a few hanging lanterns or string lights to add a warm and inviting glow. Pathway: The pathway leading to the caf\u00e9 entrance is made of colorful cobblestones, but the design is straightforward and not overly intricate. The path is lined with a few decorative plants and shrubs with bright, cartoonish features, but kept simple. Adjacent Features: To the sides of the entrance, you can see parts of the nearby areas, such as the adventure tourism shop and the retro appliance store"", ""output"": {""url"": ""https://huggingface.co/lichorosario/sam-and-max/resolve/main/images/example_t02m2yogs.png""}}]}","[{'_id': '6810ad085eb1bbf3dd3ae5d5', 'adapter': 'lora', 'adapterWeightsPath': 'hit_the_road.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/fast-sdxl', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2024-09-10 13:09:54,1,0.0,,3,648,,"['diffusers' 'text-to-image' 'stable-diffusion' 'lora' 'template:sd-lora'
 'base_model:stabilityai/stable-diffusion-xl-base-1.0'
 'base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0'
 'license:creativeml-openrail-m' 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'hit_the_road.safetensors'}
 {'rfilename': 'images/example_t02m2yogs.png'}]",2024-03-22 19:18:57,diffusers,,,adapter,64bfcd5ff462a99a04fd1ec8,stabilityai/stable-diffusion-xl-base-1.0,lichorosario/sam-and-max,"license: creativeml-openrail-m
tags:
- text-to-image
- stable-diffusion
- lora
- diffusers
- template:sd-lora
base_model: stabilityai/stable-diffusion-xl-base-1.0
instance_prompt: null
pipeline_tag: text-to-image
widget:
- text: >-
    SAMHTR style game background. The camera is positioned at the entrance of
    the café, offering a clear view of the charming and playful exterior: Café
    Entrance: The entrance to the café features a bright, welcoming façade with
    white wooden siding and a red-tiled roof. The back door is adorned with
    simple, colorful flower pots on either side. The door itself is framed with
    a cheerful design, including a decorative awning that provides shade and
    adds a touch of whimsy. Outdoor Seating Area: Directly in front of the
    entrance, there’s a cozy outdoor seating area. The tables and chairs are
    designed with playful elements, such as tables shaped like oversized
    pastries and chairs with coffee mug backrests. The design is simplified to
    avoid clutter but maintains the café’s cheerful theme. Decorative Elements:
    The walls around the entrance are adorned with minimal yet vibrant murals
    depicting coffee cups, cakes, and other café-themed items. There are also a
    few hanging lanterns or string lights to add a warm and inviting glow.
    Pathway: The pathway leading to the café entrance is made of colorful
    cobblestones, but the design is straightforward and not overly intricate.
    The path is lined with a few decorative plants and shrubs with bright,
    cartoonish features, but kept simple. Adjacent Features: To the sides of the
    entrance, you can see parts of the nearby areas, such as the adventure
    tourism shop and the retro appliance store
  output:
    url: images/example_t02m2yogs.png","# Sam and Max Hit the Road

<Gallery />

## Model description",61.0,4.3423425595597116e-06
mradermacher/distillgpt2-BittensorTuned4-i1-GGUF,6830ddd6acf8a53afd628102,mradermacher,"{""base_model"": ""asifahmed/distillgpt2-BittensorTuned4"", ""language"": [""en""], ""library_name"": ""transformers"", ""quantized_by"": ""mradermacher""}",[],2025-05-23 20:44:46,0,0.0,,7,581,,"['transformers' 'gguf' 'en'
 'base_model:asifahmed/distillgpt2-BittensorTuned4'
 'base_model:quantized:asifahmed/distillgpt2-BittensorTuned4'
 'endpoints_compatible' 'region:us' 'imatrix']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ1_M.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ1_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ2_M.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ2_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ2_XS.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ2_XXS.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ3_M.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ3_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ3_XS.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ3_XXS.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ4_NL.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-IQ4_XS.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q2_K.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q2_K_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q3_K_L.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q3_K_M.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q3_K_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q4_0.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q4_1.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q4_K_M.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q4_K_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q5_K_M.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q5_K_S.gguf'}
 {'rfilename': 'distillgpt2-BittensorTuned4.i1-Q6_K.gguf'}
 {'rfilename': 'imatrix.dat'}]",2025-05-23 20:43:02,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 124439808, ""architecture"": ""gpt2"", ""context_length"": 1024, ""quantize_imatrix_file"": ""distillgpt2-BittensorTuned4-i1-GGUF/imatrix.dat"", ""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|endoftext|>""}",quantized,638b95b3d274cbbad281ca65,asifahmed/distillgpt2-BittensorTuned4,mradermacher/distillgpt2-BittensorTuned4-i1-GGUF,"base_model: asifahmed/distillgpt2-BittensorTuned4
language:
- en
library_name: transformers
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags: nicoboss -->
weighted/imatrix quants of https://huggingface.co/asifahmed/distillgpt2-BittensorTuned4

<!-- provided-files -->
static quants are available at https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ1_S.gguf) | i1-IQ1_S | 0.1 | for the desperate |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ1_M.gguf) | i1-IQ1_M | 0.2 | mostly desperate |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ2_XS.gguf) | i1-IQ2_XS | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ2_S.gguf) | i1-IQ2_S | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ2_M.gguf) | i1-IQ2_M | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 0.2 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q2_K_S.gguf) | i1-Q2_K_S | 0.2 | very low quality |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q2_K.gguf) | i1-Q2_K | 0.2 | IQ3_XXS probably better |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ3_XS.gguf) | i1-IQ3_XS | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ3_S.gguf) | i1-IQ3_S | 0.2 | beats Q3_K* |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q3_K_S.gguf) | i1-Q3_K_S | 0.2 | IQ3_XS probably better |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ3_M.gguf) | i1-IQ3_M | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q3_K_M.gguf) | i1-Q3_K_M | 0.2 | IQ3_S probably better |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ4_XS.gguf) | i1-IQ4_XS | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-IQ4_NL.gguf) | i1-IQ4_NL | 0.2 | prefer IQ4_XS |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q4_0.gguf) | i1-Q4_0 | 0.2 | fast, low quality |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q4_K_S.gguf) | i1-Q4_K_S | 0.2 | optimal size/speed/quality |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q3_K_L.gguf) | i1-Q3_K_L | 0.2 | IQ3_M probably better |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q4_1.gguf) | i1-Q4_1 | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q4_K_M.gguf) | i1-Q4_K_M | 0.2 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q5_K_S.gguf) | i1-Q5_K_S | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q5_K_M.gguf) | i1-Q5_K_M | 0.2 |  |
| [GGUF](https://huggingface.co/mradermacher/distillgpt2-BittensorTuned4-i1-GGUF/resolve/main/distillgpt2-BittensorTuned4.i1-Q6_K.gguf) | i1-Q6_K | 0.2 | practically like static Q6_K |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.

<!-- end -->",5775.0,4.269273852239876e-06
John6666/xin-v10-sdxl,67b33f980a7e6ed731dda7ff,John6666,"{""license"": ""other"", ""license_name"": ""faipl-1.0-sd"", ""license_link"": ""https://freedevproject.org/faipl-1.0-sd/"", ""language"": [""en""], ""library_name"": ""diffusers"", ""pipeline_tag"": ""text-to-image"", ""tags"": [""text-to-image"", ""stable-diffusion"", ""stable-diffusion-xl"", ""anime"", ""girls"", ""distictive anime look"", ""illustrious""], ""base_model"": ""Laxhar/noobai-XL-1.1""}",[],2025-02-17 14:00:59,0,0.0,"{""diffusers"": {""_class_name"": ""StableDiffusionXLPipeline""}}",2,545,,"['diffusers' 'safetensors' 'text-to-image' 'stable-diffusion'
 'stable-diffusion-xl' 'anime' 'girls' 'distictive anime look'
 'illustrious' 'en' 'base_model:Laxhar/noobai-XL-1.1'
 'base_model:finetune:Laxhar/noobai-XL-1.1' 'license:other'
 'autotrain_compatible' 'endpoints_compatible'
 'diffusers:StableDiffusionXLPipeline' 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'model_index.json'}
 {'rfilename': 'scheduler/scheduler_config.json'}
 {'rfilename': 'text_encoder/config.json'}
 {'rfilename': 'text_encoder/model.safetensors'}
 {'rfilename': 'text_encoder_2/config.json'}
 {'rfilename': 'text_encoder_2/model.safetensors'}
 {'rfilename': 'tokenizer/merges.txt'}
 {'rfilename': 'tokenizer/special_tokens_map.json'}
 {'rfilename': 'tokenizer/tokenizer_config.json'}
 {'rfilename': 'tokenizer/vocab.json'}
 {'rfilename': 'tokenizer_2/merges.txt'}
 {'rfilename': 'tokenizer_2/special_tokens_map.json'}
 {'rfilename': 'tokenizer_2/tokenizer_config.json'}
 {'rfilename': 'tokenizer_2/vocab.json'} {'rfilename': 'unet/config.json'}
 {'rfilename': 'unet/diffusion_pytorch_model.safetensors'}
 {'rfilename': 'vae/config.json'}
 {'rfilename': 'vae/diffusion_pytorch_model.safetensors'}]",2025-02-17 13:54:32,diffusers,,,finetune,673f3913a31ce1fff2657292,Laxhar/noobai-XL-1.1,John6666/xin-v10-sdxl,"license: other
license_name: faipl-1.0-sd
license_link: https://freedevproject.org/faipl-1.0-sd/
language:
- en
library_name: diffusers
pipeline_tag: text-to-image
tags:
- text-to-image
- stable-diffusion
- stable-diffusion-xl
- anime
- girls
- distictive anime look
- illustrious
base_model: Laxhar/noobai-XL-1.1","Original model is [here](https://civitai.com/models/1263826/xin?modelVersionId=1425256).
This model created by [Mistless](https://civitai.com/user/Mistless).",158.0,4.226455869808768e-06
mradermacher/Qwen3-14B-DarkFusion-test0-GGUF,68abbc7996670d780ec8100b,mradermacher,"{""base_model"": ""SvalTek/Qwen3-14B-DarkFusion-test0"", ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""mradermacher"": {""readme_rev"": 1}, ""quantized_by"": ""mradermacher"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""qwen3""]}",[],2025-08-25 01:55:08,1,0.0,,2,466,,"['transformers' 'gguf' 'text-generation-inference' 'unsloth' 'qwen3' 'en'
 'base_model:SvalTek/Qwen3-14B-DarkFusion-test0'
 'base_model:quantized:SvalTek/Qwen3-14B-DarkFusion-test0'
 'license:apache-2.0' 'endpoints_compatible' 'region:us' 'conversational']",,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.IQ4_XS.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q2_K.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q3_K_L.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q3_K_M.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q3_K_S.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q4_K_M.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q4_K_S.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q5_K_M.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q5_K_S.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q6_K.gguf'}
 {'rfilename': 'Qwen3-14B-DarkFusion-test0.Q8_0.gguf'}
 {'rfilename': 'README.md'}]",2025-08-25 01:29:29,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 14765573120, ""architecture"": ""qwen3"", ""context_length"": 40960, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \""# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \""user\"" and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n                {%- set reasoning_content = message.content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\""name\"": \""' }}\n                {{- tool_call.name }}\n                {{- '\"", \""arguments\"": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}"", ""eos_token"": ""<|im_end|>""}",quantized,68aa1aaf1935e460757d27c5,SvalTek/Qwen3-14B-DarkFusion-test0,mradermacher/Qwen3-14B-DarkFusion-test0-GGUF,"base_model: SvalTek/Qwen3-14B-DarkFusion-test0
language:
- en
library_name: transformers
license: apache-2.0
mradermacher:
  readme_rev: 1
quantized_by: mradermacher
tags:
- text-generation-inference
- transformers
- unsloth
- qwen3","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
<!-- ### quants: x-f16 Q4_K_S Q2_K Q8_0 Q6_K Q3_K_M Q3_K_S Q3_K_L Q4_K_M Q5_K_S Q5_K_M IQ4_XS -->
<!-- ### quants_skip:  -->
<!-- ### skip_mmproj:  -->
static quants of https://huggingface.co/SvalTek/Qwen3-14B-DarkFusion-test0

<!-- provided-files -->

***For a convenient overview and download list, visit our [model page for this model](https://hf.tst.eu/model#Qwen3-14B-DarkFusion-test0-GGUF).***

weighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q2_K.gguf) | Q2_K | 5.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q3_K_S.gguf) | Q3_K_S | 6.8 |  |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q3_K_M.gguf) | Q3_K_M | 7.4 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q3_K_L.gguf) | Q3_K_L | 8.0 |  |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.IQ4_XS.gguf) | IQ4_XS | 8.3 |  |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q4_K_S.gguf) | Q4_K_S | 8.7 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q4_K_M.gguf) | Q4_K_M | 9.1 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q5_K_S.gguf) | Q5_K_S | 10.4 |  |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q5_K_M.gguf) | Q5_K_M | 10.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q6_K.gguf) | Q6_K | 12.2 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/Qwen3-14B-DarkFusion-test0-GGUF/resolve/main/Qwen3-14B-DarkFusion-test0.Q8_0.gguf) | Q8_0 | 15.8 | fast, best quality |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3582.0,4.1216499789727245e-06
314e/meditech-6types-gemma3-finetuned-higher-lora-alpha-Gemma3-Entity,6898a30f613568eaaf270bd4,314e,"{""license"": ""gemma"", ""library_name"": ""transformers"", ""pipeline_tag"": ""image-text-to-text"", ""extra_gated_heading"": ""Access Gemma on Hugging Face"", ""extra_gated_prompt"": ""To access Gemma on Hugging Face, you\u2019re required to review and agree to Google\u2019s usage license. To do this, please ensure you\u2019re logged in to Hugging Face and click below. Requests are processed immediately."", ""extra_gated_button_content"": ""Acknowledge license"", ""base_model"": ""google/gemma-3-12b-pt""}",[],2025-08-11 06:32:59,0,0.0,,11,452,"{'parameters': {'BF16': 12187325040.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 12187325040.0}","['transformers' 'safetensors' 'image-text-to-text' 'arxiv:1905.07830'
 'arxiv:1905.10044' 'arxiv:1911.11641' 'arxiv:1904.09728'
 'arxiv:1705.03551' 'arxiv:1911.01547' 'arxiv:1907.10641'
 'arxiv:1903.00161' 'arxiv:2009.03300' 'arxiv:2304.06364'
 'arxiv:2103.03874' 'arxiv:2110.14168' 'arxiv:2311.12022'
 'arxiv:2108.07732' 'arxiv:2107.03374' 'arxiv:2210.03057'
 'arxiv:2106.03193' 'arxiv:1910.11856' 'arxiv:2502.12404'
 'arxiv:2502.21228' 'arxiv:2404.16816' 'arxiv:2104.12756'
 'arxiv:2311.16502' 'arxiv:2203.10244' 'arxiv:2404.12390'
 'arxiv:1810.12440' 'arxiv:1908.02660' 'arxiv:2312.11805'
 'base_model:google/gemma-3-12b-pt'
 'base_model:finetune:google/gemma-3-12b-pt' 'license:gemma'
 'endpoints_compatible' 'region:us']",image-text-to-text,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'chat_template.json'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model-00001-of-00005.safetensors'}
 {'rfilename': 'model-00002-of-00005.safetensors'}
 {'rfilename': 'model-00003-of-00005.safetensors'}
 {'rfilename': 'model-00004-of-00005.safetensors'}
 {'rfilename': 'model-00005-of-00005.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'processor_config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2025-08-10 13:47:59,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}",,finetune,67c35bd50e998b1f7c4d3139,google/gemma-3-12b-pt,314e/meditech-6types-gemma3-finetuned-higher-lora-alpha-Gemma3-Entity,"license: gemma
library_name: transformers
pipeline_tag: image-text-to-text
extra_gated_heading: Access Gemma on Hugging Face
extra_gated_prompt: To access Gemma on Hugging Face, you’re required to review and
  agree to Google’s usage license. To do this, please ensure you’re logged in to Hugging
  Face and click below. Requests are processed immediately.
extra_gated_button_content: Acknowledge license
base_model: google/gemma-3-12b-pt","# Gemma 3 model card

**Model Page**: [Gemma](https://ai.google.dev/gemma/docs/core)

**Resources and Technical Documentation**:

* [Gemma 3 Technical Report][g3-tech-report]
* [Responsible Generative AI Toolkit][rai-toolkit]
* [Gemma on Kaggle][kaggle-gemma]
* [Gemma on Vertex Model Garden][vertex-mg-gemma3]

**Terms of Use**: [Terms][terms]

**Authors**: Google DeepMind

## Model Information

Summary description and brief definition of inputs and outputs.

### Description

Gemma is a family of lightweight, state-of-the-art open models from Google,
built from the same research and technology used to create the Gemini models.
Gemma 3 models are multimodal, handling text and image input and generating text
output, with open weights for both pre-trained variants and instruction-tuned
variants. Gemma 3 has a large, 128K context window, multilingual support in over
140 languages, and is available in more sizes than previous versions. Gemma 3
models are well-suited for a variety of text generation and image understanding
tasks, including question answering, summarization, and reasoning. Their
relatively small size makes it possible to deploy them in environments with
limited resources such as laptops, desktops or your own cloud infrastructure,
democratizing access to state of the art AI models and helping foster innovation
for everyone.

### Inputs and outputs

-   **Input:**
    -  Text string, such as a question, a prompt, or a document to be summarized
    -  Images, normalized to 896 x 896 resolution and encoded to 256 tokens
       each
    -  Total input context of 128K tokens for the 4B, 12B, and 27B sizes, and
       32K tokens for the 1B size

-   **Output:**
    -   Generated text in response to the input, such as an answer to a
        question, analysis of image content, or a summary of a document
    -   Total output context of 8192 tokens

### Usage

Below, there are some code snippets on how to get quickly started with running the model. First, install the Transformers library. Gemma 3 is supported starting from transformers 4.50.0. 

```sh
$ pip install -U transformers
```

Then, copy the snippet from the section that is relevant for your use case.

#### Running with the `pipeline` API

You can initialize the model and processor for inference with `pipeline` as follows.

```python
from transformers import pipeline
import torch

pipe = pipeline(
    ""image-text-to-text"",
    model=""google/gemma-3-12b-it"",
    device=""cuda"",
    torch_dtype=torch.bfloat16
)
```

With instruction-tuned models, you need to use chat templates to process our inputs first. Then, you can pass it to the pipeline.

```python
messages = [
    {
        ""role"": ""system"",
        ""content"": [{""type"": ""text"", ""text"": ""You are a helpful assistant.""}]
    },
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image"", ""url"": ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG""},
            {""type"": ""text"", ""text"": ""What animal is on the candy?""}
        ]
    }
]

output = pipe(text=messages, max_new_tokens=200)
print(output[0][""generated_text""][-1][""content""])
# Okay, let's take a look! 
# Based on the image, the animal on the candy is a **turtle**. 
# You can see the shell shape and the head and legs.
```

#### Running the model on a single / multi GPU

```python
# pip install accelerate

from transformers import AutoProcessor, Gemma3ForConditionalGeneration
from PIL import Image
import requests
import torch

model_id = ""google/gemma-3-12b-it""

model = Gemma3ForConditionalGeneration.from_pretrained(
    model_id, device_map=""auto""
).eval()

processor = AutoProcessor.from_pretrained(model_id)

messages = [
    {
        ""role"": ""system"",
        ""content"": [{""type"": ""text"", ""text"": ""You are a helpful assistant.""}]
    },
    {
        ""role"": ""user"",
        ""content"": [
            {""type"": ""image"", ""image"": ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg""},
            {""type"": ""text"", ""text"": ""Describe this image in detail.""}
        ]
    }
]

inputs = processor.apply_chat_template(
    messages, add_generation_prompt=True, tokenize=True,
    return_dict=True, return_tensors=""pt""
).to(model.device, dtype=torch.bfloat16)

input_len = inputs[""input_ids""].shape[-1]

with torch.inference_mode():
    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)
    generation = generation[0][input_len:]

decoded = processor.decode(generation, skip_special_tokens=True)
print(decoded)

# **Overall Impression:** The image is a close-up shot of a vibrant garden scene, 
# focusing on a cluster of pink cosmos flowers and a busy bumblebee. 
# It has a slightly soft, natural feel, likely captured in daylight.
```

### Citation

```none
@article{gemma_2025,
    title={Gemma 3},
    url={https://goo.gle/Gemma3Report},
    publisher={Kaggle},
    author={Gemma Team},
    year={2025}
}
```

## Model Data

Data used for model training and how the data was processed.

### Training Dataset

These models were trained on a dataset of text data that includes a wide variety
of sources. The 27B model was trained with 14 trillion tokens, the 12B model was
trained with 12 trillion tokens, 4B model was trained with 4 trillion tokens and
1B with 2 trillion tokens. Here are the key components:

-   Web Documents: A diverse collection of web text ensures the model is
    exposed to a broad range of linguistic styles, topics, and vocabulary. The
    training dataset includes content in over 140 languages.
-   Code: Exposing the model to code helps it to learn the syntax and
    patterns of programming languages, which improves its ability to generate
    code and understand code-related questions.
-   Mathematics: Training on mathematical text helps the model learn logical
    reasoning, symbolic representation, and to address mathematical queries.
-   Images: A wide range of images enables the model to perform image
    analysis and visual data extraction tasks.

The combination of these diverse data sources is crucial for training a powerful
multimodal model that can handle a wide variety of different tasks and data
formats.

### Data Preprocessing

Here are the key data cleaning and filtering methods applied to the training
data:

-   CSAM Filtering: Rigorous CSAM (Child Sexual Abuse Material) filtering
    was applied at multiple stages in the data preparation process to ensure
    the exclusion of harmful and illegal content.
-   Sensitive Data Filtering: As part of making Gemma pre-trained models
    safe and reliable, automated techniques were used to filter out certain
    personal information and other sensitive data from training sets.
-   Additional methods: Filtering based on content quality and safety in
    line with [our policies][safety-policies].

## Implementation Information

Details about the model internals.

### Hardware

Gemma was trained using [Tensor Processing Unit (TPU)][tpu] hardware (TPUv4p,
TPUv5p and TPUv5e). Training vision-language models (VLMS) requires significant
computational power. TPUs, designed specifically for matrix operations common in
machine learning, offer several advantages in this domain:

-   Performance: TPUs are specifically designed to handle the massive
    computations involved in training VLMs. They can speed up training
    considerably compared to CPUs.
-   Memory: TPUs often come with large amounts of high-bandwidth memory,
    allowing for the handling of large models and batch sizes during training.
    This can lead to better model quality.
-   Scalability: TPU Pods (large clusters of TPUs) provide a scalable
    solution for handling the growing complexity of large foundation models.
    You can distribute training across multiple TPU devices for faster and more
    efficient processing.
-   Cost-effectiveness: In many scenarios, TPUs can provide a more
    cost-effective solution for training large models compared to CPU-based
    infrastructure, especially when considering the time and resources saved
    due to faster training.
-   These advantages are aligned with
    [Google's commitments to operate sustainably][sustainability].

### Software

Training was done using [JAX][jax] and [ML Pathways][ml-pathways].

JAX allows researchers to take advantage of the latest generation of hardware,
including TPUs, for faster and more efficient training of large models. ML
Pathways is Google's latest effort to build artificially intelligent systems
capable of generalizing across multiple tasks. This is specially suitable for
foundation models, including large language models like these ones.

Together, JAX and ML Pathways are used as described in the
[paper about the Gemini family of models][gemini-2-paper]; *""the 'single
controller' programming model of Jax and Pathways allows a single Python
process to orchestrate the entire training run, dramatically simplifying the
development workflow.""*

## Evaluation

Model evaluation metrics and results.

### Benchmark Results

These models were evaluated against a large collection of different datasets and
metrics to cover different aspects of text generation:

#### Reasoning and factuality

| Benchmark                      | Metric         | Gemma 3 PT 1B  | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |
| ------------------------------ |----------------|:--------------:|:-------------:|:--------------:|:--------------:|
| [HellaSwag][hellaswag]         | 10-shot        |      62.3      |      77.2     |      84.2      |      85.6      |
| [BoolQ][boolq]                 | 0-shot         |      63.2      |      72.3     |      78.8      |      82.4      |
| [PIQA][piqa]                   | 0-shot         |      73.8      |      79.6     |      81.8      |      83.3      |
| [SocialIQA][socialiqa]         | 0-shot         |      48.9      |      51.9     |      53.4      |      54.9      |
| [TriviaQA][triviaqa]           | 5-shot         |      39.8      |      65.8     |      78.2      |      85.5      |
| [Natural Questions][naturalq]  | 5-shot         |      9.48      |      20.0     |      31.4      |      36.1      |
| [ARC-c][arc]                   | 25-shot        |      38.4      |      56.2     |      68.9      |      70.6      |
| [ARC-e][arc]                   | 0-shot         |      73.0      |      82.4     |      88.3      |      89.0      |
| [WinoGrande][winogrande]       | 5-shot         |      58.2      |      64.7     |      74.3      |      78.8      |
| [BIG-Bench Hard][bbh]          | few-shot       |      28.4      |      50.9     |      72.6      |      77.7      |
| [DROP][drop]                   | 1-shot         |      42.4      |      60.1     |      72.2      |      77.2      |

[hellaswag]: https://arxiv.org/abs/1905.07830
[boolq]: https://arxiv.org/abs/1905.10044
[piqa]: https://arxiv.org/abs/1911.11641
[socialiqa]: https://arxiv.org/abs/1904.09728
[triviaqa]: https://arxiv.org/abs/1705.03551
[naturalq]: https://github.com/google-research-datasets/natural-questions
[arc]: https://arxiv.org/abs/1911.01547
[winogrande]: https://arxiv.org/abs/1907.10641
[bbh]: https://paperswithcode.com/dataset/bbh
[drop]: https://arxiv.org/abs/1903.00161

#### STEM and code

| Benchmark                      | Metric         | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |
| ------------------------------ |----------------|:-------------:|:--------------:|:--------------:|
| [MMLU][mmlu]                   | 5-shot         |      59.6     |      74.5      |      78.6      |
| [MMLU][mmlu] (Pro COT)         | 5-shot         |      29.2     |      45.3      |      52.2      |
| [AGIEval][agieval]             | 3-5-shot       |      42.1     |      57.4      |      66.2      |
| [MATH][math]                   | 4-shot         |      24.2     |      43.3      |      50.0      |
| [GSM8K][gsm8k]                 | 8-shot         |      38.4     |      71.0      |      82.6      |
| [GPQA][gpqa]                   | 5-shot         |      15.0     |      25.4      |      24.3      |
| [MBPP][mbpp]                   | 3-shot         |      46.0     |      60.4      |      65.6      |
| [HumanEval][humaneval]         | 0-shot         |      36.0     |      45.7      |      48.8      |

[mmlu]: https://arxiv.org/abs/2009.03300
[agieval]: https://arxiv.org/abs/2304.06364
[math]: https://arxiv.org/abs/2103.03874
[gsm8k]: https://arxiv.org/abs/2110.14168
[gpqa]: https://arxiv.org/abs/2311.12022
[mbpp]: https://arxiv.org/abs/2108.07732
[humaneval]: https://arxiv.org/abs/2107.03374

#### Multilingual

| Benchmark                            | Gemma 3 PT 1B | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |
| ------------------------------------ |:-------------:|:-------------:|:--------------:|:--------------:|
| [MGSM][mgsm]                         |      2.04     |      34.7     |      64.3     |      74.3     |
| [Global-MMLU-Lite][global-mmlu-lite] |      24.9     |      57.0     |      69.4     |      75.7     |
| [WMT24++][wmt24pp] (ChrF)            |      36.7     |      48.4     |      53.9     |      55.7     |
| [FloRes][flores]                     |      29.5     |      39.2     |      46.0     |      48.8     |
| [XQuAD][xquad] (all)                 |      43.9     |      68.0     |      74.5     |      76.8     |
| [ECLeKTic][eclektic]                 |      4.69     |      11.0     |      17.2     |      24.4     |
| [IndicGenBench][indicgenbench]       |      41.4     |      57.2     |      61.7     |      63.4     |

[mgsm]: https://arxiv.org/abs/2210.03057
[flores]: https://arxiv.org/abs/2106.03193
[xquad]: https://arxiv.org/abs/1910.11856v3
[global-mmlu-lite]: https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite
[wmt24pp]: https://arxiv.org/abs/2502.12404v1
[eclektic]: https://arxiv.org/abs/2502.21228
[indicgenbench]: https://arxiv.org/abs/2404.16816

#### Multimodal

| Benchmark                      | Gemma 3 PT 4B | Gemma 3 PT 12B | Gemma 3 PT 27B |
| ------------------------------ |:-------------:|:--------------:|:--------------:|
| [COCOcap][coco-cap]            |      102      |      111       |      116       |
| [DocVQA][docvqa] (val)         |      72.8     |      82.3      |      85.6      |
| [InfoVQA][info-vqa] (val)      |      44.1     |      54.8      |      59.4      |
| [MMMU][mmmu] (pt)              |      39.2     |      50.3      |      56.1      |
| [TextVQA][textvqa] (val)       |      58.9     |      66.5      |      68.6      |
| [RealWorldQA][realworldqa]     |      45.5     |      52.2      |      53.9      |
| [ReMI][remi]                   |      27.3     |      38.5      |      44.8      |
| [AI2D][ai2d]                   |      63.2     |      75.2      |      79.0      |
| [ChartQA][chartqa]             |      63.6     |      74.7      |      76.3      |
| [VQAv2][vqav2]                 |      63.9     |      71.2      |      72.9      |
| [BLINK][blinkvqa]              |      38.0     |      35.9      |      39.6      |
| [OKVQA][okvqa]                 |      51.0     |      58.7      |      60.2      |
| [TallyQA][tallyqa]             |      42.5     |      51.8      |      54.3      |
| [SpatialSense VQA][ss-vqa]     |      50.9     |      60.0      |      59.4      |
| [CountBenchQA][countbenchqa]   |      26.1     |      17.8      |      68.0      |

[coco-cap]: https://cocodataset.org/#home
[docvqa]: https://www.docvqa.org/
[info-vqa]: https://arxiv.org/abs/2104.12756
[mmmu]: https://arxiv.org/abs/2311.16502
[textvqa]: https://textvqa.org/
[realworldqa]: https://paperswithcode.com/dataset/realworldqa
[remi]: https://arxiv.org/html/2406.09175v1
[ai2d]: https://allenai.org/data/diagrams
[chartqa]: https://arxiv.org/abs/2203.10244
[vqav2]: https://visualqa.org/index.html
[blinkvqa]: https://arxiv.org/abs/2404.12390
[okvqa]: https://okvqa.allenai.org/
[tallyqa]: https://arxiv.org/abs/1810.12440
[ss-vqa]: https://arxiv.org/abs/1908.02660
[countbenchqa]: https://github.com/google-research/big_vision/blob/main/big_vision/datasets/countbenchqa/

## Ethics and Safety

Ethics and safety evaluation approach and results.

### Evaluation Approach

Our evaluation methods include structured evaluations and internal red-teaming
testing of relevant content policies. Red-teaming was conducted by a number of
different teams, each with different goals and human evaluation metrics. These
models were evaluated against a number of different categories relevant to
ethics and safety, including:

-   **Child Safety**: Evaluation of text-to-text and image to text prompts
    covering child safety policies, including child sexual abuse and
    exploitation.
-   **Content Safety:** Evaluation of text-to-text and image to text prompts
    covering safety policies including, harassment, violence and gore, and hate
    speech.
-   **Representational Harms**: Evaluation of text-to-text and image to text
    prompts covering safety policies including bias, stereotyping, and harmful
    associations or inaccuracies.

In addition to development level evaluations, we conduct ""assurance
evaluations"" which are our 'arms-length' internal evaluations for responsibility
governance decision making. They are conducted separately from the model
development team, to inform decision making about release. High level findings
are fed back to the model team, but prompt sets are held-out to prevent
overfitting and preserve the results' ability to inform decision making.
Assurance evaluation results are reported to our Responsibility & Safety Council
as part of release review.

### Evaluation Results

For all areas of safety testing, we saw major improvements in the categories of
child safety, content safety, and representational harms relative to previous
Gemma models. All testing was conducted without safety filters to evaluate the
model capabilities and behaviors. For both text-to-text and image-to-text, and
across all model sizes, the model produced minimal policy violations, and showed
significant improvements over previous Gemma models' performance with respect
to ungrounded inferences. A limitation of our evaluations was they included only
English language prompts.

## Usage and Limitations

These models have certain limitations that users should be aware of.

### Intended Usage

Open vision-language models (VLMs) models have a wide range of applications
across various industries and domains. The following list of potential uses is
not comprehensive. The purpose of this list is to provide contextual information
about the possible use-cases that the model creators considered as part of model
training and development.

-   Content Creation and Communication
    -   Text Generation: These models can be used to generate creative text
        formats such as poems, scripts, code, marketing copy, and email drafts.
    -   Chatbots and Conversational AI: Power conversational interfaces
        for customer service, virtual assistants, or interactive applications.
    -   Text Summarization: Generate concise summaries of a text corpus,
        research papers, or reports.
    -   Image Data Extraction: These models can be used to extract,
        interpret, and summarize visual data for text communications.
-   Research and Education
    -   Natural Language Processing (NLP) and VLM Research: These
        models can serve as a foundation for researchers to experiment with VLM
        and NLP techniques, develop algorithms, and contribute to the
        advancement of the field.
    -   Language Learning Tools: Support interactive language learning
        experiences, aiding in grammar correction or providing writing practice.
    -   Knowledge Exploration: Assist researchers in exploring large
        bodies of text by generating summaries or answering questions about
        specific topics.

### Limitations

-   Training Data
    -   The quality and diversity of the training data significantly
        influence the model's capabilities. Biases or gaps in the training data
        can lead to limitations in the model's responses.
    -   The scope of the training dataset determines the subject areas
        the model can handle effectively.
-   Context and Task Complexity
    -   Models are better at tasks that can be framed with clear
        prompts and instructions. Open-ended or highly complex tasks might be
        challenging.
    -   A model's performance can be influenced by the amount of context
        provided (longer context generally leads to better outputs, up to a
        certain point).
-   Language Ambiguity and Nuance
    -   Natural language is inherently complex. Models might struggle
        to grasp subtle nuances, sarcasm, or figurative language.
-   Factual Accuracy
    -   Models generate responses based on information they learned
        from their training datasets, but they are not knowledge bases. They
        may generate incorrect or outdated factual statements.
-   Common Sense
    -   Models rely on statistical patterns in language. They might
        lack the ability to apply common sense reasoning in certain situations.

### Ethical Considerations and Risks

The development of vision-language models (VLMs) raises several ethical
concerns. In creating an open model, we have carefully considered the following:

-   Bias and Fairness
    -   VLMs trained on large-scale, real-world text and image data can
        reflect socio-cultural biases embedded in the training material. These
        models underwent careful scrutiny, input data pre-processing described
        and posterior evaluations reported in this card.
-   Misinformation and Misuse
    -   VLMs can be misused to generate text that is false, misleading,
        or harmful.
    -   Guidelines are provided for responsible use with the model, see the
        [Responsible Generative AI Toolkit][rai-toolkit].
-   Transparency and Accountability:
    -   This model card summarizes details on the models' architecture,
        capabilities, limitations, and evaluation processes.
    -   A responsibly developed open model offers the opportunity to
        share innovation by making VLM technology accessible to developers and
        researchers across the AI ecosystem.

Risks identified and mitigations:

-   **Perpetuation of biases**: It's encouraged to perform continuous
    monitoring (using evaluation metrics, human review) and the exploration of
    de-biasing techniques during model training, fine-tuning, and other use
    cases.
-   **Generation of harmful content**: Mechanisms and guidelines for content
    safety are essential. Developers are encouraged to exercise caution and
    implement appropriate content safety safeguards based on their specific
    product policies and application use cases.
-   **Misuse for malicious purposes**: Technical limitations and developer
    and end-user education can help mitigate against malicious applications of
    VLMs. Educational resources and reporting mechanisms for users to flag
    misuse are provided. Prohibited uses of Gemma models are outlined in the
    [Gemma Prohibited Use Policy][prohibited-use].
-   **Privacy violations**: Models were trained on data filtered for removal
    of certain personal information and other sensitive data. Developers are
    encouraged to adhere to privacy regulations with privacy-preserving
    techniques.

### Benefits

At the time of release, this family of models provides high-performance open
vision-language model implementations designed from the ground up for
responsible AI development compared to similarly sized models.

Using the benchmark evaluation metrics described in this document, these models
have shown to provide superior performance to other, comparably-sized open model
alternatives.

[g3-tech-report]: https://goo.gle/Gemma3Report
[rai-toolkit]: https://ai.google.dev/responsible
[kaggle-gemma]: https://www.kaggle.com/models/google/gemma-3
[vertex-mg-gemma3]: https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3
[terms]: https://ai.google.dev/gemma/terms
[safety-policies]: https://ai.google/static/documents/ai-responsibility-update-published-february-2025.pdf
[prohibited-use]: https://ai.google.dev/gemma/prohibited_use_policy
[tpu]: https://cloud.google.com/tpu/docs/intro-to-tpu
[sustainability]: https://sustainability.google/operating-sustainably/
[jax]: https://github.com/jax-ml/jax
[ml-pathways]: https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/
[sustainability]: https://sustainability.google/operating-sustainably/
[gemini-2-paper]: https://arxiv.org/abs/2312.11805",24672.0,4.10123922647727e-06
stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-4,652ea9c617d03c316f5d67bb,stefan-it,"{""language"": ""fr"", ""license"": ""mit"", ""tags"": [""flair"", ""token-classification"", ""sequence-tagger-model""], ""base_model"": ""hmteams/teams-base-historic-multilingual-discriminator"", ""widget"": [{""text"": ""Le Moniteur universel fait ressortir les avantages de la situation de l ' Allemagne , sa force militaire , le peu d ' int\u00e9r\u00eats personnels qu ' elle peut avoir dans la question d ' Orient .""}]}",[],2023-10-17 22:29:01,0,0.0,,1,451,,"['flair' 'pytorch' 'tensorboard' 'token-classification'
 'sequence-tagger-model' 'fr'
 'base_model:hmteams/teams-base-historic-multilingual-discriminator'
 'base_model:finetune:hmteams/teams-base-historic-multilingual-discriminator'
 'license:mit' 'region:us']",token-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'dev.tsv'} {'rfilename': 'loss.tsv'}
 {'rfilename': 'pytorch_model.bin'}
 {'rfilename': 'runs/events.out.tfevents.1697556160.0468bd9609d6.7281.13'}
 {'rfilename': 'test.tsv'} {'rfilename': 'training.log'}]",2023-10-17 15:35:34,flair,,,finetune,64c8c609b8685df8003c0953,hmteams/teams-base-historic-multilingual-discriminator,stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-4,"language: fr
license: mit
tags:
- flair
- token-classification
- sequence-tagger-model
base_model: hmteams/teams-base-historic-multilingual-discriminator
widget:
- text: Le Moniteur universel fait ressortir les avantages de la situation de l '
    Allemagne , sa force militaire , le peu d ' intérêts personnels qu ' elle peut
    avoir dans la question d ' Orient .","# Fine-tuned Flair Model on French NewsEye NER Dataset (HIPE-2022)

This Flair model was fine-tuned on the
[French NewsEye](https://github.com/hipe-eval/HIPE-2022-data/blob/main/documentation/README-newseye.md)
NER Dataset using hmTEAMS as backbone LM.

The NewsEye dataset is comprised of diachronic historical newspaper material published between 1850 and 1950
in French, German, Finnish, and Swedish.
More information can be found [here](https://dl.acm.org/doi/abs/10.1145/3404835.3463255).

The following NEs were annotated: `PER`, `LOC`, `ORG` and `HumanProd`.

# Results

We performed a hyper-parameter search over the following parameters with 5 different seeds per configuration:

* Batch Sizes: `[8, 4]`
* Learning Rates: `[3e-05, 5e-05]`

And report micro F1-score on development set:

| Configuration   | Run 1        | Run 2        | Run 3        | Run 4        | Run 5        | Avg.         |
|-----------------|--------------|--------------|--------------|--------------|--------------|--------------|
| bs8-e10-lr3e-05 | [0.825][1]   | [0.8248][2]  | [0.8288][3]  | [0.8309][4]  | [0.8281][5]  | 82.75 ± 0.23 |
| bs4-e10-lr3e-05 | [0.83][6]    | [0.8345][7]  | [0.8162][8]  | [0.8223][9]  | [0.8346][10] | 82.75 ± 0.72 |
| bs8-e10-lr5e-05 | [0.8237][11] | [0.8165][12] | [0.8189][13] | [0.8297][14] | [0.8283][15] | 82.34 ± 0.51 |
| bs4-e10-lr5e-05 | [0.8114][16] | [0.8061][17] | [0.8112][18] | [0.8131][19] | [0.8182][20] | 81.2 ± 0.39  |

[1]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-1
[2]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-2
[3]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-3
[4]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-4
[5]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-5
[6]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-1
[7]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-2
[8]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-3
[9]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-4
[10]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr3e-05-poolingfirst-layers-1-crfFalse-5
[11]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-1
[12]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-2
[13]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-3
[14]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-4
[15]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs8-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-5
[16]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-1
[17]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-2
[18]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-3
[19]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-4
[20]: https://hf.co/stefan-it/hmbench-newseye-fr-hmteams-bs4-wsFalse-e10-lr5e-05-poolingfirst-layers-1-crfFalse-5

The [training log](training.log) and TensorBoard logs (only for hmByT5 and hmTEAMS based models) are also uploaded to the model hub.

More information about fine-tuning can be found [here](https://github.com/stefan-it/hmBench).

# Acknowledgements

We thank [Luisa März](https://github.com/LuisaMaerz), [Katharina Schmid](https://github.com/schmika) and
[Erion Çano](https://github.com/erionc) for their fruitful discussions about Historic Language Models.

Research supported with Cloud TPUs from Google's [TPU Research Cloud](https://sites.research.google/trc/about/) (TRC).
Many Thanks for providing access to the TPUs ❤️",4353.0,4.099757265120611e-06
mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF,67c2ac235c1ed1f81008f61b,mradermacher,"{""base_model"": ""D1rtyB1rd/Dirty-Alice-Tiny-1.1B-V2"", ""language"": [""en""], ""library_name"": ""transformers"", ""license"": ""mit"", ""quantized_by"": ""mradermacher""}",[],2025-03-03 12:57:46,0,0.0,,24,419,,"['transformers' 'gguf' 'en'
 'base_model:D1rtyB1rd/Dirty-Alice-Tiny-1.1B-V2'
 'base_model:quantized:D1rtyB1rd/Dirty-Alice-Tiny-1.1B-V2' 'license:mit'
 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.IQ4_XS.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q2_K.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q3_K_L.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q3_K_M.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q3_K_S.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q4_K_M.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q4_K_S.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q5_K_M.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q5_K_S.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q6_K.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.Q8_0.gguf'}
 {'rfilename': 'Dirty-Alice-Tiny-1.1B-V2.f16.gguf'}
 {'rfilename': 'README.md'}]",2025-03-01 06:41:39,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 1100048384, ""architecture"": ""llama"", ""context_length"": 2048, ""bos_token"": ""<s>"", ""eos_token"": ""</s>""}",quantized,66e592699d477f566f16be8b,D1rtyB1rd/Dirty-Alice-Tiny-1.1B-V2,mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF,"base_model: D1rtyB1rd/Dirty-Alice-Tiny-1.1B-V2
language:
- en
library_name: transformers
license: mit
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/D1rtyB1rd/Dirty-Alice-Tiny-1.1B-V2

<!-- provided-files -->
weighted/imatrix quants are available at https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-i1-GGUF
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q2_K.gguf) | Q2_K | 0.5 |  |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q3_K_S.gguf) | Q3_K_S | 0.6 |  |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q3_K_M.gguf) | Q3_K_M | 0.6 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q3_K_L.gguf) | Q3_K_L | 0.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.IQ4_XS.gguf) | IQ4_XS | 0.7 |  |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q4_K_S.gguf) | Q4_K_S | 0.7 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q4_K_M.gguf) | Q4_K_M | 0.8 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q5_K_S.gguf) | Q5_K_S | 0.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q5_K_M.gguf) | Q5_K_M | 0.9 |  |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q6_K.gguf) | Q6_K | 1.0 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.Q8_0.gguf) | Q8_0 | 1.3 | fast, best quality |
| [GGUF](https://huggingface.co/mradermacher/Dirty-Alice-Tiny-1.1B-V2-GGUF/resolve/main/Dirty-Alice-Tiny-1.1B-V2.f16.gguf) | f16 | 2.3 | 16 bpw, overkill |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3270.0,4.0505177415712715e-06
RenauxLouis/monet-test-1000steps-116-realsize-v3,6465fd00e0fe831b478c59f7,RenauxLouis,"{""license"": ""creativeml-openrail-m"", ""base_model"": ""runwayml/stable-diffusion-v1-5"", ""tags"": [""stable-diffusion"", ""stable-diffusion-diffusers"", ""text-to-image"", ""diffusers"", ""lora""], ""inference"": true}",[],2023-05-18 11:19:41,0,0.0,,7,404,,"['diffusers' 'stable-diffusion' 'stable-diffusion-diffusers'
 'text-to-image' 'lora' 'base_model:runwayml/stable-diffusion-v1-5'
 'base_model:adapter:runwayml/stable-diffusion-v1-5'
 'license:creativeml-openrail-m' 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'checkpoint-1/optimizer.bin'}
 {'rfilename': 'checkpoint-1/pytorch_model.bin'}
 {'rfilename': 'checkpoint-1/random_states_0.pkl'}
 {'rfilename': 'checkpoint-1/scaler.pt'}
 {'rfilename': 'checkpoint-1/scheduler.bin'}
 {'rfilename': 'checkpoint-100/optimizer.bin'}
 {'rfilename': 'checkpoint-100/pytorch_model.bin'}
 {'rfilename': 'checkpoint-100/random_states_0.pkl'}
 {'rfilename': 'checkpoint-100/scaler.pt'}
 {'rfilename': 'checkpoint-100/scheduler.bin'}
 {'rfilename': 'checkpoint-1000/optimizer.bin'}
 {'rfilename': 'checkpoint-1000/pytorch_model.bin'}
 {'rfilename': 'checkpoint-1000/random_states_0.pkl'}
 {'rfilename': 'checkpoint-1000/scaler.pt'}
 {'rfilename': 'checkpoint-1000/scheduler.bin'}
 {'rfilename': 'checkpoint-1200/optimizer.bin'}
 {'rfilename': 'checkpoint-1200/pytorch_model.bin'}
 {'rfilename': 'checkpoint-1200/random_states_0.pkl'}
 {'rfilename': 'checkpoint-1200/scaler.pt'}
 {'rfilename': 'checkpoint-1200/scheduler.bin'}
 {'rfilename': 'checkpoint-1500/optimizer.bin'}
 {'rfilename': 'checkpoint-1500/pytorch_model.bin'}
 {'rfilename': 'checkpoint-1500/random_states_0.pkl'}
 {'rfilename': 'checkpoint-1500/scaler.pt'}
 {'rfilename': 'checkpoint-1500/scheduler.bin'}
 {'rfilename': 'checkpoint-1800/optimizer.bin'}
 {'rfilename': 'checkpoint-1800/pytorch_model.bin'}
 {'rfilename': 'checkpoint-1800/random_states_0.pkl'}
 {'rfilename': 'checkpoint-1800/scaler.pt'}
 {'rfilename': 'checkpoint-1800/scheduler.bin'}
 {'rfilename': 'checkpoint-2/optimizer.bin'}
 {'rfilename': 'checkpoint-2/pytorch_model.bin'}
 {'rfilename': 'checkpoint-2/random_states_0.pkl'}
 {'rfilename': 'checkpoint-2/scaler.pt'}
 {'rfilename': 'checkpoint-2/scheduler.bin'}
 {'rfilename': 'checkpoint-20/optimizer.bin'}
 {'rfilename': 'checkpoint-20/pytorch_model.bin'}
 {'rfilename': 'checkpoint-20/random_states_0.pkl'}
 {'rfilename': 'checkpoint-20/scaler.pt'}
 {'rfilename': 'checkpoint-20/scheduler.bin'}
 {'rfilename': 'checkpoint-200/optimizer.bin'}
 {'rfilename': 'checkpoint-200/pytorch_model.bin'}
 {'rfilename': 'checkpoint-200/random_states_0.pkl'}
 {'rfilename': 'checkpoint-200/scaler.pt'}
 {'rfilename': 'checkpoint-200/scheduler.bin'}
 {'rfilename': 'checkpoint-2100/optimizer.bin'}
 {'rfilename': 'checkpoint-2100/pytorch_model.bin'}
 {'rfilename': 'checkpoint-2100/random_states_0.pkl'}
 {'rfilename': 'checkpoint-2100/scaler.pt'}
 {'rfilename': 'checkpoint-2100/scheduler.bin'}
 {'rfilename': 'checkpoint-2400/optimizer.bin'}
 {'rfilename': 'checkpoint-2400/pytorch_model.bin'}
 {'rfilename': 'checkpoint-2400/random_states_0.pkl'}
 {'rfilename': 'checkpoint-2400/scaler.pt'}
 {'rfilename': 'checkpoint-2400/scheduler.bin'}
 {'rfilename': 'checkpoint-2700/optimizer.bin'}
 {'rfilename': 'checkpoint-2700/pytorch_model.bin'}
 {'rfilename': 'checkpoint-2700/random_states_0.pkl'}
 {'rfilename': 'checkpoint-2700/scaler.pt'}
 {'rfilename': 'checkpoint-2700/scheduler.bin'}
 {'rfilename': 'checkpoint-300/optimizer.bin'}
 {'rfilename': 'checkpoint-300/pytorch_model.bin'}
 {'rfilename': 'checkpoint-300/random_states_0.pkl'}
 {'rfilename': 'checkpoint-300/scaler.pt'}
 {'rfilename': 'checkpoint-300/scheduler.bin'}
 {'rfilename': 'checkpoint-3000/optimizer.bin'}
 {'rfilename': 'checkpoint-3000/pytorch_model.bin'}
 {'rfilename': 'checkpoint-3000/random_states_0.pkl'}
 {'rfilename': 'checkpoint-3000/scaler.pt'}
 {'rfilename': 'checkpoint-3000/scheduler.bin'}
 {'rfilename': 'checkpoint-4/optimizer.bin'}
 {'rfilename': 'checkpoint-4/pytorch_model.bin'}
 {'rfilename': 'checkpoint-4/random_states_0.pkl'}
 {'rfilename': 'checkpoint-4/scaler.pt'}
 {'rfilename': 'checkpoint-4/scheduler.bin'}
 {'rfilename': 'checkpoint-40/optimizer.bin'}
 {'rfilename': 'checkpoint-40/pytorch_model.bin'}
 {'rfilename': 'checkpoint-40/random_states_0.pkl'}
 {'rfilename': 'checkpoint-40/scaler.pt'}
 {'rfilename': 'checkpoint-40/scheduler.bin'}
 {'rfilename': 'checkpoint-400/optimizer.bin'}
 {'rfilename': 'checkpoint-400/pytorch_model.bin'}
 {'rfilename': 'checkpoint-400/random_states_0.pkl'}
 {'rfilename': 'checkpoint-400/scaler.pt'}
 {'rfilename': 'checkpoint-400/scheduler.bin'}
 {'rfilename': 'checkpoint-500/optimizer.bin'}
 {'rfilename': 'checkpoint-500/pytorch_model.bin'}
 {'rfilename': 'checkpoint-500/random_states_0.pkl'}
 {'rfilename': 'checkpoint-500/scaler.pt'}
 {'rfilename': 'checkpoint-500/scheduler.bin'}
 {'rfilename': 'checkpoint-60/optimizer.bin'}
 {'rfilename': 'checkpoint-60/pytorch_model.bin'}
 {'rfilename': 'checkpoint-60/random_states_0.pkl'}
 {'rfilename': 'checkpoint-60/scaler.pt'}
 {'rfilename': 'checkpoint-60/scheduler.bin'}
 {'rfilename': 'checkpoint-600/optimizer.bin'}
 {'rfilename': 'checkpoint-600/pytorch_model.bin'}
 {'rfilename': 'checkpoint-600/random_states_0.pkl'}
 {'rfilename': 'checkpoint-600/scaler.pt'}
 {'rfilename': 'checkpoint-600/scheduler.bin'}
 {'rfilename': 'checkpoint-700/optimizer.bin'}
 {'rfilename': 'checkpoint-700/pytorch_model.bin'}
 {'rfilename': 'checkpoint-700/random_states_0.pkl'}
 {'rfilename': 'checkpoint-700/scaler.pt'}
 {'rfilename': 'checkpoint-700/scheduler.bin'}
 {'rfilename': 'checkpoint-8/optimizer.bin'}
 {'rfilename': 'checkpoint-8/pytorch_model.bin'}
 {'rfilename': 'checkpoint-8/random_states_0.pkl'}
 {'rfilename': 'checkpoint-8/scaler.pt'}
 {'rfilename': 'checkpoint-8/scheduler.bin'}
 {'rfilename': 'checkpoint-800/optimizer.bin'}
 {'rfilename': 'checkpoint-800/pytorch_model.bin'}
 {'rfilename': 'checkpoint-800/random_states_0.pkl'}
 {'rfilename': 'checkpoint-800/scaler.pt'}
 {'rfilename': 'checkpoint-800/scheduler.bin'}
 {'rfilename': 'checkpoint-900/optimizer.bin'}
 {'rfilename': 'checkpoint-900/pytorch_model.bin'}
 {'rfilename': 'checkpoint-900/random_states_0.pkl'}
 {'rfilename': 'checkpoint-900/scaler.pt'}
 {'rfilename': 'checkpoint-900/scheduler.bin'}
 {'rfilename': 'image_0.png'} {'rfilename': 'image_1.png'}
 {'rfilename': 'image_2.png'} {'rfilename': 'image_3.png'}
 {'rfilename': 'pytorch_lora_weights.bin'}]",2023-05-18 10:25:04,diffusers,,,adapter,63508a7bfc22005c1aae0266,runwayml/stable-diffusion-v1-5,RenauxLouis/monet-test-1000steps-116-realsize-v3,,"---
license: creativeml-openrail-m
base_model: runwayml/stable-diffusion-v1-5
tags:
- stable-diffusion
- stable-diffusion-diffusers
- text-to-image
- diffusers
- lora
inference: true
---
    
# LoRA text2image fine-tuning - RenauxLouis/monet-test-1000steps-116-realsize-v3
These are LoRA adaption weights for runwayml/stable-diffusion-v1-5. The weights were fine-tuned on the real-size-116 dataset. You can find some example images in the following. 

![img_0](./image_0.png)
![img_1](./image_1.png)
![img_2](./image_2.png)
![img_3](./image_3.png)",547.0,4.026130063406268e-06
sthenno/tempesthenno-fusion-0309,67cca3c5526416e7c561dd70,sthenno,"{""base_model"": [""sthenno-com/miscii-14b-0218"", ""sthenno/tempesthenno-sft-0309-ckpt10""], ""library_name"": ""transformers"", ""tags"": [""mergekit"", ""merge""], ""license"": ""apache-2.0"", ""language"": [""en"", ""zh""], ""metrics"": [""accuracy""]}",[],2025-03-08 22:15:11,2,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% endif %}{% if system_message is defined %}{{ '<|im_start|>system\n' + system_message + '<|im_end|>\n' }}{% endif %}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|im_start|>user\n' + content + '<|im_end|>\n<|im_start|>assistant\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<|im_end|>\n' }}{% endif %}{% endfor %}"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}}",0,393,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 14765947904, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 14765947904.0}","['transformers' 'safetensors' 'qwen2' 'text-generation' 'mergekit' 'merge'
 'conversational' 'en' 'zh' 'base_model:sthenno-com/miscii-14b-0218'
 'base_model:merge:sthenno-com/miscii-14b-0218'
 'base_model:sthenno/tempesthenno-sft-0309-ckpt10'
 'base_model:merge:sthenno/tempesthenno-sft-0309-ckpt10'
 'license:apache-2.0' 'autotrain_compatible' 'text-generation-inference'
 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'mergekit_config.yml'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model-00001-of-00013.safetensors'}
 {'rfilename': 'model-00002-of-00013.safetensors'}
 {'rfilename': 'model-00003-of-00013.safetensors'}
 {'rfilename': 'model-00004-of-00013.safetensors'}
 {'rfilename': 'model-00005-of-00013.safetensors'}
 {'rfilename': 'model-00006-of-00013.safetensors'}
 {'rfilename': 'model-00007-of-00013.safetensors'}
 {'rfilename': 'model-00008-of-00013.safetensors'}
 {'rfilename': 'model-00009-of-00013.safetensors'}
 {'rfilename': 'model-00010-of-00013.safetensors'}
 {'rfilename': 'model-00011-of-00013.safetensors'}
 {'rfilename': 'model-00012-of-00013.safetensors'}
 {'rfilename': 'model-00013-of-00013.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2025-03-08 20:08:37,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,merge,67cca40e10db3c10f46aff22,sthenno/tempesthenno-sft-0309-ckpt10,sthenno/tempesthenno-fusion-0309,"base_model:
- sthenno-com/miscii-14b-0218
- sthenno/tempesthenno-sft-0309-ckpt10
library_name: transformers
tags:
- mergekit
- merge
license: apache-2.0
language:
- en
- zh
metrics:
- accuracy","# fs-01

This is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).

## Merge Details
### Merge Method

This model was merged using the [Arcee Fusion](https://arcee.ai) merge method using [sthenno-com/miscii-14b-0218](https://huggingface.co/sthenno-com/miscii-14b-0218) as a base.

### Models Merged

The following models were included in the merge:
* /home/ubuntu/tmp/models/tempesthenno-sft-0309-stage1-ckpt10

### Configuration

The following YAML configuration was used to produce this model:

```yaml
name: fs-01
merge_method: arcee_fusion
base_model: sthenno-com/miscii-14b-0218
tokenizer:
  source: base
dtype: bfloat16
parameters:
  normalize: true
  rescale: false
models:
  - model: /home/ubuntu/tmp/models/tempesthenno-sft-0309-stage1-ckpt10

```",806.0,4.007664667314573e-06
rzgar/kurdish_east_clothing_styles,67d67e5898af067a2a23a3ec,rzgar,"{""tags"": [""text-to-image"", ""lora"", ""diffusers"", ""template:diffusion-lora""], ""widget"": [{""text"": ""Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking blue hue with long sleeves and a shimmering sequin finish, paired with voluminous trousers and elegant high heels, complemented by an ornate headpiece adorned with decorative coins"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e11_01.png""}}, {""text"": ""Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking blue hue with long sleeves and a shimmering sequin finish, paired with voluminous trousers and elegant high heels, complemented by an ornate headpiece adorned with decorative coins"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e10_01.png""}}, {""text"": ""Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking blue hue with long sleeves and a shimmering sequin finish, paired with voluminous trousers and elegant high heels, complemented by an ornate headpiece adorned with decorative coins"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e9-.01.png""}}, {""text"": ""Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking blue hue with long sleeves and a shimmering sequin finish, paired with voluminous trousers and elegant high heels, complemented by an ornate headpiece adorned with decorative coins"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e8_01.png""}}, {""text"": ""Kurdish_clothing, a kurdish woman"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e11_random_01.png""}}, {""text"": ""Kurdish_clothing a woman in a black dress standing in front of a herd of sheep, surrounded by lush green grass, trees, and mountains in the background, with a bright blue sky above."", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e11_random_02.png""}}, {""text"": ""Kurdish_clothing a woman standing on the ground, holding a bouquet of flowers in her hand, surrounded by trees and a clear blue sky. She is wearing a plus size wedding dress, with a long, flowing skirt and a fitted bodice."", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e11_random_03.png""}}, {""text"": ""Kurdish_clothing, a woman sits on a throne of smooth, pearlescent stone in a grand throne room with polished marble walls and tall, gold-veined columns. Her very long, wavy dark-brown hair flows over her shoulders, framed by an elaborate crown of twisted silver and gold with sapphires and filigree. She wears flowing robes of deep blue and purple with silver embroidery, her legs crossed elegantly. In her right hand, she holds a slender scepter with a golden, vine-etched tip, and in her left, a glowing orb pulses with soft radiance. Crystal chandeliers flicker above, illuminating the scene. Tall arched windows reveal emerald hills and orchards under a pastel sky, with her hair swaying gently against the luminous throne"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/e11_random_04.png""}}, {""text"": ""Kurdish_clothing, a kurdish woman indoor posing for camera, wearing dark blue kurdish dress"", ""output"": {""url"": ""https://huggingface.co/rzgar/kurdish_east_clothing_styles/resolve/main/images/example_icrt2tfu3.png""}}], ""base_model"": ""black-forest-labs/FLUX.1-dev"", ""instance_prompt"": ""Kurdish_clothing"", ""license"": ""mit""}","[{'_id': '680b71dfa07785a516f270ef', 'adapter': 'lora', 'adapterWeightsPath': 'Kurdish East Clothing_epoch_10.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/flux-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}
 {'_id': '6818c94d5111343b72f18ab4', 'adapter': 'lora', 'adapterWeightsPath': 'Kurdish East Clothing_epoch_10.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'replicate', 'providerDetails': None, 'providerId': 'black-forest-labs/flux-dev-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2025-04-15 04:23:37,0,0.0,,43,389,,"['diffusers' 'text-to-image' 'lora' 'template:diffusion-lora'
 'base_model:black-forest-labs/FLUX.1-dev'
 'base_model:adapter:black-forest-labs/FLUX.1-dev' 'license:mit'
 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'}
 {'rfilename': 'ComfyUI_Kurdish_East_Workflow.json'}
 {'rfilename': 'Kurdish East Clothing_epoch_10.safetensors'}
 {'rfilename': 'Kurdish East Clothing_epoch_11.safetensors'}
 {'rfilename': 'Kurdish East Clothing_epoch_8.safetensors'}
 {'rfilename': 'Kurdish East Clothing_epoch_9.safetensors'}
 {'rfilename': 'README.md'} {'rfilename': 'images/e10_01.png'}
 {'rfilename': 'images/e11_01.png'}
 {'rfilename': 'images/e11_random_01.png'}
 {'rfilename': 'images/e11_random_02.png'}
 {'rfilename': 'images/e11_random_03.png'}
 {'rfilename': 'images/e11_random_04.png'}
 {'rfilename': 'images/e8_01.png'} {'rfilename': 'images/e9-.01.png'}
 {'rfilename': 'images/example_icrt2tfu3.png'}]",2025-03-16 07:31:36,diffusers,,,adapter,66aaa908fc35e079a941470d,black-forest-labs/FLUX.1-dev,rzgar/kurdish_east_clothing_styles,"tags:
- text-to-image
- lora
- diffusers
- template:diffusion-lora
widget:
- text: >-
    Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking
    blue hue with long sleeves and a shimmering sequin finish, paired with
    voluminous trousers and elegant high heels, complemented by an ornate
    headpiece adorned with decorative coins
  output:
    url: images/e11_01.png
- text: >-
    Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking
    blue hue with long sleeves and a shimmering sequin finish, paired with
    voluminous trousers and elegant high heels, complemented by an ornate
    headpiece adorned with decorative coins
  output:
    url: images/e10_01.png
- text: >-
    Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking
    blue hue with long sleeves and a shimmering sequin finish, paired with
    voluminous trousers and elegant high heels, complemented by an ornate
    headpiece adorned with decorative coins
  output:
    url: images/e9-.01.png
- text: >-
    Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking
    blue hue with long sleeves and a shimmering sequin finish, paired with
    voluminous trousers and elegant high heels, complemented by an ornate
    headpiece adorned with decorative coins
  output:
    url: images/e8_01.png
- text: Kurdish_clothing, a kurdish woman
  output:
    url: images/e11_random_01.png
- text: >-
    Kurdish_clothing a woman in a black dress standing in front of a herd of
    sheep, surrounded by lush green grass, trees, and mountains in the
    background, with a bright blue sky above.
  output:
    url: images/e11_random_02.png
- text: >-
    Kurdish_clothing a woman standing on the ground, holding a bouquet of
    flowers in her hand, surrounded by trees and a clear blue sky. She is
    wearing a plus size wedding dress, with a long, flowing skirt and a fitted
    bodice.
  output:
    url: images/e11_random_03.png
- text: >-
    Kurdish_clothing, a woman sits on a throne of smooth, pearlescent stone in a
    grand throne room with polished marble walls and tall, gold-veined columns.
    Her very long, wavy dark-brown hair flows over her shoulders, framed by an
    elaborate crown of twisted silver and gold with sapphires and filigree. She
    wears flowing robes of deep blue and purple with silver embroidery, her legs
    crossed elegantly. In her right hand, she holds a slender scepter with a
    golden, vine-etched tip, and in her left, a glowing orb pulses with soft
    radiance. Crystal chandeliers flicker above, illuminating the scene. Tall
    arched windows reveal emerald hills and orchards under a pastel sky, with
    her hair swaying gently against the luminous throne
  output:
    url: images/e11_random_04.png
- text: >-
    Kurdish_clothing, a kurdish woman indoor posing for camera, wearing dark
    blue kurdish dress
  output:
    url: images/example_icrt2tfu3.png
base_model: black-forest-labs/FLUX.1-dev
instance_prompt: Kurdish_clothing
license: mit","# Kurdish East Clothing Styles

<Gallery />

## Model description 


# Kurdish East Clothing Styles LoRA Model

The LoRA is trained to generate images of Kurdish traditional clothing, with a focus on the eastern part of Kurdistan (primarily the Iran region, known as Rojhelat). The model captures the rich variety of custom-made patterns, vibrant colors, and ornate decorative accessories and jewelry characteristic of Kurdish fashion.

## Model Overview
This model is trained on images, emphasizing the diverse styles found in eastern Kurdish clothing. Due to the intricate designs and regional variations (e.g., Mokrian section with flower-patterned &quot;pshten&quot; hip belts vs. Sanandaj&#39;s distinct custom designs), generating a consistent style can be challenging. As a result, the model may not always produce perfectly accurate representations and is provided &quot;as is&quot; under the MIT License—use it at your own risk.

### Training Settings
- **Engine**: Kohya
- **LoRA Type**: LoRA
- **Resolution**: 1024
- **Max Train Epochs**: 11
- **Num Repeats**: 17
- **Network Dim**: 64
- **Network Alpha**: 32
- **LR Scheduler**: Cosine
- **Min SNR Gamma**: 5
- **Noise Offset**: 0.1
- **Optimizer**: AdamW8Bit
- **Enable Bucket**: True
- **Clip Skip**: 1
- **LR Scheduler Num Cycles**: 1

### Usage Recommendations
- **Batch Size**: For desirable image quality, use at least a batch size of 4.
- **Steps**: 30 steps recommended for sampling.
- **Guidance Scale**: Conditioning guidance of 3.5 works well.
- **Clip Strength**: In cases where the environment or style deviates from Kurdish themes, lower the Clip strength (e.g., to 0.5) to allow the base model to handle the request better.

### Available Models
The training process produced high-quality results from epochs 8 to 11. All 4 epochs are uploaded for use.


### Sample Images
#### Consistent Prompt Test
Images generated with the prompt:  
*&quot;Kurdish_clothing, A woman wearing a Kurdish traditional dress in a striking blue hue with long sleeves and a shimmering sequin finish, paired with voluminous trousers and elegant high heels, complemented by an ornate headpiece adorned with decorative coins.&quot;*  
- Best of batch size 2, 30 steps, Euler simple, guidance scale 3.5.
- Files: &#x60;e8_01.png&#x60;, &#x60;e9_01.png&#x60;, &#x60;e10_01.png&#x60;, &#x60;e11_01.png&#x60;

#### Random Prompt Variations (Epoch 11)
Three additional prompts were tested with the e11 model, batch size 4, showcasing 4 variations per prompt. Screenshots include all 4 images per prompt:
1. **Prompt 1**: *&quot;Kurdish_clothing, a Kurdish woman.&quot;*  
   - Files: &#x60;e11_random_01.png&#x60;, &#x60;e11_random_02.png&#x60;, &#x60;e11_random_03.png&#x60;, &#x60;e11_random_04.png&#x60;
2. **Prompt 2**: *&quot;Kurdish_clothing, a woman in a black dress standing in front of a herd of sheep, surrounded by lush green grass, trees, and mountains in the background, with a bright blue sky above.&quot;*  
   - Files: &#x60;e11_random_01.png&#x60;, &#x60;e11_random_02.png&#x60;, &#x60;e11_random_03.png&#x60;, &#x60;e11_random_04.png&#x60;
3. **Prompt 3**: *&quot;Kurdish_clothing, a woman standing on the ground, holding a bouquet of flowers in her hand, surrounded by trees and a clear blue sky. She is wearing a plus-size wedding dress, with a long, flowing skirt and a fitted bodice.&quot;*  
   - Files: &#x60;e11_random_01.png&#x60;, &#x60;e11_random_02.png&#x60;, &#x60;e11_random_03.png&#x60;, &#x60;e11_random_04.png&#x60;
4. **Prompt 4**: *&quot;Kurdish_clothing, a woman sits on a throne of smooth, pearlescent stone in a grand throne room with polished marble walls and tall, gold-veined columns. Her very long, wavy dark-brown hair flows over her shoulders, framed by an elaborate crown of twisted silver and gold with sapphires and filigree. She wears flowing robes of deep blue and purple with silver embroidery, her legs crossed elegantly. In her right hand, she holds a slender scepter with a golden, vine-etched tip, and in her left, a glowing orb pulses with soft radiance. Crystal chandeliers flicker above, illuminating the scene. Tall arched windows reveal emerald hills and orchards under a pastel sky, with her hair swaying gently against the luminous throne.&quot;* (Clip: 0.5)  
   - Files: &#x60;e11_random_01.png&#x60;, &#x60;e11_random_02.png&#x60;, &#x60;e11_random_03.png&#x60;, &#x60;e11_random_04.png&#x60;

### Limitations
- The model is trained primarily on eastern Kurdish (Rojhelat) styles, with significant regional variation (e.g., Mokrian vs. Sanandaj designs).
- Due to the complexity of custom patterns, colors, and accessories, the model may not always reproduce the exact style accurately.
- For non-Kurdish environments or styles, reducing Clip strength is recommended to leverage the base model’s capabilities.

### License
This model is released under the **MIT License**—completely free to use, modify, and distribute, with no warranty. Use it at your own risk. See the [LICENSE](LICENSE) file for details.

&#x60;&#x60;&#x60;

## Trigger words

You should use `Kurdish_clothing` to trigger the image generation.


## Download model

Weights for this model are available in Safetensors format.

[Download](/rzgar/kurdish_east_clothing_styles/tree/main) them in the Files & versions tab.",5300.0,4.000821880991075e-06
codermert/tugce2-lora,66e59941c8bad3f385f63893,codermert,"{""license"": ""other"", ""license_name"": ""flux-1-dev-non-commercial-license"", ""license_link"": ""https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md"", ""language"": [""en""], ""tags"": [""flux"", ""diffusers"", ""lora""], ""base_model"": ""black-forest-labs/FLUX.1-dev"", ""pipeline_tag"": ""text-to-image"", ""instance_prompt"": ""DHANUSH""}","[{'_id': '680b71dfa07785a516f270ef', 'adapter': 'lora', 'adapterWeightsPath': 'flux_train_replicate.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/flux-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}
 {'_id': '6818c94d5111343b72f18ab4', 'adapter': 'lora', 'adapterWeightsPath': 'flux_train_replicate.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'replicate', 'providerDetails': None, 'providerId': 'black-forest-labs/flux-dev-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2025-03-13 20:27:25,2,0.0,,4,353,,"['diffusers' 'flux' 'lora' 'text-to-image' 'en'
 'base_model:black-forest-labs/FLUX.1-dev'
 'base_model:adapter:black-forest-labs/FLUX.1-dev' 'license:other'
 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.yaml'}
 {'rfilename': 'flux_train_replicate.safetensors'}
 {'rfilename': 'optimizer.pt'}
 {'rfilename': 'samples/1726325095756__000000000_0.jpg'}
 {'rfilename': 'samples/1726325633019__000000250_0.jpg'}
 {'rfilename': 'samples/1726326170687__000000500_0.jpg'}
 {'rfilename': 'samples/1726326704095__000000750_0.jpg'}
 {'rfilename': 'samples/1726327241367__000001000_0.jpg'}]",2024-09-14 14:10:09,diffusers,,,adapter,66aaa908fc35e079a941470d,black-forest-labs/FLUX.1-dev,codermert/tugce2-lora,"license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md
language:
- en
tags:
- flux
- diffusers
- lora
base_model: ""black-forest-labs/FLUX.1-dev""
pipeline_tag: text-to-image
instance_prompt: DHANUSH","# Tugce_Flux

Trained on Replicate using:

https://replicate.com/ostris/flux-dev-lora-trainer/train


## Trigger words
You should use `tugce` to trigger the image generation.


## Use it with the [🧨 diffusers library](https://github.com/huggingface/diffusers)

```py
from diffusers import AutoPipelineForText2Image
import torch
pipeline = AutoPipelineForText2Image.from_pretrained('prithivMLmods/Canopus-LoRA-Flux-UltraRealism-2.0', torch_dtype=torch.float16).to('cuda')
pipeline.load_lora_weights('codermert/tugce2-lora', weight_name='flux_train_replicate.safetensors')
image = pipeline('your prompt').images[0]
```

For more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)",818.0,3.9358756233923966e-06
kreabs/my_awesome_food_model,6631faad236fcf7b7b2edbb7,kreabs,"{""license"": ""apache-2.0"", ""base_model"": ""google/vit-base-patch16-224-in21k"", ""tags"": [""generated_from_trainer""], ""metrics"": [""accuracy""], ""model-index"": [{""name"": ""my_awesome_food_model"", ""results"": []}]}",[],2024-05-01 08:27:33,0,0.0,"{""architectures"": [""ViTForImageClassification""], ""model_type"": ""vit""}",1,337,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 85876325, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 85876325.0}","['transformers' 'tensorboard' 'safetensors' 'vit' 'image-classification'
 'generated_from_trainer' 'base_model:google/vit-base-patch16-224-in21k'
 'base_model:finetune:google/vit-base-patch16-224-in21k'
 'license:apache-2.0' 'autotrain_compatible' 'endpoints_compatible'
 'region:us']",image-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'runs/May01_08-17-49_1970fc496fce/events.out.tfevents.1714551470.1970fc496fce.244.0'}
 {'rfilename': 'training_args.bin'}]",2024-05-01 08:17:49,transformers,"{'auto_model': 'AutoModelForImageClassification', 'custom_class': None, 'pipeline_tag': 'image-classification', 'processor': 'AutoImageProcessor'}",,finetune,621ffdc136468d709f17b7d6,google/vit-base-patch16-224-in21k,kreabs/my_awesome_food_model,"license: apache-2.0
base_model: google/vit-base-patch16-224-in21k
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: my_awesome_food_model
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_awesome_food_model

This model is a fine-tuned version of [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 1.5923
- Accuracy: 0.895

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 64
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 2.6787        | 0.992 | 62   | 2.4852          | 0.831    |
| 1.8344        | 2.0   | 125  | 1.7766          | 0.87     |
| 1.6057        | 2.976 | 186  | 1.5923          | 0.895    |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.19.0
- Tokenizers 0.19.1",1448.0,3.904860281074077e-06
FanJing0914/vit-base-beans,65e96aa50ba2a33b5a99c2d9,FanJing0914,"{""license"": ""apache-2.0"", ""base_model"": ""google/vit-base-patch16-224-in21k"", ""tags"": [""image-classification"", ""vision"", ""generated_from_trainer""], ""metrics"": [""accuracy""], ""model-index"": [{""name"": ""vit-base-beans"", ""results"": []}]}",[],2024-03-07 07:23:45,0,0.0,"{""architectures"": [""ViTForImageClassification""], ""model_type"": ""vit""}",1,332,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 85800963, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 85800963.0}","['transformers' 'safetensors' 'vit' 'image-classification' 'vision'
 'generated_from_trainer' 'base_model:google/vit-base-patch16-224-in21k'
 'base_model:finetune:google/vit-base-patch16-224-in21k'
 'license:apache-2.0' 'autotrain_compatible' 'endpoints_compatible'
 'region:us']",image-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'all_results.json'} {'rfilename': 'config.json'}
 {'rfilename': 'eval_results.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'train_results.json'} {'rfilename': 'trainer_state.json'}
 {'rfilename': 'training_args.bin'}]",2024-03-07 07:20:05,transformers,"{'auto_model': 'AutoModelForImageClassification', 'custom_class': None, 'pipeline_tag': 'image-classification', 'processor': 'AutoImageProcessor'}",,finetune,621ffdc136468d709f17b7d6,google/vit-base-patch16-224-in21k,FanJing0914/vit-base-beans,"license: apache-2.0
base_model: google/vit-base-patch16-224-in21k
tags:
- image-classification
- vision
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: vit-base-beans
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# vit-base-beans

This model is a fine-tuned version of [google/vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k) on the beans dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0644
- Accuracy: 0.9925

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 1337
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5.0

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 0.2826        | 1.0   | 130  | 0.2146          | 0.9624   |
| 0.1306        | 2.0   | 260  | 0.1282          | 0.9699   |
| 0.1451        | 3.0   | 390  | 0.0966          | 0.9774   |
| 0.0785        | 4.0   | 520  | 0.0644          | 0.9925   |
| 0.1125        | 5.0   | 650  | 0.0795          | 0.9774   |


### Framework versions

- Transformers 4.39.0.dev0
- Pytorch 1.13.1+cu117
- Datasets 2.18.0
- Tokenizers 0.15.2",1478.0,3.894866247497098e-06
pszemraj/GPT-Neo-33M-simplewiki-2048-scratch,6503d78d69558920758b2d0c,pszemraj,"{""base_model"": ""roneneldan/TinyStories-33M"", ""tags"": [""generated_from_trainer""], ""metrics"": [""accuracy""], ""inference"": {""parameters"": {""max_new_tokens"": 64, ""do_sample"": true, ""repetition_penalty"": 1.1, ""no_repeat_ngram_size"": 5, ""guidance_scale"": 1.01, ""eta_cutoff"": 0.001}}, ""widget"": [{""text"": ""My name is El Microondas the Wise and"", ""example_title"": ""El Microondas""}, {""text"": ""A meme is"", ""example_title"": ""meme""}, {""text"": ""Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had"", ""example_title"": ""Coreference resolution""}, {""text"": ""On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book"", ""example_title"": ""Logic puzzles""}, {""text"": ""The two men running to become New York City's next mayor will face off in their first debate Wednesday night"", ""example_title"": ""Reading comprehension""}], ""pipeline_tag"": ""text-generation"", ""datasets"": [""pszemraj/simple_wikipedia_LM""], ""license"": ""apache-2.0"", ""language"": [""en""]}",[],2023-09-15 12:15:09,1,0.0,"{""architectures"": [""GPTNeoForCausalLM""], ""model_type"": ""gpt_neo"", ""tokenizer_config"": {""bos_token"": {""__type"": ""AddedToken"", ""content"": ""<|endoftext|>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""eos_token"": {""__type"": ""AddedToken"", ""content"": ""<|endoftext|>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}, ""pad_token"": null, ""unk_token"": {""__type"": ""AddedToken"", ""content"": ""<|endoftext|>"", ""lstrip"": false, ""normalized"": true, ""rstrip"": false, ""single_word"": false}}}",0,331,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 68514048, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 68514048.0}","['transformers' 'safetensors' 'gpt_neo' 'text-generation'
 'generated_from_trainer' 'en' 'dataset:pszemraj/simple_wikipedia_LM'
 'base_model:roneneldan/TinyStories-33M'
 'base_model:finetune:roneneldan/TinyStories-33M' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'merges.txt'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.json'}]",2023-09-15 04:03:25,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,645e9075340d0b66d14352ed,roneneldan/TinyStories-33M,pszemraj/GPT-Neo-33M-simplewiki-2048-scratch,"base_model: roneneldan/TinyStories-33M
tags:
- generated_from_trainer
metrics:
- accuracy
inference:
  parameters:
    max_new_tokens: 64
    do_sample: true
    repetition_penalty: 1.1
    no_repeat_ngram_size: 5
    guidance_scale: 1.01
    eta_cutoff: 0.001
widget:
- text: My name is El Microondas the Wise and
  example_title: El Microondas
- text: A meme is
  example_title: meme
- text: >-
    Barack Obama nominated Hilary Clinton as his secretary of state on Monday.
    He chose her because she had
  example_title: Coreference resolution
- text: >-
    On a shelf, there are five books: a gray book, a red book, a purple book, a
    blue book, and a black book
  example_title: Logic puzzles
- text: >-
    The two men running to become New York City's next mayor will face off in
    their first debate Wednesday night
  example_title: Reading comprehension
pipeline_tag: text-generation
datasets:
- pszemraj/simple_wikipedia_LM
license: apache-2.0
language:
- en","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# GPT-Neo-33M-simplewiki-2048-scratch

Initialized from random weights based on config from [roneneldan/TinyStories-33M](https://huggingface.co/roneneldan/TinyStories-33M), 3 epochs bf16.

It achieves the following results on the evaluation set:
- Loss: 3.9511
- Accuracy: 0.3843

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0005
- train_batch_size: 2
- eval_batch_size: 2
- seed: 80085
- gradient_accumulation_steps: 64
- total_train_batch_size: 128
- optimizer: Adam with betas=(0.9,0.99) and epsilon=1e-07
- lr_scheduler_type: inverse_sqrt
- lr_scheduler_warmup_ratio: 0.05
- num_epochs: 3.0

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 5.4676        | 0.45  | 100  | 5.0139          | 0.2811   |
| 5.1729        | 0.89  | 200  | 4.6737          | 0.3050   |
| 4.8702        | 1.34  | 300  | 4.4922          | 0.3170   |
| 4.5538        | 1.79  | 400  | 4.3026          | 0.3348   |
| 4.4818        | 2.23  | 500  | 4.0908          | 0.3649   |
| 4.4583        | 2.68  | 600  | 3.9511          | 0.3843   |


### Framework versions

- Transformers 4.33.1
- Pytorch 2.2.0.dev20230907+cu118
- Datasets 2.14.5
- Tokenizers 0.13.3",1668.0,3.892849442244671e-06
Augusto777/vit-base-patch16-224-perros-y-gatos,6639613301eaf0ea6f9d0bc2,Augusto777,"{""license"": ""apache-2.0"", ""base_model"": ""google/vit-base-patch16-224"", ""tags"": [""generated_from_trainer""], ""metrics"": [""accuracy""], ""model-index"": [{""name"": ""vit-base-patch16-224-perros-y-gatos"", ""results"": []}]}",[],2024-05-06 23:15:18,0,0.0,"{""architectures"": [""ViTForImageClassification""], ""model_type"": ""vit""}",1,330,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 85800194, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 85800194.0}","['transformers' 'tensorboard' 'safetensors' 'vit' 'image-classification'
 'generated_from_trainer' 'base_model:google/vit-base-patch16-224'
 'base_model:finetune:google/vit-base-patch16-224' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",image-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'all_results.json'} {'rfilename': 'config.json'}
 {'rfilename': 'eval_results.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'runs/May06_23-00-41_6f336873e3ec/events.out.tfevents.1715036941.6f336873e3ec.1386.0'}
 {'rfilename': 'runs/May06_23-00-41_6f336873e3ec/events.out.tfevents.1715037313.6f336873e3ec.1386.1'}
 {'rfilename': 'train_results.json'} {'rfilename': 'trainer_state.json'}
 {'rfilename': 'training_args.bin'}]",2024-05-06 23:01:07,transformers,"{'auto_model': 'AutoModelForImageClassification', 'custom_class': None, 'pipeline_tag': 'image-classification', 'processor': 'AutoImageProcessor'}",,finetune,621ffdc136468d709f17b7d7,google/vit-base-patch16-224,Augusto777/vit-base-patch16-224-perros-y-gatos,"license: apache-2.0
base_model: google/vit-base-patch16-224
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: vit-base-patch16-224-perros-y-gatos
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# vit-base-patch16-224-perros-y-gatos

This model is a fine-tuned version of [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.1577
- Accuracy: 1.0

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 32
- eval_batch_size: 32
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 128
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 10

### Training results

| Training Loss | Epoch  | Step | Validation Loss | Accuracy |
|:-------------:|:------:|:----:|:---------------:|:--------:|
| No log        | 0.8889 | 6    | 0.6543          | 0.5789   |
| 0.5484        | 1.9259 | 13   | 0.3385          | 0.8421   |
| 0.5484        | 2.9630 | 20   | 0.2123          | 0.9474   |
| 0.2164        | 4.0    | 27   | 0.1577          | 1.0      |
| 0.2164        | 4.8889 | 33   | 0.1404          | 1.0      |
| 0.1367        | 5.9259 | 40   | 0.1375          | 0.9474   |
| 0.1367        | 6.9630 | 47   | 0.1312          | 1.0      |
| 0.1155        | 8.0    | 54   | 0.1140          | 1.0      |
| 0.0878        | 8.8889 | 60   | 0.1107          | 1.0      |


### Framework versions

- Transformers 4.40.1
- Pytorch 2.2.1+cu121
- Datasets 2.19.1
- Tokenizers 0.19.1",1833.0,3.8908265531059504e-06
jangsh/Meta-Llama-3.1-8B_Jeon-gyeol-gguf,66c2e65aa474a917fc868431,jangsh,"{""base_model"": ""jangsh/Meta-Llama-3.1-8B_KOR"", ""language"": [""en""], ""license"": ""apache-2.0"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""llama"", ""gguf""]}",[],2024-08-19 06:45:31,0,0.0,"{""model_type"": ""llama""}",3,324,,"['transformers' 'gguf' 'llama' 'text-generation-inference' 'unsloth' 'en'
 'base_model:jangsh/Meta-Llama-3.1-8B_KOR'
 'base_model:quantized:jangsh/Meta-Llama-3.1-8B_KOR' 'license:apache-2.0'
 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'unsloth.F16.gguf'}
 {'rfilename': 'unsloth.Q4_K_M.gguf'}]",2024-08-19 06:29:46,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 8030261312, ""architecture"": ""llama"", ""context_length"": 131072, ""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|end_of_text|>""}",quantized,66bc5b037b487f99aa77377b,jangsh/Meta-Llama-3.1-8B_KOR,jangsh/Meta-Llama-3.1-8B_Jeon-gyeol-gguf,"base_model: jangsh/Meta-Llama-3.1-8B_KOR
language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- gguf","# Uploaded  model

- **Developed by:** jangsh
- **License:** apache-2.0
- **Finetuned from model :** jangsh/Meta-Llama-3.1-8B_KOR

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)",416.0,3.878559371251874e-06
abcorrea/p3-v2,68a646965df94b040a2a734e,abcorrea,"{""base_model"": ""abcorrea/p3-v1"", ""library_name"": ""transformers"", ""model_name"": ""p3-v2"", ""tags"": [""generated_from_trainer"", ""trl"", ""sft""], ""licence"": ""license""}",[],2025-08-20 22:13:23,0,0.0,"{""architectures"": [""Qwen3ForCausalLM""], ""model_type"": ""qwen3"", ""tokenizer_config"": {""bos_token"": null, ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}, ""chat_template_jinja"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \""# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \""user\"" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\""name\"": \""' }}\n                {{- tool_call.name }}\n                {{- '\"", \""arguments\"": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}""}",12,320,"{'parameters': {'BF16': 4021812736.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 4021812736.0}","['transformers' 'safetensors' 'qwen3' 'text-generation'
 'generated_from_trainer' 'trl' 'sft' 'conversational'
 'base_model:abcorrea/p3-v1' 'base_model:finetune:abcorrea/p3-v1'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'chat_template.jinja'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'merges.txt'}
 {'rfilename': 'model-00001-of-00002.safetensors'}
 {'rfilename': 'model-00002-of-00002.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.json'}]",2025-08-20 22:05:10,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,68a5e2051c9e746aa2266b0c,abcorrea/p3-v1,abcorrea/p3-v2,"base_model: abcorrea/p3-v1
library_name: transformers
model_name: p3-v2
tags:
- generated_from_trainer
- trl
- sft
licence: license","# Model Card for p3-v2

This model is a fine-tuned version of [abcorrea/p3-v1](https://huggingface.co/abcorrea/p3-v1).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""abcorrea/p3-v2"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

 


This model was trained with SFT.

### Framework versions

- TRL: 0.19.1
- Transformers: 4.52.1
- Pytorch: 2.7.0
- Datasets: 4.0.0
- Tokenizers: 0.21.1

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\'e}dec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```",1263.0,3.8702547791611645e-06
preetk21/codeparrot-ds,655bf23409d90b0a0a75e809,preetk21,"{""license"": ""mit"", ""base_model"": ""gpt2"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""codeparrot-ds"", ""results"": []}]}",[],2023-11-21 00:09:55,0,0.0,"{""architectures"": [""GPT2LMHeadModel""], ""model_type"": ""gpt2"", ""tokenizer_config"": {""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|endoftext|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": ""<|endoftext|>""}}",0,315,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 124242432, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 124242432.0}","['transformers' 'tensorboard' 'safetensors' 'gpt2' 'text-generation'
 'generated_from_trainer' 'base_model:openai-community/gpt2'
 'base_model:finetune:openai-community/gpt2' 'license:mit'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'merges.txt'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Nov21_00-04-51_a1b35feaf182/events.out.tfevents.1700525098.a1b35feaf182.713.0'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.json'}]",2023-11-20 23:56:36,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f17434d,openai-community/gpt2,preetk21/codeparrot-ds,"license: mit
base_model: gpt2
tags:
- generated_from_trainer
model-index:
- name: codeparrot-ds
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# codeparrot-ds

This model is a fine-tuned version of [gpt2](https://huggingface.co/gpt2) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0005
- train_batch_size: 32
- eval_batch_size: 32
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 256
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 1000
- num_epochs: 1

### Training results



### Framework versions

- Transformers 4.35.2
- Pytorch 2.1.0+cu118
- Datasets 2.15.0
- Tokenizers 0.15.0",983.0,3.85972729072447e-06
hzx405416956/wav2vec2-base-finetuned-ks,65b35ad14c9e50e74aa5fde7,hzx405416956,"{""license"": ""apache-2.0"", ""base_model"": ""facebook/wav2vec2-base"", ""tags"": [""generated_from_trainer""], ""datasets"": [""superb""], ""metrics"": [""accuracy""], ""model-index"": [{""name"": ""wav2vec2-base-finetuned-ks"", ""results"": [{""task"": {""name"": ""Audio Classification"", ""type"": ""audio-classification""}, ""dataset"": {""name"": ""superb"", ""type"": ""superb"", ""config"": ""ks"", ""split"": ""validation"", ""args"": ""ks""}, ""metrics"": [{""name"": ""Accuracy"", ""type"": ""accuracy"", ""value"": 0.9823477493380406, ""verified"": false}]}]}]}",[],2024-02-09 03:59:19,0,0.0,"{""architectures"": [""Wav2Vec2ForSequenceClassification""], ""model_type"": ""wav2vec2""}",4,291,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 94571660, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 94571660.0}","['transformers' 'safetensors' 'wav2vec2' 'audio-classification'
 'generated_from_trainer' 'dataset:superb'
 'base_model:facebook/wav2vec2-base'
 'base_model:finetune:facebook/wav2vec2-base' 'license:apache-2.0'
 'model-index' 'endpoints_compatible' 'region:us']",audio-classification,"[{'rfilename': '.DS_Store'} {'rfilename': '.gitattributes'}
 {'rfilename': 'README.md'} {'rfilename': 'config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'training_args.bin'}]",2024-01-26 07:10:09,transformers,"{'auto_model': 'AutoModelForAudioClassification', 'custom_class': None, 'pipeline_tag': 'audio-classification', 'processor': 'AutoProcessor'}",,finetune,621ffdc136468d709f17ae62,facebook/wav2vec2-base,hzx405416956/wav2vec2-base-finetuned-ks,"license: apache-2.0
base_model: facebook/wav2vec2-base
tags:
- generated_from_trainer
datasets:
- superb
metrics:
- accuracy
model-index:
- name: wav2vec2-base-finetuned-ks
  results:
  - task:
      name: Audio Classification
      type: audio-classification
    dataset:
      name: superb
      type: superb
      config: ks
      split: validation
      args: ks
    metrics:
    - name: Accuracy
      type: accuracy
      value: 0.9823477493380406","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# wav2vec2-base-finetuned-ks

This model is a fine-tuned version of [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base) on the superb dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0860
- Accuracy: 0.9823

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 3e-05
- train_batch_size: 32
- eval_batch_size: 32
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 128
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_ratio: 0.1
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| 0.6904        | 1.0   | 399  | 0.5744          | 0.9219   |
| 0.2737        | 2.0   | 798  | 0.1771          | 0.9771   |
| 0.2163        | 3.0   | 1197 | 0.1120          | 0.9803   |
| 0.1877        | 4.0   | 1597 | 0.0919          | 0.9821   |
| 0.1583        | 5.0   | 1995 | 0.0860          | 0.9823   |


### Framework versions

- Transformers 4.37.1
- Pytorch 2.1.2+cu121
- Datasets 2.16.1
- Tokenizers 0.15.1",1557.0,3.806758669909972e-06
Kedenike/my_awesome_opus_books_model,6692421606f73093a4100a99,Kedenike,"{""license"": ""apache-2.0"", ""base_model"": ""t5-small"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""my_awesome_opus_books_model"", ""results"": []}]}",[],2024-07-13 09:08:18,0,0.0,"{""architectures"": [""T5ForConditionalGeneration""], ""model_type"": ""t5"", ""tokenizer_config"": {""eos_token"": ""</s>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>""}}",0,284,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 60506624, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 60506624.0}","['transformers' 'safetensors' 't5' 'text2text-generation'
 'generated_from_trainer' 'base_model:google-t5/t5-small'
 'base_model:finetune:google-t5/t5-small' 'license:apache-2.0'
 'text-generation-inference' 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'spiece.model'}
 {'rfilename': 'tokenizer.json'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2024-07-13 09:00:06,transformers,"{'auto_model': 'AutoModelForSeq2SeqLM', 'custom_class': None, 'pipeline_tag': 'text2text-generation', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174358,google-t5/t5-small,Kedenike/my_awesome_opus_books_model,"license: apache-2.0
base_model: t5-small
tags:
- generated_from_trainer
model-index:
- name: my_awesome_opus_books_model
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# my_awesome_opus_books_model

This model is a fine-tuned version of [t5-small](https://huggingface.co/t5-small) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 2
- mixed_precision_training: Native AMP

### Framework versions

- Transformers 4.42.4
- Pytorch 2.3.0+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1",924.0,3.790487123990141e-06
kalese/opus-mt-en-bkm-10e,65f95968c1d406d7162dc2d3,kalese,"{""license"": ""apache-2.0"", ""base_model"": ""Helsinki-NLP/opus-mt-en-ro"", ""tags"": [""generated_from_trainer""], ""datasets"": [""arrow""], ""metrics"": [""bleu""], ""model-index"": [{""name"": ""opus-mt-en-bkm-10e"", ""results"": [{""task"": {""name"": ""Sequence-to-sequence Language Modeling"", ""type"": ""text2text-generation""}, ""dataset"": {""name"": ""arrow"", ""type"": ""arrow"", ""config"": ""default"", ""split"": ""train"", ""args"": ""default""}, ""metrics"": [{""name"": ""Bleu"", ""type"": ""bleu"", ""value"": 7.1794, ""verified"": false}]}]}]}",[],2024-03-19 11:23:20,0,0.0,"{""architectures"": [""MarianMTModel""], ""model_type"": ""marian"", ""tokenizer_config"": {""eos_token"": ""</s>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>""}}",0,279,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 74684055, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 74684055.0}","['transformers' 'tensorboard' 'safetensors' 'marian'
 'text2text-generation' 'generated_from_trainer' 'dataset:arrow'
 'base_model:Helsinki-NLP/opus-mt-en-ro'
 'base_model:finetune:Helsinki-NLP/opus-mt-en-ro' 'license:apache-2.0'
 'model-index' 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Mar19_09-21-49_43de2ec8c46d/events.out.tfevents.1710840172.43de2ec8c46d.1689.0'}
 {'rfilename': 'runs/Mar19_10-24-03_43de2ec8c46d/events.out.tfevents.1710843844.43de2ec8c46d.1689.1'}
 {'rfilename': 'source.spm'} {'rfilename': 'special_tokens_map.json'}
 {'rfilename': 'target.spm'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'} {'rfilename': 'vocab.json'}]",2024-03-19 09:22:48,transformers,"{'auto_model': 'AutoModelForSeq2SeqLM', 'custom_class': None, 'pipeline_tag': 'text2text-generation', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f1759b0,Helsinki-NLP/opus-mt-en-ro,kalese/opus-mt-en-bkm-10e,"license: apache-2.0
base_model: Helsinki-NLP/opus-mt-en-ro
tags:
- generated_from_trainer
datasets:
- arrow
metrics:
- bleu
model-index:
- name: opus-mt-en-bkm-10e
  results:
  - task:
      name: Sequence-to-sequence Language Modeling
      type: text2text-generation
    dataset:
      name: arrow
      type: arrow
      config: default
      split: train
      args: default
    metrics:
    - name: Bleu
      type: bleu
      value: 7.1794","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# opus-mt-en-bkm-10e

This model is a fine-tuned version of [Helsinki-NLP/opus-mt-en-ro](https://huggingface.co/Helsinki-NLP/opus-mt-en-ro) on the arrow dataset.
It achieves the following results on the evaluation set:
- Loss: 1.5644
- Bleu: 7.1794
- Gen Len: 60.0222

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 96
- eval_batch_size: 96
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 10

### Training results

| Training Loss | Epoch | Step | Validation Loss | Bleu   | Gen Len |
|:-------------:|:-----:|:----:|:---------------:|:------:|:-------:|
| No log        | 1.0   | 186  | 2.6868          | 0.299  | 76.178  |
| No log        | 2.0   | 372  | 2.0190          | 2.2444 | 67.167  |
| 3.115         | 3.0   | 558  | 1.8364          | 4.5959 | 59.7357 |
| 3.115         | 4.0   | 744  | 1.7372          | 5.1827 | 61.7218 |
| 3.115         | 5.0   | 930  | 1.6732          | 5.8295 | 59.7346 |
| 1.8706        | 6.0   | 1116 | 1.6301          | 6.4389 | 60.9085 |
| 1.8706        | 7.0   | 1302 | 1.6002          | 6.6498 | 60.4191 |
| 1.8706        | 8.0   | 1488 | 1.5792          | 6.8315 | 60.2721 |
| 1.7133        | 9.0   | 1674 | 1.5680          | 7.1239 | 60.5609 |
| 1.7133        | 10.0  | 1860 | 1.5644          | 7.1794 | 60.0222 |


### Framework versions

- Transformers 4.38.2
- Pytorch 2.2.1+cu121
- Datasets 2.18.0
- Tokenizers 0.15.2",1882.0,3.778618013416777e-06
MamalexKing/Addy,67a66b775fab975ae107453f,MamalexKing,"{""tags"": [""text-to-image"", ""lora"", ""diffusers"", ""template:diffusion-lora""], ""widget"": [{""text"": ""Adaarok the big bearded muscular skinhead men,Adaarok muscular men warrior bearded skinhead,Adaarok muscular men warrior bearded skinhead,"", ""output"": {""url"": ""https://huggingface.co/MamalexKing/Addy/resolve/main/images/10bf21751375297949278efb38f9ada9cdf21f4088725f89b00d0feca005de1e.png""}}, {""text"": ""Adaarok the big bearded muscular skinhead men,Adaarok muscular men warrior bearded skinhead,Adaarok muscular men warrior bearded skinhead,"", ""output"": {""url"": ""https://huggingface.co/MamalexKing/Addy/resolve/main/images/43f8b4429f7777df8262857425a9834d5baf7fe30125ea7df564af963c85498f.png""}}], ""base_model"": ""black-forest-labs/FLUX.1-dev"", ""instance_prompt"": ""Adaarok big bearded big muscular skinhead men"", ""license"": ""apache-2.0"", ""datasets"": [""Rapidata/flux1.1-likert-scale-preference""], ""language"": [""en""], ""metrics"": [""character""], ""pipeline_tag"": ""text-to-image"", ""library_name"": ""diffusers""}","[{'_id': '680b71dfa07785a516f270ef', 'adapter': 'lora', 'adapterWeightsPath': 'flux-lora-000011-adaarok.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/flux-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}
 {'_id': '6818c94d5111343b72f18ab4', 'adapter': 'lora', 'adapterWeightsPath': 'flux-lora-000011-adaarok.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'replicate', 'providerDetails': None, 'providerId': 'black-forest-labs/flux-dev-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2025-02-07 20:47:01,0,0.0,,13,239,,"['diffusers' 'text-to-image' 'lora' 'template:diffusion-lora' 'en'
 'dataset:Rapidata/flux1.1-likert-scale-preference'
 'base_model:black-forest-labs/FLUX.1-dev'
 'base_model:adapter:black-forest-labs/FLUX.1-dev' 'license:apache-2.0'
 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'flux-lora-000011-adaarok.safetensors'}
 {'rfilename': 'images/10bf21751375297949278efb38f9ada9cdf21f4088725f89b00d0feca005de1e.png'}
 {'rfilename': 'images/43f8b4429f7777df8262857425a9834d5baf7fe30125ea7df564af963c85498f.png'}]",2025-02-07 20:22:15,diffusers,,,adapter,66aaa908fc35e079a941470d,black-forest-labs/FLUX.1-dev,MamalexKing/Addy,"tags:
- text-to-image
- lora
- diffusers
- template:diffusion-lora
widget:
- text: >-
    Adaarok the big bearded muscular skinhead men,Adaarok muscular men warrior
    bearded skinhead,Adaarok muscular men warrior bearded skinhead,
  output:
    url: >-
      images/10bf21751375297949278efb38f9ada9cdf21f4088725f89b00d0feca005de1e.png
- text: >-
    Adaarok the big bearded muscular skinhead men,Adaarok muscular men warrior
    bearded skinhead,Adaarok muscular men warrior bearded skinhead,
  output:
    url: >-
      images/43f8b4429f7777df8262857425a9834d5baf7fe30125ea7df564af963c85498f.png
base_model: black-forest-labs/FLUX.1-dev
instance_prompt: Adaarok big bearded big muscular skinhead men
license: apache-2.0
datasets:
- Rapidata/flux1.1-likert-scale-preference
language:
- en
metrics:
- character
pipeline_tag: text-to-image
library_name: diffusers","# FLUX LORA

<Gallery />

## Model description 

Adaarok
![0e8ce4ee41f83e6176bb737b21dc4fd89c90cc6b0cb40b6bd25809c740818e2f.png](https:&#x2F;&#x2F;cdn-uploads.huggingface.co&#x2F;production&#x2F;uploads&#x2F;6611a5b0ba1204325e0fb613&#x2F;2Sz4YaQaRKaHSbJmosEUQ.png)


## Trigger words

You should use `Adaarok big bearded big muscular skinhead men` to trigger the image generation.


## Download model

Weights for this model are available in Safetensors format.

[Download](/MamalexKing/Addy/tree/main) them in the Files & versions tab.",536.0,3.675246534338284e-06
simonmok/xlm-roberta-base-finetuned-panx-en,67063a4228bea569ee82a30d,simonmok,"{""library_name"": ""transformers"", ""license"": ""mit"", ""base_model"": ""xlm-roberta-base"", ""tags"": [""generated_from_trainer""], ""metrics"": [""f1""], ""model-index"": [{""name"": ""xlm-roberta-base-finetuned-panx-en"", ""results"": []}]}",[],2024-10-09 08:12:13,0,0.0,"{""architectures"": [""XLMRobertaForTokenClassification""], ""model_type"": ""xlm-roberta"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""cls_token"": ""<s>"", ""eos_token"": ""</s>"", ""mask_token"": ""<mask>"", ""pad_token"": ""<pad>"", ""sep_token"": ""</s>"", ""unk_token"": ""<unk>""}}",0,237,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 277459208, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 277459208.0}","['transformers' 'safetensors' 'xlm-roberta' 'token-classification'
 'generated_from_trainer' 'base_model:FacebookAI/xlm-roberta-base'
 'base_model:finetune:FacebookAI/xlm-roberta-base' 'license:mit'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",token-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'sentencepiece.bpe.model'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2024-10-09 08:09:38,transformers,"{'auto_model': 'AutoModelForTokenClassification', 'custom_class': None, 'pipeline_tag': 'token-classification', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174364,FacebookAI/xlm-roberta-base,simonmok/xlm-roberta-base-finetuned-panx-en,"library_name: transformers
license: mit
base_model: xlm-roberta-base
tags:
- generated_from_trainer
metrics:
- f1
model-index:
- name: xlm-roberta-base-finetuned-panx-en
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# xlm-roberta-base-finetuned-panx-en

This model is a fine-tuned version of [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3905
- F1: 0.6861

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 24
- eval_batch_size: 24
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss | F1     |
|:-------------:|:-----:|:----:|:---------------:|:------:|
| 1.0479        | 1.0   | 50   | 0.4854          | 0.5857 |
| 0.4604        | 2.0   | 100  | 0.3995          | 0.6605 |
| 0.3797        | 3.0   | 150  | 0.3905          | 0.6861 |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.1
- Tokenizers 0.19.1",1316.0,3.669634892880012e-06
JasonBounre/sft-chatbot,670e0ddbc30879131761cd93,JasonBounre,"{""base_model"": ""meta-llama/Meta-Llama-3-8B"", ""library_name"": ""peft"", ""license"": ""llama3"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""sft-chatbot"", ""results"": []}]}",[],2024-10-24 09:22:24,0,0.0,"{""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|end_of_text|>"", ""pad_token"": ""<|end_of_text|>""}, ""peft"": {""base_model_name_or_path"": ""meta-llama/Meta-Llama-3-8B"", ""task_type"": ""CAUSAL_LM""}}",3,229,,"['peft' 'safetensors' 'generated_from_trainer'
 'base_model:meta-llama/Meta-Llama-3-8B'
 'base_model:adapter:meta-llama/Meta-Llama-3-8B' 'license:llama3'
 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'}
 {'rfilename': 'adapter_model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2024-10-15 06:38:19,peft,,,adapter,661f97d48e7f3438386f755d,meta-llama/Meta-Llama-3-8B,JasonBounre/sft-chatbot,"base_model: meta-llama/Meta-Llama-3-8B
library_name: peft
license: llama3
tags:
- generated_from_trainer
model-index:
- name: sft-chatbot
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# sft-chatbot

This model is a fine-tuned version of [meta-llama/Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8663

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 4
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 8
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- num_epochs: 6
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| No log        | 0.9836 | 30   | 0.7847          |
| No log        | 2.0    | 61   | 0.7587          |
| No log        | 2.9836 | 91   | 0.7729          |
| No log        | 4.0    | 122  | 0.8109          |
| No log        | 4.9836 | 152  | 0.8601          |
| No log        | 5.9016 | 180  | 0.8663          |


### Framework versions

- PEFT 0.13.1
- Transformers 4.43.3
- Pytorch 2.4.0+cu121
- Datasets 3.0.1
- Tokenizers 0.19.1",1529.0,3.64670659992858e-06
abbassix/original_oomr_800,659552fd372a71b317a15e9d,abbassix,"{""license"": ""apache-2.0"", ""base_model"": ""bert-base-uncased"", ""tags"": [""generated_from_trainer""], ""metrics"": [""accuracy""], ""model-index"": [{""name"": ""original_oomr_800"", ""results"": []}]}",[],2024-01-03 12:48:40,0,0.0,"{""architectures"": [""BertForSequenceClassification""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",0,219,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 109484546, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 109484546.0}","['transformers' 'tensorboard' 'safetensors' 'bert' 'text-classification'
 'generated_from_trainer' 'base_model:google-bert/bert-base-uncased'
 'base_model:finetune:google-bert/bert-base-uncased' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Jan03_12-13-23_8e92d7ba8236/events.out.tfevents.1704284010.8e92d7ba8236.436.2'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2024-01-03 12:28:45,transformers,"{'auto_model': 'AutoModelForSequenceClassification', 'custom_class': None, 'pipeline_tag': 'text-classification', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174338,google-bert/bert-base-uncased,abbassix/original_oomr_800,"license: apache-2.0
base_model: bert-base-uncased
tags:
- generated_from_trainer
metrics:
- accuracy
model-index:
- name: original_oomr_800
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# original_oomr_800

This model is a fine-tuned version of [bert-base-uncased](https://huggingface.co/bert-base-uncased) on the [ComNum](https://huggingface.co/datasets/abbassix/ComNum) dataset.
This model used 800 samples as training, 200 as validation, and 1200 as test on three epochs.
- Loss: 0.3552
- Accuracy: 0.765

This model achieves the following results on the test set:
- Loss: 0.3520
- Accuracy: 0.75
<!--
{'eval_loss': 0.35200029611587524, 'eval_accuracy': 0.75, 'eval_runtime': 987.9512, 'eval_samples_per_second': 10.122, 'eval_steps_per_second': 1.265}
-->

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3.0

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| No log        | 1.0   | 100  | 0.3559          | 0.765    |
| No log        | 2.0   | 200  | 0.3547          | 0.765    |
| No log        | 3.0   | 300  | 0.3552          | 0.765    |


### Framework versions

- Transformers 4.36.2
- Pytorch 2.1.0+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0",1657.0,3.6168978151100997e-06
hw2942/chinese-roberta-climate-transition-physical-risk-prediction-v2,66f0cbd51a17deb2b383e0b8,hw2942,"{""base_model"": ""hfl/chinese-roberta-wwm-ext"", ""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""metrics"": [""accuracy""], ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""chinese-roberta-climate-transition-physical-risk-prediction-v2"", ""results"": []}]}",[],2024-09-23 02:10:58,0,0.0,"{""architectures"": [""BertForSequenceClassification""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",3,219,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 102269186, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 102269186.0}","['transformers' 'tensorboard' 'safetensors' 'bert' 'text-classification'
 'generated_from_trainer' 'base_model:hfl/chinese-roberta-wwm-ext'
 'base_model:finetune:hfl/chinese-roberta-wwm-ext' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Sep23_02-00-53_6070e04615ab/events.out.tfevents.1727056855.6070e04615ab.1231.2'}
 {'rfilename': 'special_tokens_map.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2024-09-23 02:00:53,transformers,"{'auto_model': 'AutoModelForSequenceClassification', 'custom_class': None, 'pipeline_tag': 'text-classification', 'processor': 'AutoTokenizer'}",,finetune,621ffdc136468d709f17bab8,hfl/chinese-roberta-wwm-ext,hw2942/chinese-roberta-climate-transition-physical-risk-prediction-v2,"base_model: hfl/chinese-roberta-wwm-ext
library_name: transformers
license: apache-2.0
metrics:
- accuracy
tags:
- generated_from_trainer
model-index:
- name: chinese-roberta-climate-transition-physical-risk-prediction-v2
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# chinese-roberta-climate-transition-physical-risk-prediction-v2

This model is a fine-tuned version of [hfl/chinese-roberta-wwm-ext](https://huggingface.co/hfl/chinese-roberta-wwm-ext) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.1642
- Accuracy: 0.98

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 10

### Training results

| Training Loss | Epoch | Step | Validation Loss | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:--------:|
| No log        | 1.0   | 57   | 0.0004          | 1.0      |
| No log        | 2.0   | 114  | 0.0004          | 1.0      |
| No log        | 3.0   | 171  | 0.0431          | 0.98     |
| No log        | 4.0   | 228  | 0.1605          | 0.98     |
| No log        | 5.0   | 285  | 0.1491          | 0.98     |
| No log        | 6.0   | 342  | 0.1602          | 0.98     |
| No log        | 7.0   | 399  | 0.1613          | 0.98     |
| No log        | 8.0   | 456  | 0.1631          | 0.98     |
| 0.0106        | 9.0   | 513  | 0.1639          | 0.98     |
| 0.0106        | 10.0  | 570  | 0.1642          | 0.98     |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.0
- Tokenizers 0.19.1",1813.0,3.6168978151100997e-06
seddiktrk/bert-base-uncased-issues-128,66893555b9ea94733adddb39,seddiktrk,"{""license"": ""apache-2.0"", ""base_model"": ""bert-base-uncased"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""bert-base-uncased-issues-128"", ""results"": []}]}",[],2024-07-06 13:24:03,0,0.0,"{""architectures"": [""BertForMaskedLM""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",1,215,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 109514298, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 109514298.0}","['transformers' 'safetensors' 'bert' 'fill-mask' 'generated_from_trainer'
 'base_model:google-bert/bert-base-uncased'
 'base_model:finetune:google-bert/bert-base-uncased' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",fill-mask,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2024-07-06 12:15:17,transformers,"{'auto_model': 'AutoModelForMaskedLM', 'custom_class': None, 'pipeline_tag': 'fill-mask', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174338,google-bert/bert-base-uncased,seddiktrk/bert-base-uncased-issues-128,"license: apache-2.0
base_model: bert-base-uncased
tags:
- generated_from_trainer
model-index:
- name: bert-base-uncased-issues-128
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bert-base-uncased-issues-128

This model is a fine-tuned version of [bert-base-uncased](https://huggingface.co/bert-base-uncased) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 1.2436

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 32
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 16

### Training results

| Training Loss | Epoch | Step | Validation Loss |
|:-------------:|:-----:|:----:|:---------------:|
| 2.1037        | 1.0   | 291  | 1.6976          |
| 1.6309        | 2.0   | 582  | 1.5133          |
| 1.4962        | 3.0   | 873  | 1.3515          |
| 1.3944        | 4.0   | 1164 | 1.3300          |
| 1.3308        | 5.0   | 1455 | 1.2333          |
| 1.2835        | 6.0   | 1746 | 1.3543          |
| 1.2308        | 7.0   | 2037 | 1.3045          |
| 1.2023        | 8.0   | 2328 | 1.3405          |
| 1.1662        | 9.0   | 2619 | 1.2236          |
| 1.1389        | 10.0  | 2910 | 1.1759          |
| 1.1279        | 11.0  | 3201 | 1.1468          |
| 1.1096        | 12.0  | 3492 | 1.1844          |
| 1.0896        | 13.0  | 3783 | 1.2305          |
| 1.0762        | 14.0  | 4074 | 1.2072          |
| 1.0721        | 15.0  | 4365 | 1.2287          |
| 1.0638        | 16.0  | 4656 | 1.2436          |


### Framework versions

- Transformers 4.41.2
- Pytorch 2.3.0+cu121
- Datasets 2.20.0
- Tokenizers 0.19.1",1916.0,3.6045931168364795e-06
ryzax/1.5B-v18,68313778538c5c10a1c0b212,ryzax,"{""base_model"": ""ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4"", ""datasets"": ""agentica-org/DeepScaleR-Preview-Dataset"", ""library_name"": ""transformers"", ""model_name"": ""1.5B-v18"", ""tags"": [""generated_from_trainer"", ""open-r1"", ""trl"", ""grpo""], ""licence"": ""license""}",[],2025-06-09 03:48:04,0,0.0,"{""architectures"": [""Qwen3ForCausalLM""], ""model_type"": ""qwen3"", ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \""# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \""user\"" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\""name\"": \""' }}\n                {{- tool_call.name }}\n                {{- '\"", \""arguments\"": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}}",0,215,"{'parameters': {'BF16': 2030654464.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 2030654464.0}","['transformers' 'safetensors' 'qwen3' 'text-generation'
 'generated_from_trainer' 'open-r1' 'trl' 'grpo' 'conversational'
 'dataset:agentica-org/DeepScaleR-Preview-Dataset' 'arxiv:2402.03300'
 'base_model:ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4'
 'base_model:finetune:ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'all_results.json'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'merges.txt'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'train_results.json'} {'rfilename': 'trainer_state.json'}
 {'rfilename': 'training_args.bin'} {'rfilename': 'vocab.json'}]",2025-05-24 03:05:28,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,68310e6c1a3bca3b92f04120,ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4,ryzax/1.5B-v18,"base_model: ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4
datasets: agentica-org/DeepScaleR-Preview-Dataset
library_name: transformers
model_name: 1.5B-v18
tags:
- generated_from_trainer
- open-r1
- trl
- grpo
licence: license","# Model Card for 1.5B-v18

This model is a fine-tuned version of [ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4](https://huggingface.co/ryzax/qwen3_1.7B_sft_correct_v3_1e-5_4) on the [agentica-org/DeepScaleR-Preview-Dataset](https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset) dataset.
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""ryzax/1.5B-v18"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/muennighoff/s2/runs/1hnb0ebv) 


This model was trained with GRPO, a method introduced in [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).

### Framework versions

- TRL: 0.17.0.dev0
- Transformers: 4.51.3
- Pytorch: 2.6.0
- Datasets: 3.4.1
- Tokenizers: 0.21.1

## Citations

Cite GRPO as:

```bibtex
@article{zhihong2024deepseekmath,
    title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
    author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
    year         = 2024,
    eprint       = {arXiv:2402.03300},
}

```

Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\'e}dec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```",2187.0,3.6045931168364795e-06
eunyoung2/model_output,670724f6b9c9f7ddac34babf,eunyoung2,"{""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""base_model"": ""beomi/kcbert-base"", ""tags"": [""ley"", ""categorical"", ""multi_label"", ""10_class"", ""generated_from_trainer""], ""model-index"": [{""name"": ""model_output"", ""results"": []}]}",[],2024-10-10 00:51:51,0,0.0,"{""architectures"": [""BertForSequenceClassification""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",1,201,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 108926218, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 108926218.0}","['transformers' 'tensorboard' 'safetensors' 'bert' 'text-classification'
 'ley' 'categorical' 'multi_label' '10_class' 'generated_from_trainer'
 'base_model:beomi/kcbert-base' 'base_model:finetune:beomi/kcbert-base'
 'license:apache-2.0' 'autotrain_compatible' 'endpoints_compatible'
 'region:us']",text-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Oct10_00-20-48_5be7eb960424/events.out.tfevents.1728519972.5be7eb960424.202.0'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2024-10-10 00:51:02,transformers,"{'auto_model': 'AutoModelForSequenceClassification', 'custom_class': None, 'pipeline_tag': 'text-classification', 'processor': 'AutoTokenizer'}",,finetune,621ffdc136468d709f1798c8,beomi/kcbert-base,eunyoung2/model_output,"library_name: transformers
license: apache-2.0
base_model: beomi/kcbert-base
tags:
- ley
- categorical
- multi_label
- 10_class
- generated_from_trainer
model-index:
- name: model_output
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# model_output

This model is a fine-tuned version of [beomi/kcbert-base](https://huggingface.co/beomi/kcbert-base) on the unsmile_data dataset.
It achieves the following results on the evaluation set:
- Loss: 0.1294
- Lrap: 0.8838

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 64
- eval_batch_size: 64
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 5

### Training results

| Training Loss | Epoch | Step | Validation Loss | Lrap   |
|:-------------:|:-----:|:----:|:---------------:|:------:|
| No log        | 1.0   | 235  | 0.1492          | 0.8575 |
| No log        | 2.0   | 470  | 0.1294          | 0.8728 |
| 0.1739        | 3.0   | 705  | 0.1215          | 0.8852 |
| 0.1739        | 4.0   | 940  | 0.1282          | 0.8831 |
| 0.0781        | 5.0   | 1175 | 0.1294          | 0.8838 |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.1+cu121
- Datasets 3.0.1
- Tokenizers 0.19.1",1424.0,3.5596565895126146e-06
macadeliccc/Liberated-Qwen1.5-14B-AWQ,65f205acda08bddc241e6f31,macadeliccc,"{""license"": ""apache-2.0"", ""base_model"": ""abacusai/Liberated-Qwen1.5-14B""}",[],2024-03-13 21:41:29,0,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""quantization_config"": {""bits"": 4, ""quant_method"": ""awq""}, ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful assistant.' %}{% endif %}{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{{'<|im_start|>system\n' + system_message + '<|im_end|>\n'}}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}}",0,195,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': 1656673280, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': 1588454400, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 3245127680.0}","['transformers' 'safetensors' 'qwen2' 'text-generation' 'conversational'
 'base_model:abacusai/Liberated-Qwen1.5-14B'
 'base_model:quantized:abacusai/Liberated-Qwen1.5-14B'
 'license:apache-2.0' 'autotrain_compatible' 'text-generation-inference'
 'endpoints_compatible' '4-bit' 'awq' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model-00001-of-00002.safetensors'}
 {'rfilename': 'model-00002-of-00002.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2024-03-13 19:59:40,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,quantized,65e6b17209de4d8e68afe845,abacusai/Liberated-Qwen1.5-14B,macadeliccc/Liberated-Qwen1.5-14B-AWQ,"license: apache-2.0
base_model: abacusai/Liberated-Qwen1.5-14B","![image/png](https://cdn-uploads.huggingface.co/production/uploads/6455cc8d679315e4ef16fbec/bAcidiZMewJnXuJ6o9AlA.png)


4bit AWQ",129.0,3.539436346838971e-06
mradermacher/cmaesar-br-v1.0-GGUF,684bf59df3ac55e1020a5567,mradermacher,"{""base_model"": ""abhishekchohan/cmaesar-br-v1.0"", ""language"": [""en""], ""library_name"": ""transformers"", ""quantized_by"": ""mradermacher""}",[],2025-06-13 11:33:07,0,0.0,,1,182,,"['transformers' 'gguf' 'en' 'base_model:abhishekchohan/cmaesar-br-v1.0'
 'base_model:quantized:abhishekchohan/cmaesar-br-v1.0'
 'endpoints_compatible' 'region:us' 'conversational']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'cmaesar-br-v1.0.IQ4_XS.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q2_K.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q3_K_L.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q3_K_M.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q3_K_S.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q4_K_M.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q4_K_S.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q5_K_M.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q5_K_S.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q6_K.gguf'}
 {'rfilename': 'cmaesar-br-v1.0.Q8_0.gguf'}]",2025-06-13 09:55:41,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 32763876352, ""architecture"": ""qwen2"", ""context_length"": 40960, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- '' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n  {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" and not message.tool_calls %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {%- endif %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {%- set content = message.content %}\n        {%- if not loop.last %}\n            {%- set content = message.content.split('</think>')[-1].lstrip('\\n') %}\n        {%- endif %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n<think>\\n' }}\n{%- endif %}\n"", ""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|im_end|>""}",quantized,684b1302c017ba3a4ed3c656,abhishekchohan/cmaesar-br-v1.0,mradermacher/cmaesar-br-v1.0-GGUF,"base_model: abhishekchohan/cmaesar-br-v1.0
language:
- en
library_name: transformers
quantized_by: mradermacher","## About

<!-- ### quantize_version: 2 -->
<!-- ### output_tensor_quantised: 1 -->
<!-- ### convert_type: hf -->
<!-- ### vocab_type:  -->
<!-- ### tags:  -->
static quants of https://huggingface.co/abhishekchohan/cmaesar-br-v1.0

<!-- provided-files -->
weighted/imatrix quants seem not to be available (by me) at this time. If they do not show up a week or so after the static ones, I have probably not planned for them. Feel free to request them by opening a Community Discussion.
## Usage

If you are unsure how to use GGUF files, refer to one of [TheBloke's
READMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for
more details, including on how to concatenate multi-part files.

## Provided Quants

(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)

| Link | Type | Size/GB | Notes |
|:-----|:-----|--------:|:------|
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q2_K.gguf) | Q2_K | 12.4 |  |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q3_K_S.gguf) | Q3_K_S | 14.5 |  |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q3_K_M.gguf) | Q3_K_M | 16.0 | lower quality |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q3_K_L.gguf) | Q3_K_L | 17.3 |  |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.IQ4_XS.gguf) | IQ4_XS | 18.0 |  |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q4_K_S.gguf) | Q4_K_S | 18.9 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q4_K_M.gguf) | Q4_K_M | 20.0 | fast, recommended |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q5_K_S.gguf) | Q5_K_S | 22.7 |  |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q5_K_M.gguf) | Q5_K_M | 23.4 |  |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q6_K.gguf) | Q6_K | 27.0 | very good quality |
| [GGUF](https://huggingface.co/mradermacher/cmaesar-br-v1.0-GGUF/resolve/main/cmaesar-br-v1.0.Q8_0.gguf) | Q8_0 | 34.9 | fast, best quality |

Here is a handy graph by ikawrakow comparing some lower-quality quant
types (lower is better):

![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)

And here are Artefact2's thoughts on the matter:
https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9

## FAQ / Model Request

See https://huggingface.co/mradermacher/model_requests for some answers to
questions you might have and/or if you want some other model quantized.

## Thanks

I thank my company, [nethype GmbH](https://www.nethype.de/), for letting
me use its servers and providing upgrades to my workstation to enable
this work in my free time.

<!-- end -->",3042.0,3.4934149460878455e-06
Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF,67916d2574baf4309c3cc06f,Triangle104,"{""license"": ""apache-2.0"", ""license_link"": ""https://huggingface.co/huihui-ai/Dria-Agent-a-7B-abliterated/blob/main/LICENSE"", ""language"": [""en""], ""base_model"": ""huihui-ai/Dria-Agent-a-7B-abliterated"", ""pipeline_tag"": ""text-generation"", ""library_name"": ""transformers"", ""tags"": [""code"", ""chat"", ""qwen"", ""qwen-coder"", ""agent"", ""abliterated"", ""uncensored"", ""llama-cpp"", ""gguf-my-repo""]}",[],2025-01-22 22:12:14,0,0.0,,11,161,,"['transformers' 'gguf' 'code' 'chat' 'qwen' 'qwen-coder' 'agent'
 'abliterated' 'uncensored' 'llama-cpp' 'gguf-my-repo' 'text-generation'
 'en' 'base_model:huihui-ai/Dria-Agent-a-7B-abliterated'
 'base_model:quantized:huihui-ai/Dria-Agent-a-7B-abliterated'
 'license:apache-2.0' 'endpoints_compatible' 'region:us' 'conversational']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'dria-agent-a-7b-abliterated-q5_k_m.gguf'}]",2025-01-22 22:11:49,transformers,"{'auto_model': 'AutoModel', 'custom_class': None, 'pipeline_tag': None, 'processor': None}","{""total"": 7615616512, ""architecture"": ""qwen2"", ""context_length"": 32768, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) or (message.role == \""assistant\"" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"", ""bos_token"": ""<|endoftext|>"", ""eos_token"": ""<|im_end|>""}",quantized,678e47dd14153f4ba16f8729,huihui-ai/Dria-Agent-a-7B-abliterated,Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF,"license: apache-2.0
license_link: https://huggingface.co/huihui-ai/Dria-Agent-a-7B-abliterated/blob/main/LICENSE
language:
- en
base_model: huihui-ai/Dria-Agent-a-7B-abliterated
pipeline_tag: text-generation
library_name: transformers
tags:
- code
- chat
- qwen
- qwen-coder
- agent
- abliterated
- uncensored
- llama-cpp
- gguf-my-repo","# Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF
This model was converted to GGUF format from [`huihui-ai/Dria-Agent-a-7B-abliterated`](https://huggingface.co/huihui-ai/Dria-Agent-a-7B-abliterated) using llama.cpp via the ggml.ai's [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space.
Refer to the [original model card](https://huggingface.co/huihui-ai/Dria-Agent-a-7B-abliterated) for more details on the model.

## Use with llama.cpp
Install llama.cpp through brew (works on Mac and Linux)

```bash
brew install llama.cpp

```
Invoke the llama.cpp server or the CLI.

### CLI:
```bash
llama-cli --hf-repo Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF --hf-file dria-agent-a-7b-abliterated-q5_k_m.gguf -p ""The meaning to life and the universe is""
```

### Server:
```bash
llama-server --hf-repo Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF --hf-file dria-agent-a-7b-abliterated-q5_k_m.gguf -c 2048
```

Note: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.

Step 1: Clone llama.cpp from GitHub.
```
git clone https://github.com/ggerganov/llama.cpp
```

Step 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).
```
cd llama.cpp && LLAMA_CURL=1 make
```

Step 3: Run inference through the main binary.
```
./llama-cli --hf-repo Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF --hf-file dria-agent-a-7b-abliterated-q5_k_m.gguf -p ""The meaning to life and the universe is""
```
or 
```
./llama-server --hf-repo Triangle104/Dria-Agent-a-7B-abliterated-Q5_K_M-GGUF --hf-file dria-agent-a-7b-abliterated-q5_k_m.gguf -c 2048
```",1797.0,3.4116771895954748e-06
tahertadpatri/youtubeMcqgen,65edd5d5fb20bf78df6eec27,tahertadpatri,"{""license"": ""apache-2.0"", ""base_model"": ""bert-base-uncased"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""youtubeMcqgen"", ""results"": []}]}",[],2024-03-10 16:09:18,0,0.0,"{""architectures"": [""BertForMultipleChoice""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",0,160,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 109483009, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 109483009.0}","['transformers' 'tensorboard' 'safetensors' 'bert' 'multiple-choice'
 'generated_from_trainer' 'base_model:google-bert/bert-base-uncased'
 'base_model:finetune:google-bert/bert-base-uncased' 'license:apache-2.0'
 'endpoints_compatible' 'region:us']",multiple-choice,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Mar10_15-46-13_9956fec7d747/events.out.tfevents.1710085596.9956fec7d747.13140.0'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2024-03-10 15:46:29,transformers,"{'auto_model': 'AutoModelForMultipleChoice', 'custom_class': None, 'pipeline_tag': 'multiple-choice', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174338,google-bert/bert-base-uncased,tahertadpatri/youtubeMcqgen,"license: apache-2.0
base_model: bert-base-uncased
tags:
- generated_from_trainer
model-index:
- name: youtubeMcqgen
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# youtubeMcqgen

This model is a fine-tuned version of [bert-base-uncased](https://huggingface.co/bert-base-uncased) on an unknown dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 3

### Framework versions

- Transformers 4.38.2
- Pytorch 2.1.0+cu121
- Datasets 2.18.0
- Tokenizers 0.15.2",889.0,3.407524933350774e-06
Astral7/bert_base_cased_qa,65dc678fbda21d181b1f96bb,Astral7,"{""license"": ""apache-2.0"", ""tags"": [""generated_from_keras_callback""], ""base_model"": ""bert-base-cased"", ""model-index"": [{""name"": ""bert_base_cased_qa"", ""results"": []}]}",[],2024-02-29 12:25:05,0,0.0,"{""architectures"": [""BertForQuestionAnswering""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",0,153,,"['transformers' 'tf' 'bert' 'question-answering'
 'generated_from_keras_callback' 'base_model:google-bert/bert-base-cased'
 'base_model:finetune:google-bert/bert-base-cased' 'license:apache-2.0'
 'endpoints_compatible' 'region:us']",question-answering,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'special_tokens_map.json'}
 {'rfilename': 'tf_model.h5'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.txt'}]",2024-02-26 10:27:27,transformers,"{'auto_model': 'AutoModelForQuestionAnswering', 'custom_class': None, 'pipeline_tag': 'question-answering', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174331,google-bert/bert-base-cased,Astral7/bert_base_cased_qa,"license: apache-2.0
tags:
- generated_from_keras_callback
base_model: bert-base-cased
model-index:
- name: bert_base_cased_qa
  results: []","<!-- This model card has been generated automatically according to the information Keras had access to. You should
probably proofread and complete it, then remove this comment. -->

# bert_base_cased_qa

This model is a fine-tuned version of [bert-base-cased](https://huggingface.co/bert-base-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Train Loss: 1.2898
- Epoch: 0

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- optimizer: {'inner_optimizer': {'module': 'transformers.optimization_tf', 'class_name': 'AdamWeightDecay', 'config': {'name': 'AdamWeightDecay', 'learning_rate': {'module': 'keras.optimizers.schedules', 'class_name': 'PolynomialDecay', 'config': {'initial_learning_rate': 2e-05, 'decay_steps': 5545, 'end_learning_rate': 0.0, 'power': 1.0, 'cycle': False, 'name': None}, 'registered_name': None}, 'decay': 0.0, 'beta_1': 0.8999999761581421, 'beta_2': 0.9990000128746033, 'epsilon': 1e-08, 'amsgrad': False, 'weight_decay_rate': 0.01}, 'registered_name': 'AdamWeightDecay'}, 'dynamic': True, 'initial_scale': 32768.0, 'dynamic_growth_steps': 2000}
- training_precision: mixed_float16

### Training results

| Train Loss | Epoch |
|:----------:|:-----:|
| 1.2898     | 0     |


### Framework versions

- Transformers 4.37.2
- TensorFlow 2.15.0
- Datasets 2.17.1
- Tokenizers 0.15.2",1567.0,3.3777161485322946e-06
Ariffiq99/KUCI_xlm_roberta_large_Finetuned,6653d30d868d2c5a7ce83585,Ariffiq99,"{""license"": ""mit"", ""base_model"": ""FacebookAI/xlm-roberta-large"", ""tags"": [""generated_from_trainer""], ""metrics"": [""f1""], ""model-index"": [{""name"": ""KUCI_xlm_roberta_large_Finetuned"", ""results"": []}]}",[],2024-05-27 21:12:57,0,0.0,"{""architectures"": [""XLMRobertaForMultipleChoice""], ""model_type"": ""xlm-roberta"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""cls_token"": ""<s>"", ""eos_token"": ""</s>"", ""mask_token"": ""<mask>"", ""pad_token"": ""<pad>"", ""sep_token"": ""</s>"", ""unk_token"": ""<unk>""}}",1,149,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 559891457, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 559891457.0}","['transformers' 'tensorboard' 'safetensors' 'xlm-roberta'
 'multiple-choice' 'generated_from_trainer'
 'base_model:FacebookAI/xlm-roberta-large'
 'base_model:finetune:FacebookAI/xlm-roberta-large' 'license:mit'
 'endpoints_compatible' 'region:us']",multiple-choice,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/May27_00-25-48_a363ce93fab4/events.out.tfevents.1716769549.a363ce93fab4.7781.0'}
 {'rfilename': 'runs/May27_00-27-24_a363ce93fab4/events.out.tfevents.1716769644.a363ce93fab4.7781.1'}
 {'rfilename': 'runs/May27_00-27-38_a363ce93fab4/events.out.tfevents.1716769658.a363ce93fab4.7781.2'}
 {'rfilename': 'runs/May27_00-28-13_a363ce93fab4/events.out.tfevents.1716769694.a363ce93fab4.7781.3'}
 {'rfilename': 'runs/May27_15-12-48_bf20e0192dc7/events.out.tfevents.1716822770.bf20e0192dc7.2655.0'}
 {'rfilename': 'sentencepiece.bpe.model'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2024-05-27 00:25:49,transformers,"{'auto_model': 'AutoModelForMultipleChoice', 'custom_class': None, 'pipeline_tag': 'multiple-choice', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174369,FacebookAI/xlm-roberta-large,Ariffiq99/KUCI_xlm_roberta_large_Finetuned,"license: mit
base_model: FacebookAI/xlm-roberta-large
tags:
- generated_from_trainer
metrics:
- f1
model-index:
- name: KUCI_xlm_roberta_large_Finetuned
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# KUCI_xlm_roberta_large_Finetuned

This model is a fine-tuned version of [FacebookAI/xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.8479
- F1: 0.8503

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 1e-05
- train_batch_size: 16
- eval_batch_size: 16
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 4

### Training results

| Training Loss | Epoch | Step  | Validation Loss | F1     |
|:-------------:|:-----:|:-----:|:---------------:|:------:|
| 0.4577        | 1.0   | 5196  | 0.4667          | 0.8350 |
| 0.3242        | 2.0   | 10392 | 0.5279          | 0.8406 |
| 0.1904        | 3.0   | 15588 | 0.6255          | 0.8510 |
| 0.1002        | 4.0   | 20784 | 0.8479          | 0.8503 |


### Framework versions

- Transformers 4.41.0
- Pytorch 2.3.0+cu121
- Datasets 2.19.1
- Tokenizers 0.19.1",1405.0,3.3600680973580785e-06
chenggong1995/OLMoE-1B-7B-0125-Instruct-grpo,67b42f695c29912827dea8c1,chenggong1995,"{""base_model"": ""allenai/OLMoE-1B-7B-0125-Instruct"", ""library_name"": ""transformers"", ""model_name"": ""OLMoE-1B-7B-0125-Instruct-grpo"", ""tags"": [""generated_from_trainer"", ""trl"", ""grpo""], ""licence"": ""license""}",[],2025-02-25 01:54:04,0,0.0,"{""architectures"": [""OlmoeForCausalLM""], ""model_type"": ""olmoe"", ""tokenizer_config"": {""bos_token"": ""|||IP_ADDRESS|||"", ""chat_template"": ""{{ bos_token }}{% for message in messages %}{% if message['role'] == 'system' %}{{ '<|system|>\n' + message['content'] + '\n' }}{% elif message['role'] == 'user' %}{{ '<|user|>\n' + message['content'] + '\n' }}{% elif message['role'] == 'assistant' %}{% if not loop.last %}{{ '<|assistant|>\n'  + message['content'] + eos_token + '\n' }}{% else %}{{ '<|assistant|>\n'  + message['content'] + eos_token }}{% endif %}{% endif %}{% if loop.last and add_generation_prompt %}{{ '<|assistant|>\n' }}{% endif %}{% endfor %}"", ""eos_token"": ""|||IP_ADDRESS|||"", ""pad_token"": ""<pad>"", ""unk_token"": null}}",0,140,"{'parameters': {'BF16': 6919161856.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 6919161856.0}","['transformers' 'safetensors' 'olmoe' 'text-generation'
 'generated_from_trainer' 'trl' 'grpo' 'conversational' 'arxiv:2402.03300'
 'base_model:allenai/OLMoE-1B-7B-0125-Instruct'
 'base_model:finetune:allenai/OLMoE-1B-7B-0125-Instruct'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'all_results.json'} {'rfilename': 'config.json'}
 {'rfilename': 'eval_results.json'}
 {'rfilename': 'generation_config.json'}
 {'rfilename': 'model-00001-of-00003.safetensors'}
 {'rfilename': 'model-00002-of-00003.safetensors'}
 {'rfilename': 'model-00003-of-00003.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'train_results.json'} {'rfilename': 'trainer_state.json'}
 {'rfilename': 'training_args.bin'}]",2025-02-18 06:57:45,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,6797d65120c7240ceda5c1b8,allenai/OLMoE-1B-7B-0125-Instruct,chenggong1995/OLMoE-1B-7B-0125-Instruct-grpo,"base_model: allenai/OLMoE-1B-7B-0125-Instruct
library_name: transformers
model_name: OLMoE-1B-7B-0125-Instruct-grpo
tags:
- generated_from_trainer
- trl
- grpo
licence: license","# Model Card for OLMoE-1B-7B-0125-Instruct-grpo

This model is a fine-tuned version of [allenai/OLMoE-1B-7B-0125-Instruct](https://huggingface.co/allenai/OLMoE-1B-7B-0125-Instruct).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""chenggong1995/OLMoE-1B-7B-0125-Instruct-grpo"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/gongc1995-city-university-of-hong-kong/huggingface/runs/ywgmacxu) 


This model was trained with GRPO, a method introduced in [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).

### Framework versions

- TRL: 0.16.0.dev0
- Transformers: 4.49.0.dev0
- Pytorch: 2.5.1
- Datasets: 3.2.0
- Tokenizers: 0.21.0

## Citations

Cite GRPO as:

```bibtex
@article{zhihong2024deepseekmath,
    title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
    author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
    year         = 2024,
    eprint       = {arXiv:2402.03300},
}

```

Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```",2136.0,3.318575241095069e-06
Triangle104/Lumimaid-Rocinate-v0.1,66d2ee096accd34f755e73aa,Triangle104,"{""base_model"": [""NeverSleep/Lumimaid-v0.2-12B"", ""TheDrummer/Rocinante-12B-v1.1""], ""library_name"": ""transformers"", ""tags"": [""mergekit"", ""merge""]}",[],2024-08-31 11:50:45,4,0.0,"{""architectures"": [""MistralForCausalLM""], ""model_type"": ""mistral"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""unk_token"": ""<unk>""}}",1,134,"{'parameters': {'BF16': 12247782400.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 12247782400.0}","['transformers' 'safetensors' 'mistral' 'text-generation' 'mergekit'
 'merge' 'base_model:NeverSleep/Lumimaid-v0.2-12B'
 'base_model:merge:NeverSleep/Lumimaid-v0.2-12B'
 'base_model:TheDrummer/Rocinante-12B-v1.1'
 'base_model:merge:TheDrummer/Rocinante-12B-v1.1' 'autotrain_compatible'
 'text-generation-inference' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'mergekit_config.yml'}
 {'rfilename': 'model-00001-of-00005.safetensors'}
 {'rfilename': 'model-00002-of-00005.safetensors'}
 {'rfilename': 'model-00003-of-00005.safetensors'}
 {'rfilename': 'model-00004-of-00005.safetensors'}
 {'rfilename': 'model-00005-of-00005.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2024-08-31 10:18:49,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,merge,66be1717f1685a3ade8be5a3,TheDrummer/Rocinante-12B-v1.1,Triangle104/Lumimaid-Rocinate-v0.1,"base_model:
- NeverSleep/Lumimaid-v0.2-12B
- TheDrummer/Rocinante-12B-v1.1
library_name: transformers
tags:
- mergekit
- merge","# merge

This is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).

## Merge Details
### Merge Method

This model was merged using the SLERP merge method.

### Models Merged

The following models were included in the merge:
* [NeverSleep/Lumimaid-v0.2-12B](https://huggingface.co/NeverSleep/Lumimaid-v0.2-12B)
* [TheDrummer/Rocinante-12B-v1.1](https://huggingface.co/TheDrummer/Rocinante-12B-v1.1)

### Configuration

The following YAML configuration was used to produce this model:

```yaml
models:
  - model: TheDrummer/Rocinante-12B-v1.1
  - model: NeverSleep/Lumimaid-v0.2-12B
merge_method: slerp
base_model: TheDrummer/Rocinante-12B-v1.1
dtype: bfloat16
parameters:
  t: [0, 0.5, 1, 0.5, 0]

```
Best used with Alpaca Prompt Template",788.0,3.2894146798562743e-06
Barrybw/llama3_finetune_by_colab_unsloth_1,6629fa49c441d02cbe755d16,Barrybw,"{""language"": [""en""], ""license"": ""apache-2.0"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""llama"", ""trl"", ""sft""], ""base_model"": ""unsloth/llama-3-8b-bnb-4bit""}",[],2024-04-25 06:44:23,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|end_of_text|>"", ""pad_token"": ""<|end_of_text|>""}}",0,126,,"['transformers' 'pytorch' 'llama' 'text-generation'
 'text-generation-inference' 'unsloth' 'trl' 'sft' 'en'
 'base_model:unsloth/llama-3-8b-bnb-4bit'
 'base_model:finetune:unsloth/llama-3-8b-bnb-4bit' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'pytorch_model-00001-of-00004.bin'}
 {'rfilename': 'pytorch_model-00002-of-00004.bin'}
 {'rfilename': 'pytorch_model-00003-of-00004.bin'}
 {'rfilename': 'pytorch_model-00004-of-00004.bin'}
 {'rfilename': 'pytorch_model.bin.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2024-04-25 06:38:01,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,66214ee7f3502429f9b50922,unsloth/llama-3-8b-bnb-4bit,Barrybw/llama3_finetune_by_colab_unsloth_1,"language:
- en
license: apache-2.0
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
base_model: unsloth/llama-3-8b-bnb-4bit","# Uploaded  model

- **Developed by:** Barrybw
- **License:** apache-2.0
- **Finetuned from model :** unsloth/llama-3-8b-bnb-4bit

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)",416.0,3.2484500530344944e-06
davidschulte/ESM_CATIE-AQ__french_book_reviews_fr_prompt_sentiment_analysis_default,6751dd6515963b842b8640bb,davidschulte,"{""base_model"": ""bert-base-multilingual-uncased"", ""datasets"": [""CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis""], ""license"": ""apache-2.0"", ""tags"": [""embedding_space_map"", ""BaseLM:bert-base-multilingual-uncased""]}",[],2025-03-26 14:29:22,0,0.0,"{""additional_chat_templates"": {}}",0,126,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 590592, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 590592.0}","['safetensors' 'embedding_space_map'
 'BaseLM:bert-base-multilingual-uncased'
 'dataset:CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis'
 'base_model:google-bert/bert-base-multilingual-uncased'
 'base_model:finetune:google-bert/bert-base-multilingual-uncased'
 'license:apache-2.0' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}]",2024-12-05 17:05:41,,,,finetune,621ffdc036468d709f174337,google-bert/bert-base-multilingual-uncased,davidschulte/ESM_CATIE-AQ__french_book_reviews_fr_prompt_sentiment_analysis_default,"base_model: bert-base-multilingual-uncased
datasets:
- CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis
license: apache-2.0
tags:
- embedding_space_map
- BaseLM:bert-base-multilingual-uncased","# ESM CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis

<!-- Provide a quick summary of what the model is/does. -->



## Model Details

### Model Description

<!-- Provide a longer summary of what this model is. -->

ESM

- **Developed by:** David Schulte
- **Model type:** ESM
- **Base Model:** bert-base-multilingual-uncased
- **Intermediate Task:** CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis
- **ESM architecture:** linear
- **ESM embedding dimension:** 768
- **Language(s) (NLP):** [More Information Needed]
- **License:** Apache-2.0 license
- **ESM version:** 0.1.0

## Training Details

### Intermediate Task
- **Task ID:** CATIE-AQ/french_book_reviews_fr_prompt_sentiment_analysis
- **Subset [optional]:** default
- **Text Column:** inputs
- **Label Column:** targets
- **Dataset Split:**  train
- **Sample size [optional]:** 10000
- **Sample seed [optional]:** 42

### Training Procedure [optional]

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

#### Language Model Training Hyperparameters [optional]
- **Epochs:** 3
- **Batch size:** 32
- **Learning rate:** 2e-05
- **Weight Decay:** 0.01
- **Optimizer**: AdamW

### ESM Training Hyperparameters [optional]
- **Epochs:** 10
- **Batch size:** 32
- **Learning rate:** 0.001
- **Weight Decay:** 0.01
- **Optimizer**: AdamW


### Additional trainiung details [optional]


## Model evaluation

### Evaluation of fine-tuned language model [optional]


### Evaluation of ESM [optional]
MSE: 

### Additional evaluation details [optional]


## What are Embedding Space Maps used for?
Embedding Space Maps are a part of ESM-LogME, a efficient method for finding intermediate datasets for transfer learning. There are two reasons to use ESM-LogME:

### You don't have enough training data for your problem
If you don't have a enough training data for your problem, just use ESM-LogME to find more.
You can supplement model training by including publicly available datasets in the training process. 

1. Fine-tune a language model on suitable intermediate dataset.
2. Fine-tune the resulting model on your target dataset.

This workflow is called intermediate task transfer learning and it can significantly improve the target performance.

But what is a suitable dataset for your problem? ESM-LogME enable you to quickly rank thousands of datasets on the Hugging Face Hub by how well they are exptected to transfer to your target task.

### You want to find similar datasets to your target dataset
Using ESM-LogME can be used like search engine on the Hugging Face Hub. You can find similar tasks to your target task without having to rely on heuristics. ESM-LogME estimates how language models fine-tuned on each intermediate task would benefinit your target task. This quantitative approach combines the effects of domain similarity and task similarity. 

## How can I use ESM-LogME / ESMs?
[![PyPI version](https://img.shields.io/pypi/v/hf-dataset-selector.svg)](https://pypi.org/project/hf-dataset-selector)

We release **hf-dataset-selector**, a Python package for intermediate task selection using Embedding Space Maps.

**hf-dataset-selector** fetches ESMs for a given language model and uses it to find the best dataset for applying intermediate training to the target task. ESMs are found by their tags on the Huggingface Hub.

```python
from hfselect import Dataset, compute_task_ranking

# Load target dataset from the Hugging Face Hub
dataset = Dataset.from_hugging_face(
    name=""stanfordnlp/imdb"",
    split=""train"",
    text_col=""text"",
    label_col=""label"",
    is_regression=False,
    num_examples=1000,
    seed=42
)

# Fetch ESMs and rank tasks
task_ranking = compute_task_ranking(
    dataset=dataset,
    model_name=""bert-base-multilingual-uncased""
)

# Display top 5 recommendations
print(task_ranking[:5])
```
```python
1.   davanstrien/test_imdb_embedd2                     Score: -0.618529
2.   davanstrien/test_imdb_embedd                      Score: -0.618644
3.   davanstrien/test1                                 Score: -0.619334
4.   stanfordnlp/imdb                                  Score: -0.619454
5.   stanfordnlp/sst                                   Score: -0.62995
```

|   Rank | Task ID                       | Task Subset     | Text Column   | Label Column   | Task Split   |   Num Examples | ESM Architecture   |     Score |
|-------:|:------------------------------|:----------------|:--------------|:---------------|:-------------|---------------:|:-------------------|----------:|
|      1 | davanstrien/test_imdb_embedd2 | default         | text          | label          | train        |          10000 | linear             | -0.618529 |
|      2 | davanstrien/test_imdb_embedd  | default         | text          | label          | train        |          10000 | linear             | -0.618644 |
|      3 | davanstrien/test1             | default         | text          | label          | train        |          10000 | linear             | -0.619334 |
|      4 | stanfordnlp/imdb              | plain_text      | text          | label          | train        |          10000 | linear             | -0.619454 |
|      5 | stanfordnlp/sst               | dictionary      | phrase        | label          | dictionary   |          10000 | linear             | -0.62995  |
|      6 | stanfordnlp/sst               | default         | sentence      | label          | train        |           8544 | linear             | -0.63312  |
|      7 | kuroneko5943/snap21           | CDs_and_Vinyl_5 | sentence      | label          | train        |           6974 | linear             | -0.634365 |
|      8 | kuroneko5943/snap21           | Video_Games_5   | sentence      | label          | train        |           6997 | linear             | -0.638787 |
|      9 | kuroneko5943/snap21           | Movies_and_TV_5 | sentence      | label          | train        |           6989 | linear             | -0.639068 |
|     10 | fancyzhx/amazon_polarity      | amazon_polarity | content       | label          | train        |          10000 | linear             | -0.639718 |

For more information on how to use ESMs please have a look at the [official Github repository](https://github.com/davidschulte/hf-dataset-selector). We provide documentation further documentation and tutorials for finding intermediate datasets and training your own ESMs.


## How do Embedding Space Maps work?

<!-- This section describes the evaluation protocols and provides the results. -->
Embedding Space Maps (ESMs) are neural networks that approximate the effect of fine-tuning a language model on a task. They can be used to quickly transform embeddings from a base model to approximate how a fine-tuned model would embed the the input text.
ESMs can be used for intermediate task selection with the ESM-LogME workflow.

## How can I use Embedding Space Maps for Intermediate Task Selection?

## Citation


<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->
If you are using this Embedding Space Maps, please cite our [paper](https://aclanthology.org/2024.emnlp-main.529/).

**BibTeX:**


```
@inproceedings{schulte-etal-2024-less,
    title = ""Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning"",
    author = ""Schulte, David  and
      Hamborg, Felix  and
      Akbik, Alan"",
    editor = ""Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung"",
    booktitle = ""Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2024"",
    address = ""Miami, Florida, USA"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2024.emnlp-main.529/"",
    doi = ""10.18653/v1/2024.emnlp-main.529"",
    pages = ""9431--9442"",
    abstract = ""Intermediate task transfer learning can greatly improve model performance. If, for example, one has little training data for emotion detection, first fine-tuning a language model on a sentiment classification dataset may improve performance strongly. But which task to choose for transfer learning? Prior methods producing useful task rankings are infeasible for large source pools, as they require forward passes through all source language models. We overcome this by introducing Embedding Space Maps (ESMs), light-weight neural networks that approximate the effect of fine-tuning a language model. We conduct the largest study on NLP task transferability and task selection with 12k source-target pairs. We find that applying ESMs on a prior method reduces execution time and disk space usage by factors of 10 and 278, respectively, while retaining high selection performance (avg. regret@5 score of 2.95).""
}
```


**APA:**

```
Schulte, D., Hamborg, F., & Akbik, A. (2024, November). Less is More: Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (pp. 9431-9442).
```

## Additional Information",9191.0,3.2484500530344944e-06
cgifbribcgfbi/Llama-3.3-70B-chem-o4-mini-div-v2,6855ca6ed9d19ddc87ebb76f,cgifbribcgfbi,"{""library_name"": ""peft"", ""license"": ""llama3.3"", ""base_model"": ""huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""datasets"": [""o4-mini-diverse-v2_5000.jsonl""], ""model-index"": [{""name"": ""Llama-3.3-70B-chem-o4-mini-div-v2"", ""results"": []}]}",[],2025-06-20 23:06:12,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""quantization_config"": {""load_in_4bit"": true, ""load_in_8bit"": false, ""quant_method"": ""bitsandbytes""}, ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|eot_id|>"", ""pad_token"": ""<|finetune_right_pad_id|>""}, ""chat_template_jinja"": ""{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \""26 Jul 2024\"" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \""\"" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \""<|start_header_id|>system<|end_header_id|>\\n\\n\"" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \""Environment: ipython\\n\"" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \""Tools: \"" + builtin_tools | reject('equalto', 'code_interpreter') | join(\"", \"") + \""\\n\\n\""}}\n{%- endif %}\n{{- \""Cutting Knowledge Date: December 2023\\n\"" }}\n{{- \""Today Date: \"" + date_string + \""\\n\\n\"" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \""You have access to the following functions. To call a function, please respond with JSON for a function call.\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \""<|eot_id|>\"" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\""Cannot put tools in the first user message when there's no first user message!\"") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \""Given the following functions, please respond with a JSON for a function call \"" }}\n    {{- \""with its proper arguments that best answers the given prompt.\\n\\n\"" }}\n    {{- 'Respond in the format {\""name\"": function name, \""parameters\"": dictionary of argument name and its value}.' }}\n    {{- \""Do not use variables.\\n\\n\"" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \""\\n\\n\"" }}\n    {%- endfor %}\n    {{- first_user_message + \""<|eot_id|>\""}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\""This model only supports single tool-calls at once!\"") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \""<|python_tag|>\"" + tool_call.name + \"".call(\"" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\""' + arg_val + '\""' }}\n                {%- if not loop.last %}\n                    {{- \"", \"" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \"")\"" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\""name\"": \""' + tool_call.name + '\"", ' }}\n            {{- '\""parameters\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \""}\"" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \""<|eom_id|>\"" }}\n        {%- else %}\n            {{- \""<|eot_id|>\"" }}\n        {%- endif %}\n    {%- elif message.role == \""tool\"" or message.role == \""ipython\"" %}\n        {{- \""<|start_header_id|>ipython<|end_header_id|>\\n\\n\"" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \""<|eot_id|>\"" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n"", ""peft"": {""base_model_name_or_path"": ""huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned"", ""task_type"": ""CAUSAL_LM""}}",1,125,,"['peft' 'safetensors' 'llama' 'axolotl' 'generated_from_trainer'
 'dataset:o4-mini-diverse-v2_5000.jsonl'
 'base_model:huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned'
 'base_model:adapter:huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned'
 'license:llama3.3' '4-bit' 'bitsandbytes' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'}
 {'rfilename': 'adapter_model.safetensors'}
 {'rfilename': 'chat_template.jinja'} {'rfilename': 'config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2025-06-20 20:54:06,peft,,,adapter,6763e1c5220f276cee2d7b3b,huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned,cgifbribcgfbi/Llama-3.3-70B-chem-o4-mini-div-v2,"library_name: peft
license: llama3.3
base_model: huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned
tags:
- axolotl
- generated_from_trainer
datasets:
- o4-mini-diverse-v2_5000.jsonl
model-index:
- name: Llama-3.3-70B-chem-o4-mini-div-v2
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.10.0`
```yaml
base_model: huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned
load_in_8bit: false
load_in_4bit: true
adapter: qlora
wandb_name: Llama-3.3-70B-chem-o4-mini-div-v2
output_dir: ./outputs/out/Llama-3.3-70B-chem-o4-mini-div-v2
hub_model_id: cgifbribcgfbi/Llama-3.3-70B-chem-o4-mini-div-v2

tokenizer_type: AutoTokenizer
push_dataset_to_hub:
strict: false

datasets:
  - path: o4-mini-diverse-v2_5000.jsonl
    type: chat_template
    field_messages: messages

dataset_prepared_path: last_run_prepared
# val_set_size: 0.05
# eval_sample_packing: False
save_safetensors: true

sequence_len: 3746
sample_packing: true
pad_to_sequence_len: true

lora_r: 64
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
lora_target_linear: false
lora_modules_to_save:

wandb_mode:
wandb_project: finetune-sweep
wandb_entity: gpoisjgqetpadsfke
wandb_watch:
wandb_run_id:
wandb_log_model:

gradient_accumulation_steps: 1
micro_batch_size: 2  # This will be automatically adjusted based on available GPU memory
num_epochs: 4
optimizer: adamw_torch_fused
lr_scheduler: cosine
learning_rate: 0.00002

train_on_inputs: false
group_by_length: true
bf16: true
tf32: true

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: true
logging_steps: 1
flash_attention: true

warmup_steps: 10
evals_per_epoch: 3
saves_per_epoch: 1
weight_decay: 0.01
fsdp:
  - full_shard
  - auto_wrap
fsdp_config:
  fsdp_limit_all_gathers: true
  fsdp_sync_module_states: true
  fsdp_offload_params: false
  fsdp_use_orig_params: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer
  fsdp_state_dict_type: FULL_STATE_DICT
  fsdp_sharding_strategy: FULL_SHARD
special_tokens:
  pad_token: <|finetune_right_pad_id|>

```

</details><br>

# Llama-3.3-70B-chem-o4-mini-div-v2

This model is a fine-tuned version of [huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned](https://huggingface.co/huihui-ai/Llama-3.3-70B-Instruct-abliterated-finetuned) on the o4-mini-diverse-v2_5000.jsonl dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- distributed_type: multi-GPU
- num_devices: 8
- total_train_batch_size: 16
- total_eval_batch_size: 16
- optimizer: Use OptimizerNames.ADAMW_TORCH_FUSED with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- training_steps: 832

### Training results



### Framework versions

- PEFT 0.15.2
- Transformers 4.52.3
- Pytorch 2.6.0+cu124
- Datasets 3.6.0
- Tokenizers 0.21.1",3450.0,3.2431489405194734e-06
gghfez/sao10k_32B-Qwen2.5-Kunou-v1-exl2-4.5bpw,6772406cd1512e00e4638324,gghfez,"{""library_name"": ""transformers"", ""license"": ""other"", ""license_name"": ""qwen"", ""license_link"": ""https://huggingface.co/Qwen/Qwen2.5-32B-Instruct/blob/main/LICENSE"", ""base_model"": [""Sao10K/32B-Qwen2.5-Kunou-v1""], ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""32B-Qwen2.5-Kunou-v1"", ""results"": []}]}",[],2024-12-30 07:16:50,0,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""quantization_config"": {""quant_method"": ""exl2""}, ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) or (message.role == \""assistant\"" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}}",0,116,,"['transformers' 'safetensors' 'qwen2' 'text-generation'
 'generated_from_trainer' 'conversational'
 'base_model:Sao10K/32B-Qwen2.5-Kunou-v1'
 'base_model:quantized:Sao10K/32B-Qwen2.5-Kunou-v1' 'license:other'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'exl2' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'measurement.json'}
 {'rfilename': 'merges.txt'} {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'output-00001-of-00003.safetensors'}
 {'rfilename': 'output-00002-of-00003.safetensors'}
 {'rfilename': 'output-00003-of-00003.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2024-12-30 06:40:44,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,quantized,676f9b8e4d148b90435bda9a,Sao10K/32B-Qwen2.5-Kunou-v1,gghfez/sao10k_32B-Qwen2.5-Kunou-v1-exl2-4.5bpw,"library_name: transformers
license: other
license_name: qwen
license_link: https://huggingface.co/Qwen/Qwen2.5-32B-Instruct/blob/main/LICENSE
base_model:
- Sao10K/32B-Qwen2.5-Kunou-v1
tags:
- generated_from_trainer
model-index:
- name: 32B-Qwen2.5-Kunou-v1
  results: []","EXL2 Quant of [32B-Kunou-v1](https://huggingface.co/Sao10K/32B-Qwen2.5-Kunou-v1)

measurement.json included

![Kunou](https://huggingface.co/Sao10K/72B-Qwen2.5-Kunou-v1/resolve/main/knn.png)

**Sister Versions for Lightweight Use!**

[72B-Kunou-v1](https://huggingface.co/Sao10K/72B-Qwen2.5-Kunou-v1)

[14B-Kunou-v1](https://huggingface.co/Sao10K/14B-Qwen2.5-Kunou-v1)

# 32B-Qwen2.5-Kunou-v1

*training delays and all...*

I do not really have anything planned for this model other than it being a generalist, and Roleplay Model? It was just something made and planned in minutes. 
<br>Same with the 14B and 72B version.
<br>Kunou's the name of an OC I worked on for a couple of years, for a... fanfic. mmm... 

A kind-of successor to L3-70B-Euryale-v2.2 in all but name? I'm keeping Stheno/Euryale lineage to Llama series for now.
<br>I had a version made on top of Nemotron, a supposed Euryale 2.4 but that flopped hard, it was not my cup of tea.
<br>This version is basically a better, more cleaned up Dataset used on Euryale and Stheno.

Recommended Model Settings | *Look, I just use these, they work fine enough. I don't even know how DRY or other meme samplers work. Your system prompt matters more anyway.*
```
Prompt Format: ChatML
Temperature: 1.1
min_p: 0.1
```


Future-ish plans:
~~<br>\- Complete this model series.~~
<br>\- Further refine the Datasets used for quality, more secondary chats, more creative-related domains. (Inspired by Drummer)
<br>\- Work on my other incomplete projects. About half a dozen on the backburner for a while now.

Special thanks to my wallet for funding this, my juniors who share a single braincell between them, and my current national service. 
<br>Stay safe. There have been more emergency calls, more incidents this holiday season.

Also sorry for the inactivity. Life was in the way. It still is, just less so, for now. Burnout is a thing, huh?

https://sao10k.carrd.co/ for contact.

---

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.5.2`
```yaml
base_model: Qwen/Qwen2.5-32B-Instruct
model_type: AutoModelForCausalLM
tokenizer_type: AutoTokenizer

load_in_8bit: false
load_in_4bit: true
strict: false
sequence_len: 16384
bf16: auto
fp16:
tf32: false
flash_attention: true

adapter: qlora
lora_model_dir:
lora_r: 32
lora_alpha: 64
lora_dropout: 0.1
lora_target_linear: true
lora_fan_in_fan_out:
peft_use_rslora: true

# Data
dataset_prepared_path: last_run_prepared
datasets:
  - path: datasets/amoral-full-sys-prompt.json # Unalignment Data - Cleaned Up from Original, Split to its own file
    type: customchatml
  - path: datasets/mimi-superfix-RP-filtered-fixed.json # RP / Creative-Instruct Data
    type: customchatml
  - path: datasets/hespera-smartshuffle.json # Hesperus-v2-Instruct Data
    type: customchatml
warmup_steps: 15

plugins:
  - axolotl.integrations.liger.LigerPlugin
liger_rope: true
liger_rms_norm: true
liger_layer_norm: true
liger_glu_activation: true
liger_fused_linear_cross_entropy: true

# Iterations
num_epochs: 1

# Batching
gradient_accumulation_steps: 4
micro_batch_size: 1
gradient_checkpointing: ""unsloth""

# Optimizer
optimizer: paged_ademamix_8bit
lr_scheduler: cosine
learning_rate: 0.000004
weight_decay: 0.1
max_grad_norm: 25.0

# Iterations
num_epochs: 1

# Misc
deepspeed: ./deepspeed_configs/zero3_bf16.json
```

</details><br>",3549.0,3.1934530799392767e-06
JayHyeon/Qwen_0.5-MDPO_0.5_7e-7-3ep_0alp_0lam,677e39b42b763404dd9c7d7a,JayHyeon,"{""base_model"": ""JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep"", ""datasets"": ""trl-lib/ultrafeedback_binarized"", ""library_name"": ""transformers"", ""model_name"": ""Qwen_0.5-MDPO_0.5_7e-7-3ep_0alp_0lam"", ""tags"": [""generated_from_trainer"", ""trl"", ""dpo""], ""licence"": ""license""}",[],2025-01-08 14:35:00,0,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) or (message.role == \""assistant\"" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|endoftext|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}}",1,114,"{'parameters': {'BF16': 630167424.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 630167424.0}","['transformers' 'safetensors' 'qwen2' 'text-generation'
 'generated_from_trainer' 'trl' 'dpo' 'conversational'
 'dataset:trl-lib/ultrafeedback_binarized' 'arxiv:2305.18290'
 'base_model:JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep'
 'base_model:finetune:JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'all_results.json'}
 {'rfilename': 'config.json'} {'rfilename': 'eval_results.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.json'}]",2025-01-08 08:39:16,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,6770d93ba50f25fd7a2e8f3b,JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep,JayHyeon/Qwen_0.5-MDPO_0.5_7e-7-3ep_0alp_0lam,"base_model: JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep
datasets: trl-lib/ultrafeedback_binarized
library_name: transformers
model_name: Qwen_0.5-MDPO_0.5_7e-7-3ep_0alp_0lam
tags:
- generated_from_trainer
- trl
- dpo
licence: license","# Model Card for Qwen_0.5-MDPO_0.5_7e-7-3ep_0alp_0lam

This model is a fine-tuned version of [JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep](https://huggingface.co/JayHyeon/Qwen2.5-0.5B-SFT-2e-5-2ep) on the [trl-lib/ultrafeedback_binarized](https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized) dataset.
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""JayHyeon/Qwen_0.5-MDPO_0.5_7e-7-3ep_0alp_0lam"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/bonin147/huggingface/runs/mo2od0s0) 


This model was trained with DPO, a method introduced in [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://huggingface.co/papers/2305.18290).

### Framework versions

- TRL: 0.14.0.dev0
- Transformers: 4.47.1
- Pytorch: 2.5.1
- Datasets: 3.2.0
- Tokenizers: 0.21.0

## Citations

Cite DPO as:

```bibtex
@inproceedings{rafailov2023direct,
    title        = {{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}},
    author       = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D. Manning and Stefano Ermon and Chelsea Finn},
    year         = 2023,
    booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023},
    url          = {http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html},
    editor       = {Alice Oh and Tristan Naumann and Amir Globerson and Kate Saenko and Moritz Hardt and Sergey Levine},
}
```

Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```",2611.0,3.1818909445330803e-06
rsskumar/peft-text-to-sql,65f89847d760cfdf5e89cab7,rsskumar,"{""license"": ""apache-2.0"", ""library_name"": ""peft"", ""tags"": [""trl"", ""sft"", ""generated_from_trainer""], ""datasets"": [""generator""], ""base_model"": ""TinyLlama/TinyLlama-1.1B-Chat-v1.0"", ""model-index"": [{""name"": ""peft-text-to-sql"", ""results"": []}]}",[],2024-03-18 21:22:34,0,0.0,"{""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""peft"": {""base_model_name_or_path"": ""TinyLlama/TinyLlama-1.1B-Chat-v1.0"", ""task_type"": ""CAUSAL_LM""}}",8,110,,"['peft' 'tensorboard' 'safetensors' 'trl' 'sft' 'generated_from_trainer'
 'dataset:generator' 'base_model:TinyLlama/TinyLlama-1.1B-Chat-v1.0'
 'base_model:adapter:TinyLlama/TinyLlama-1.1B-Chat-v1.0'
 'license:apache-2.0' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'}
 {'rfilename': 'adapter_model.safetensors'}
 {'rfilename': 'runs/Mar18_19-38-47_58e209c14544/events.out.tfevents.1710790730.58e209c14544.436.0'}
 {'rfilename': 'runs/Mar18_19-39-14_58e209c14544/events.out.tfevents.1710790756.58e209c14544.436.1'}
 {'rfilename': 'runs/Mar18_20-10-11_58e209c14544/events.out.tfevents.1710792612.58e209c14544.436.2'}
 {'rfilename': 'runs/Mar18_20-11-26_58e209c14544/events.out.tfevents.1710792831.58e209c14544.436.3'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.model'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2024-03-18 19:38:47,peft,,,adapter,658fb85235c41262d661dc48,TinyLlama/TinyLlama-1.1B-Chat-v1.0,rsskumar/peft-text-to-sql,"license: apache-2.0
library_name: peft
tags:
- trl
- sft
- generated_from_trainer
datasets:
- generator
base_model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
model-index:
- name: peft-text-to-sql
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# peft-text-to-sql

This model is a fine-tuned version of [TinyLlama/TinyLlama-1.1B-Chat-v1.0](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) on the generator dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 1
- eval_batch_size: 8
- seed: 42
- gradient_accumulation_steps: 2
- total_train_batch_size: 2
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: constant
- lr_scheduler_warmup_ratio: 0.03
- num_epochs: 3
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- PEFT 0.9.0
- Transformers 4.38.2
- Pytorch 2.2.1+cu121
- Datasets 2.18.0
- Tokenizers 0.15.2",1101.0,3.1581508639471046e-06
Soulex95/speecht5_finetuned_wolof_text_speech,673f1b416aa40c3ae5e19cfc,Soulex95,"{""library_name"": ""transformers"", ""license"": ""mit"", ""base_model"": ""microsoft/speecht5_tts"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""speecht5_finetuned_wolof_text_speech"", ""results"": []}]}",[],2024-11-27 10:15:55,0,0.0,"{""architectures"": [""SpeechT5ForTextToSpeech""], ""model_type"": ""speecht5"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""mask_token"": ""<mask>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>""}}",3,105,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 144433890, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 144433890.0}","['transformers' 'tensorboard' 'safetensors' 'speecht5' 'text-to-audio'
 'generated_from_trainer' 'base_model:microsoft/speecht5_tts'
 'base_model:finetune:microsoft/speecht5_tts' 'license:mit'
 'endpoints_compatible' 'region:us']",text-to-audio,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'runs/Nov21_11-29-51_6336f71db834/events.out.tfevents.1732189003.6336f71db834.2300.0'}
 {'rfilename': 'runs/Nov24_00-45-29_c3acbae7c97b/events.out.tfevents.1732409259.c3acbae7c97b.415.0'}
 {'rfilename': 'runs/Nov24_12-55-21_507fa3be9e33/events.out.tfevents.1732452929.507fa3be9e33.578.0'}
 {'rfilename': 'runs/Nov26_10-18-23_345f9524d29c/events.out.tfevents.1732616316.345f9524d29c.179.0'}
 {'rfilename': 'runs/Nov26_10-57-01_345f9524d29c/events.out.tfevents.1732618631.345f9524d29c.179.1'}
 {'rfilename': 'runs/Nov27_09-50-48_a141b5c53eb3/events.out.tfevents.1732701063.a141b5c53eb3.30.0'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'spm_char.model'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2024-11-21 11:36:33,transformers,"{'auto_model': 'AutoModelForTextToSpectrogram', 'custom_class': None, 'pipeline_tag': 'text-to-audio', 'processor': 'AutoProcessor'}",,finetune,63dbb316057a688a88b910e5,microsoft/speecht5_tts,Soulex95/speecht5_finetuned_wolof_text_speech,"library_name: transformers
license: mit
base_model: microsoft/speecht5_tts
tags:
- generated_from_trainer
model-index:
- name: speecht5_finetuned_wolof_text_speech
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# speecht5_finetuned_wolof_text_speech

This model is a fine-tuned version of [microsoft/speecht5_tts](https://huggingface.co/microsoft/speecht5_tts) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.3296

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 4
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 32
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 100
- training_steps: 500
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.4792        | 1.0681 | 100  | 0.4004          |
| 0.3975        | 2.1362 | 200  | 0.3634          |
| 0.3755        | 3.2043 | 300  | 0.3431          |
| 0.3599        | 4.2724 | 400  | 0.3335          |
| 0.3597        | 5.3405 | 500  | 0.3296          |


### Framework versions

- Transformers 4.45.1
- Pytorch 2.4.0
- Datasets 3.0.1
- Tokenizers 0.20.0",1514.0,3.1272427555365798e-06
zelk12/MT-Merge1-BB-gemma-2-MT1g1MT5g1-9B,672cebafbad60f496bb1b656,zelk12,"{""base_model"": [""zelk12/MT1-Gen1-gemma-2-9B"", ""zelk12/MT5-Gen1-gemma-2-9B""], ""library_name"": ""transformers"", ""tags"": [""mergekit"", ""merge""]}",[],2024-11-07 16:39:12,0,0.0,"{""architectures"": [""Gemma2ForCausalLM""], ""model_type"": ""gemma2"", ""tokenizer_config"": {""bos_token"": ""<bos>"", ""chat_template"": ""{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"", ""eos_token"": ""<eos>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""additional_chat_templates"": {}}",0,89,"{'parameters': {'BF16': 10159209984.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 10159209984.0}","['transformers' 'safetensors' 'gemma2' 'text-generation' 'mergekit'
 'merge' 'conversational' 'base_model:zelk12/MT1-Gen1-gemma-2-9B'
 'base_model:merge:zelk12/MT1-Gen1-gemma-2-9B'
 'base_model:zelk12/MT5-Gen1-gemma-2-9B'
 'base_model:merge:zelk12/MT5-Gen1-gemma-2-9B' 'autotrain_compatible'
 'text-generation-inference' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'mergekit_config.yml'}
 {'rfilename': 'model-00001-of-00005.safetensors'}
 {'rfilename': 'model-00002-of-00005.safetensors'}
 {'rfilename': 'model-00003-of-00005.safetensors'}
 {'rfilename': 'model-00004-of-00005.safetensors'}
 {'rfilename': 'model-00005-of-00005.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2024-11-07 16:32:47,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,merge,671c10f94764e417b228396c,zelk12/MT5-Gen1-gemma-2-9B,zelk12/MT-Merge1-BB-gemma-2-MT1g1MT5g1-9B,"base_model:
- zelk12/MT1-Gen1-gemma-2-9B
- zelk12/MT5-Gen1-gemma-2-9B
library_name: transformers
tags:
- mergekit
- merge","# merge

This is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).

## Merge Details
### Merge Method

This model was merged using the SLERP merge method.

### Models Merged

The following models were included in the merge:
* [zelk12/MT1-Gen1-gemma-2-9B](https://huggingface.co/zelk12/MT1-Gen1-gemma-2-9B)
* [zelk12/MT5-Gen1-gemma-2-9B](https://huggingface.co/zelk12/MT5-Gen1-gemma-2-9B)

### Configuration

The following YAML configuration was used to produce this model:

```yaml
models:
  - model: zelk12/MT1-Gen1-gemma-2-9B
  - model: zelk12/MT5-Gen1-gemma-2-9B
merge_method: slerp
base_model: zelk12/MT1-Gen1-gemma-2-9B
dtype: bfloat16
parameters:
  t: 0.5
```",715.0,3.01751495170178e-06
benitch/fucktard_flux,678a53e8029cff435f766fa0,benitch,"{""license"": ""other"", ""license_name"": ""flux-1-dev-non-commercial-license"", ""license_link"": ""https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md"", ""language"": [""en""], ""tags"": [""flux"", ""diffusers"", ""lora"", ""replicate""], ""base_model"": ""black-forest-labs/FLUX.1-dev"", ""pipeline_tag"": ""text-to-image"", ""instance_prompt"": ""fck""}","[{'_id': '680b71dfa07785a516f270ef', 'adapter': 'lora', 'adapterWeightsPath': 'lora.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/flux-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}
 {'_id': '6818c94d5111343b72f18ab4', 'adapter': 'lora', 'adapterWeightsPath': 'lora.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'replicate', 'providerDetails': None, 'providerId': 'black-forest-labs/flux-dev-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2025-01-17 14:17:19,0,0.0,,1,88,,"['diffusers' 'flux' 'lora' 'replicate' 'text-to-image' 'en'
 'base_model:black-forest-labs/FLUX.1-dev'
 'base_model:adapter:black-forest-labs/FLUX.1-dev' 'license:other'
 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.yaml'} {'rfilename': 'lora.safetensors'}]",2025-01-17 12:58:16,diffusers,,,adapter,66aaa908fc35e079a941470d,black-forest-labs/FLUX.1-dev,benitch/fucktard_flux,"license: other
license_name: flux-1-dev-non-commercial-license
license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md
language:
- en
tags:
- flux
- diffusers
- lora
- replicate
base_model: ""black-forest-labs/FLUX.1-dev""
pipeline_tag: text-to-image
# widget:
#   - text: >-
#       prompt
#     output:
#       url: https://...
instance_prompt: fck","# Fucktard_Flux

<Gallery />

Trained on Replicate using:

https://replicate.com/ostris/flux-dev-lora-trainer/train


## Trigger words
You should use `fck` to trigger the image generation.


## Use it with the [🧨 diffusers library](https://github.com/huggingface/diffusers)

```py
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained('black-forest-labs/FLUX.1-dev', torch_dtype=torch.float16).to('cuda')
pipeline.load_lora_weights('benitch/fucktard_flux', weight_name='lora.safetensors')
image = pipeline('your prompt').images[0]
```

For more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)",797.0,3.010022278881192e-06
itlwas/TinyLLama-v0-Q4_K_M-GGUF,67714491076645f7fec869fa,itlwas,"{""license"": ""apache-2.0"", ""base_model"": ""Maykeye/TinyLLama-v0"", ""tags"": [""llama-cpp"", ""gguf-my-repo""]}",[],2024-12-29 12:46:11,0,0.0,,20,86,,"['gguf' 'llama-cpp' 'gguf-my-repo' 'base_model:Maykeye/TinyLLama-v0'
 'base_model:quantized:Maykeye/TinyLLama-v0' 'license:apache-2.0'
 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'tinyllama-v0-q4_k_m.gguf'}]",2024-12-29 12:46:09,,,"{""total"": 4621376, ""architecture"": ""llama"", ""context_length"": 2048, ""bos_token"": ""<s>"", ""eos_token"": ""</s>""}",quantized,64a8eb078a1a9187c2dddef1,Maykeye/TinyLLama-v0,itlwas/TinyLLama-v0-Q4_K_M-GGUF,"license: apache-2.0
base_model: Maykeye/TinyLLama-v0
tags:
- llama-cpp
- gguf-my-repo","# AIronMind/TinyLLama-v0-Q4_K_M-GGUF
This model was converted to GGUF format from [`Maykeye/TinyLLama-v0`](https://huggingface.co/Maykeye/TinyLLama-v0) using llama.cpp via the ggml.ai's [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space.
Refer to the [original model card](https://huggingface.co/Maykeye/TinyLLama-v0) for more details on the model.

## Use with llama.cpp
Install llama.cpp through brew (works on Mac and Linux)

```bash
brew install llama.cpp

```
Invoke the llama.cpp server or the CLI.

### CLI:
```bash
llama-cli --hf-repo AIronMind/TinyLLama-v0-Q4_K_M-GGUF --hf-file tinyllama-v0-q4_k_m.gguf -p ""The meaning to life and the universe is""
```

### Server:
```bash
llama-server --hf-repo AIronMind/TinyLLama-v0-Q4_K_M-GGUF --hf-file tinyllama-v0-q4_k_m.gguf -c 2048
```

Note: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.

Step 1: Clone llama.cpp from GitHub.
```
git clone https://github.com/ggerganov/llama.cpp
```

Step 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).
```
cd llama.cpp && LLAMA_CURL=1 make
```

Step 3: Run inference through the main binary.
```
./llama-cli --hf-repo AIronMind/TinyLLama-v0-Q4_K_M-GGUF --hf-file tinyllama-v0-q4_k_m.gguf -p ""The meaning to life and the universe is""
```
or 
```
./llama-server --hf-repo AIronMind/TinyLLama-v0-Q4_K_M-GGUF --hf-file tinyllama-v0-q4_k_m.gguf -c 2048
```",1601.0,2.9947810036990964e-06
cportoca/MMS_Quechua_finetuned_qve,678adff9da686d59649bc570,cportoca,"{""library_name"": ""transformers"", ""license"": ""cc-by-nc-4.0"", ""base_model"": ""facebook/mms-1b-all"", ""tags"": [""generated_from_trainer""], ""metrics"": [""wer""], ""model-index"": [{""name"": ""MMS_Quechua_finetuned_qve"", ""results"": []}]}",[],2025-01-18 07:42:20,0,0.0,"{""architectures"": [""Wav2Vec2ForCTC""], ""model_type"": ""wav2vec2""}",0,75,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 964703659, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 964703659.0}","['transformers' 'tensorboard' 'safetensors' 'wav2vec2'
 'automatic-speech-recognition' 'generated_from_trainer'
 'base_model:facebook/mms-1b-all'
 'base_model:finetune:facebook/mms-1b-all' 'license:cc-by-nc-4.0'
 'endpoints_compatible' 'region:us']",automatic-speech-recognition,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'preprocessor_config.json'}
 {'rfilename': 'runs/Jan17_14-53-35_jagupard36.stanford.edu/events.out.tfevents.1737154767.jagupard36.stanford.edu.981343.0'}
 {'rfilename': 'training_args.bin'}]",2025-01-17 22:55:53,transformers,"{'auto_model': 'AutoModelForCTC', 'custom_class': None, 'pipeline_tag': 'automatic-speech-recognition', 'processor': 'AutoProcessor'}",,finetune,6471ecd95afd6a69657d8815,facebook/mms-1b-all,cportoca/MMS_Quechua_finetuned_qve,"library_name: transformers
license: cc-by-nc-4.0
base_model: facebook/mms-1b-all
tags:
- generated_from_trainer
metrics:
- wer
model-index:
- name: MMS_Quechua_finetuned_qve
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# MMS_Quechua_finetuned_qve

This model is a fine-tuned version of [facebook/mms-1b-all](https://huggingface.co/facebook/mms-1b-all) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.2527
- Wer: 0.3191

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.001
- train_batch_size: 16
- eval_batch_size: 8
- seed: 42
- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 100
- num_epochs: 20
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch   | Step  | Validation Loss | Wer    |
|:-------------:|:-------:|:-----:|:---------------:|:------:|
| 1.5146        | 0.1355  | 100   | 0.6473          | 0.5519 |
| 0.74          | 0.2710  | 200   | 0.5775          | 0.4716 |
| 0.6064        | 0.4065  | 300   | 0.5398          | 0.4490 |
| 0.59          | 0.5420  | 400   | 0.5168          | 0.4365 |
| 0.5733        | 0.6775  | 500   | 0.5090          | 0.4707 |
| 1.0555        | 0.8130  | 600   | 0.5121          | 0.4321 |
| 0.5509        | 0.9485  | 700   | 0.4951          | 0.4506 |
| 0.6772        | 1.0840  | 800   | 0.4541          | 0.4161 |
| 0.5271        | 1.2195  | 900   | 0.4531          | 0.4437 |
| 0.6283        | 1.3550  | 1000  | 0.4684          | 0.4198 |
| 0.6823        | 1.4905  | 1100  | 0.4647          | 0.4518 |
| 0.7715        | 1.6260  | 1200  | 0.4325          | 0.4057 |
| 0.4755        | 1.7615  | 1300  | 0.4222          | 0.4048 |
| 0.4751        | 1.8970  | 1400  | 0.4169          | 0.4321 |
| 0.6891        | 2.0325  | 1500  | 0.4054          | 0.4063 |
| 0.6245        | 2.1680  | 1600  | 0.3862          | 0.4060 |
| 0.4548        | 2.3035  | 1700  | 0.3839          | 0.4302 |
| 0.4506        | 2.4390  | 1800  | 0.3661          | 0.4079 |
| 0.7592        | 2.5745  | 1900  | 0.3587          | 0.3916 |
| 0.4698        | 2.7100  | 2000  | 0.3737          | 0.4114 |
| 0.6196        | 2.8455  | 2100  | 0.3558          | 0.3888 |
| 0.6997        | 2.9810  | 2200  | 0.3518          | 0.3822 |
| 0.5634        | 3.1165  | 2300  | 0.3674          | 0.3897 |
| 0.4163        | 3.2520  | 2400  | 0.3636          | 0.3894 |
| 0.6243        | 3.3875  | 2500  | 0.3652          | 0.3721 |
| 0.623         | 3.5230  | 2600  | 0.3393          | 0.3778 |
| 0.557         | 3.6585  | 2700  | 0.3453          | 0.3900 |
| 0.573         | 3.7940  | 2800  | 0.3574          | 0.3885 |
| 0.4114        | 3.9295  | 2900  | 0.3537          | 0.3812 |
| 0.6551        | 4.0650  | 3000  | 0.3646          | 0.3897 |
| 0.5701        | 4.2005  | 3100  | 0.3513          | 0.3954 |
| 0.5688        | 4.3360  | 3200  | 0.3395          | 0.3772 |
| 0.5151        | 4.4715  | 3300  | 0.3633          | 0.3919 |
| 0.4063        | 4.6070  | 3400  | 0.3365          | 0.3781 |
| 0.671         | 4.7425  | 3500  | 0.3765          | 0.4016 |
| 0.3761        | 4.8780  | 3600  | 0.3294          | 0.3734 |
| 0.4151        | 5.0136  | 3700  | 0.3322          | 0.3634 |
| 0.3634        | 5.1491  | 3800  | 0.3291          | 0.3740 |
| 0.5112        | 5.2846  | 3900  | 0.3723          | 0.3762 |
| 0.5892        | 5.4201  | 4000  | 0.3300          | 0.3863 |
| 0.723         | 5.5556  | 4100  | 0.3261          | 0.3659 |
| 0.4026        | 5.6911  | 4200  | 0.3332          | 0.3888 |
| 0.3836        | 5.8266  | 4300  | 0.3407          | 0.3574 |
| 0.5441        | 5.9621  | 4400  | 0.3370          | 0.3803 |
| 0.5204        | 6.0976  | 4500  | 0.3405          | 0.3778 |
| 0.3738        | 6.2331  | 4600  | 0.3310          | 0.3784 |
| 0.4918        | 6.3686  | 4700  | 0.3300          | 0.3768 |
| 0.5163        | 6.5041  | 4800  | 0.3314          | 0.3652 |
| 0.3839        | 6.6396  | 4900  | 0.3071          | 0.3574 |
| 0.5652        | 6.7751  | 5000  | 0.3008          | 0.3624 |
| 0.4961        | 6.9106  | 5100  | 0.3086          | 0.3894 |
| 0.58          | 7.0461  | 5200  | 0.3137          | 0.4010 |
| 0.3617        | 7.1816  | 5300  | 0.3328          | 0.3787 |
| 0.4433        | 7.3171  | 5400  | 0.3015          | 0.3806 |
| 0.3693        | 7.4526  | 5500  | 0.2966          | 0.3693 |
| 0.6693        | 7.5881  | 5600  | 0.3112          | 0.3693 |
| 0.4791        | 7.7236  | 5700  | 0.3076          | 0.3596 |
| 0.3541        | 7.8591  | 5800  | 0.3129          | 0.3590 |
| 0.4792        | 7.9946  | 5900  | 0.3028          | 0.3590 |
| 0.3385        | 8.1301  | 6000  | 0.3127          | 0.3960 |
| 0.6656        | 8.2656  | 6100  | 0.3139          | 0.3806 |
| 0.3467        | 8.4011  | 6200  | 0.3235          | 0.3706 |
| 0.4183        | 8.5366  | 6300  | 0.2967          | 0.3624 |
| 0.4535        | 8.6721  | 6400  | 0.3101          | 0.3524 |
| 0.3376        | 8.8076  | 6500  | 0.3109          | 0.3966 |
| 0.4307        | 8.9431  | 6600  | 0.3290          | 0.3822 |
| 0.4738        | 9.0786  | 6700  | 0.3083          | 0.3511 |
| 0.4388        | 9.2141  | 6800  | 0.2974          | 0.3778 |
| 0.3341        | 9.3496  | 6900  | 0.2917          | 0.3417 |
| 0.5376        | 9.4851  | 7000  | 0.2960          | 0.3436 |
| 0.4649        | 9.6206  | 7100  | 0.3293          | 0.3593 |
| 0.347         | 9.7561  | 7200  | 0.2865          | 0.3470 |
| 0.3213        | 9.8916  | 7300  | 0.2892          | 0.3458 |
| 0.5857        | 10.0271 | 7400  | 0.2850          | 0.3386 |
| 0.3177        | 10.1626 | 7500  | 0.2967          | 0.3502 |
| 0.3382        | 10.2981 | 7600  | 0.2828          | 0.3411 |
| 0.6518        | 10.4336 | 7700  | 0.2918          | 0.3467 |
| 0.3755        | 10.5691 | 7800  | 0.2953          | 0.3404 |
| 0.2995        | 10.7046 | 7900  | 0.2988          | 0.3646 |
| 0.3929        | 10.8401 | 8000  | 0.2903          | 0.3477 |
| 0.3097        | 10.9756 | 8100  | 0.2762          | 0.3659 |
| 0.3004        | 11.1111 | 8200  | 0.2929          | 0.3599 |
| 0.312         | 11.2466 | 8300  | 0.2822          | 0.3527 |
| 0.3812        | 11.3821 | 8400  | 0.2833          | 0.3386 |
| 0.3812        | 11.5176 | 8500  | 0.2926          | 0.3395 |
| 0.3133        | 11.6531 | 8600  | 0.2838          | 0.3414 |
| 0.5852        | 11.7886 | 8700  | 0.2836          | 0.3517 |
| 0.3755        | 11.9241 | 8800  | 0.2772          | 0.3536 |
| 0.3526        | 12.0596 | 8900  | 0.2828          | 0.3621 |
| 0.3837        | 12.1951 | 9000  | 0.2826          | 0.3652 |
| 0.318         | 12.3306 | 9100  | 0.2790          | 0.3696 |
| 0.2932        | 12.4661 | 9200  | 0.2846          | 0.3649 |
| 0.5292        | 12.6016 | 9300  | 0.2754          | 0.3417 |
| 0.2977        | 12.7371 | 9400  | 0.2808          | 0.3652 |
| 0.3229        | 12.8726 | 9500  | 0.2811          | 0.3521 |
| 0.3711        | 13.0081 | 9600  | 0.2855          | 0.3436 |
| 0.255         | 13.1436 | 9700  | 0.3100          | 0.3546 |
| 0.369         | 13.2791 | 9800  | 0.2865          | 0.3326 |
| 0.3094        | 13.4146 | 9900  | 0.2817          | 0.3423 |
| 0.5302        | 13.5501 | 10000 | 0.2715          | 0.3295 |
| 0.3156        | 13.6856 | 10100 | 0.2777          | 0.3439 |
| 0.356         | 13.8211 | 10200 | 0.2766          | 0.3354 |
| 0.2856        | 13.9566 | 10300 | 0.2806          | 0.3436 |
| 0.326         | 14.0921 | 10400 | 0.2738          | 0.3502 |
| 0.4744        | 14.2276 | 10500 | 0.2732          | 0.3339 |
| 0.2968        | 14.3631 | 10600 | 0.2683          | 0.3332 |
| 0.2897        | 14.4986 | 10700 | 0.2704          | 0.3373 |
| 0.2684        | 14.6341 | 10800 | 0.2694          | 0.3470 |
| 0.2939        | 14.7696 | 10900 | 0.2762          | 0.3345 |
| 0.2777        | 14.9051 | 11000 | 0.2646          | 0.3514 |
| 0.2785        | 15.0407 | 11100 | 0.2633          | 0.3404 |
| 0.2633        | 15.1762 | 11200 | 0.2640          | 0.3426 |
| 0.2957        | 15.3117 | 11300 | 0.2654          | 0.3546 |
| 0.2758        | 15.4472 | 11400 | 0.2694          | 0.3467 |
| 0.2881        | 15.5827 | 11500 | 0.2628          | 0.3329 |
| 0.522         | 15.7182 | 11600 | 0.2636          | 0.3348 |
| 0.2944        | 15.8537 | 11700 | 0.2656          | 0.3458 |
| 0.2911        | 15.9892 | 11800 | 0.2598          | 0.3401 |
| 0.2578        | 16.1247 | 11900 | 0.2629          | 0.3452 |
| 0.2737        | 16.2602 | 12000 | 0.2612          | 0.3464 |
| 0.3107        | 16.3957 | 12100 | 0.2566          | 0.3401 |
| 0.4585        | 16.5312 | 12200 | 0.2489          | 0.3549 |
| 0.2808        | 16.6667 | 12300 | 0.2566          | 0.3558 |
| 0.2682        | 16.8022 | 12400 | 0.2566          | 0.3455 |
| 0.2603        | 16.9377 | 12500 | 0.2552          | 0.3348 |
| 0.2951        | 17.0732 | 12600 | 0.2521          | 0.3310 |
| 0.2658        | 17.2087 | 12700 | 0.2599          | 0.3332 |
| 0.2642        | 17.3442 | 12800 | 0.2578          | 0.3282 |
| 0.2603        | 17.4797 | 12900 | 0.2586          | 0.3204 |
| 0.2675        | 17.6152 | 13000 | 0.2625          | 0.3332 |
| 0.2587        | 17.7507 | 13100 | 0.2566          | 0.3282 |
| 0.4357        | 17.8862 | 13200 | 0.2550          | 0.3191 |
| 0.2487        | 18.0217 | 13300 | 0.2534          | 0.3248 |
| 0.4185        | 18.1572 | 13400 | 0.2579          | 0.3241 |
| 0.2771        | 18.2927 | 13500 | 0.2549          | 0.3201 |
| 0.2514        | 18.4282 | 13600 | 0.2588          | 0.3175 |
| 0.2615        | 18.5637 | 13700 | 0.2527          | 0.3222 |
| 0.2658        | 18.6992 | 13800 | 0.2534          | 0.3160 |
| 0.2415        | 18.8347 | 13900 | 0.2525          | 0.3229 |
| 0.2427        | 18.9702 | 14000 | 0.2533          | 0.3248 |
| 0.2477        | 19.1057 | 14100 | 0.2517          | 0.3185 |
| 0.2475        | 19.2412 | 14200 | 0.2521          | 0.3201 |
| 0.259         | 19.3767 | 14300 | 0.2535          | 0.3232 |
| 0.2765        | 19.5122 | 14400 | 0.2522          | 0.3197 |
| 0.2503        | 19.6477 | 14500 | 0.2513          | 0.3210 |
| 0.3933        | 19.7832 | 14600 | 0.2525          | 0.3188 |
| 0.2434        | 19.9187 | 14700 | 0.2527          | 0.3191 |


### Framework versions

- Transformers 4.46.3
- Pytorch 2.4.1+cu121
- Datasets 3.1.0
- Tokenizers 0.20.3",10537.0,2.904134522024853e-06
toxibunny/mistral-small-rpmax-base-rpmax-shallow-v-curve-slerp,671bbc271bafae643630962a,toxibunny,"{""base_model"": [""unsloth/Mistral-Small-Instruct-2409"", ""ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1""], ""library_name"": ""transformers"", ""tags"": [""mergekit"", ""merge""]}",[],2024-10-25 15:55:25,0,0.0,"{""architectures"": [""MistralForCausalLM""], ""model_type"": ""mistral"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{%- if messages[0][\""role\""] == \""system\"" %}\n    {%- set system_message = messages[0][\""content\""] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\""role\"", \""equalto\"", \""user\"") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \""tool\"" or message.role == \""tool_results\"" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\""role\""] == \""user\"") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\""After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\"") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\""role\""] == \""user\"" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \""[AVAILABLE_TOOLS] [\"" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\""type\"": \""function\"", \""function\"": {' }}\n                {%- for key, val in tool.items() if key != \""return\"" %}\n                    {%- if val is string %}\n                        {{- '\""' + key + '\"": \""' + val + '\""' }}\n                    {%- else %}\n                        {{- '\""' + key + '\"": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \"", \"" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \""}}\"" }}\n                {%- if not loop.last %}\n                    {{- \"", \"" }}\n                {%- else %}\n                    {{- \""]\"" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \""[/AVAILABLE_TOOLS]\"" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \""[INST] \"" + system_message + \""\\n\\n\"" + message[\""content\""] + \""[/INST]\"" }}\n        {%- else %}\n            {{- \""[INST] \"" + message[\""content\""] + \""[/INST]\"" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \""[TOOL_CALLS] [\"" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\""Tool call IDs should be alphanumeric strings with length 9!\"") }}\n            {%- endif %}\n            {{- ', \""id\"": \""' + tool_call.id + '\""}' }}\n            {%- if not loop.last %}\n                {{- \"", \"" }}\n            {%- else %}\n                {{- \""]\"" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\""role\""] == \""assistant\"" %}\n        {{- \"" \"" + message[\""content\""]|trim + eos_token}}\n    {%- elif message[\""role\""] == \""tool_results\"" or message[\""role\""] == \""tool\"" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS] {\""content\"": ' + content|string + \"", \"" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\""Tool call IDs should be alphanumeric strings with length 9!\"") }}\n        {%- endif %}\n        {{- '\""call_id\"": \""' + message.tool_call_id + '\""}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\""Only user and assistant roles are supported, with the exception of an initial optional system message!\"") }}\n    {%- endif %}\n{%- endfor %}\n"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}}",0,72,"{'parameters': {'BF16': 22247282688.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 22247282688.0}","['transformers' 'safetensors' 'mistral' 'text-generation' 'mergekit'
 'merge' 'conversational'
 'base_model:ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1'
 'base_model:merge:ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1'
 'base_model:unsloth/Mistral-Small-Instruct-2409'
 'base_model:merge:unsloth/Mistral-Small-Instruct-2409'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'mergekit_config.yml'}
 {'rfilename': 'model-00001-of-00010.safetensors'}
 {'rfilename': 'model-00002-of-00010.safetensors'}
 {'rfilename': 'model-00003-of-00010.safetensors'}
 {'rfilename': 'model-00004-of-00010.safetensors'}
 {'rfilename': 'model-00005-of-00010.safetensors'}
 {'rfilename': 'model-00006-of-00010.safetensors'}
 {'rfilename': 'model-00007-of-00010.safetensors'}
 {'rfilename': 'model-00008-of-00010.safetensors'}
 {'rfilename': 'model-00009-of-00010.safetensors'}
 {'rfilename': 'model-00010-of-00010.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2024-10-25 15:41:27,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,merge,66e9c1018230291463949a03,unsloth/Mistral-Small-Instruct-2409,toxibunny/mistral-small-rpmax-base-rpmax-shallow-v-curve-slerp,"base_model:
- unsloth/Mistral-Small-Instruct-2409
- ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1
library_name: transformers
tags:
- mergekit
- merge","# merge

This is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).

## Merge Details
### Merge Method

This model was merged using the SLERP merge method.

### Models Merged

The following models were included in the merge:
* [unsloth/Mistral-Small-Instruct-2409](https://huggingface.co/unsloth/Mistral-Small-Instruct-2409)
* [ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1](https://huggingface.co/ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1)

### Configuration

The following YAML configuration was used to produce this model:

```yaml
models:
  - model: unsloth/Mistral-Small-Instruct-2409
  - model: ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1
merge_method: slerp
base_model: ArliAI/Mistral-Small-22B-ArliAI-RPMax-v1.1
dtype: bfloat16
parameters:
  t: [0.55, 0.35, 0.25, 0.35, 0.55] # shallow shaped curve: mostly rpmax for input & output, mostly base in the middle layers

```",924.0,2.877127359118973e-06
CoCoRooXin/lora_adapter_bart_mnli_eu,660d44e6269cc6397c436603,CoCoRooXin,"{""license"": ""mit"", ""library_name"": ""peft"", ""tags"": [""generated_from_trainer""], ""base_model"": ""facebook/bart-large-mnli"", ""metrics"": [""f1"", ""precision"", ""recall"", ""accuracy""], ""model-index"": [{""name"": ""finetuned_bart"", ""results"": []}]}",[],2024-05-17 08:40:49,0,0.0,"{""tokenizer_config"": {""bos_token"": ""<s>"", ""cls_token"": ""<s>"", ""eos_token"": ""</s>"", ""mask_token"": ""<mask>"", ""pad_token"": ""<pad>"", ""sep_token"": ""</s>"", ""unk_token"": ""<unk>""}, ""peft"": {""base_model_name_or_path"": ""facebook/bart-large-mnli"", ""task_type"": ""SEQ_CLS""}}",1,72,,"['peft' 'safetensors' 'generated_from_trainer'
 'base_model:facebook/bart-large-mnli'
 'base_model:adapter:facebook/bart-large-mnli' 'license:mit' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'merges.txt'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.json'}]",2024-04-03 12:00:38,peft,,,adapter,621ffdc136468d709f17adb7,facebook/bart-large-mnli,CoCoRooXin/lora_adapter_bart_mnli_eu,"license: mit
library_name: peft
tags:
- generated_from_trainer
base_model: facebook/bart-large-mnli
metrics:
- f1
- precision
- recall
- accuracy
model-index:
- name: finetuned_bart
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# finetuned_bart

This model is a fine-tuned version of [facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0620
- F1: 0.9236
- Precision: 0.9000
- Recall: 0.9485
- Accuracy: 0.9216

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 64
- eval_batch_size: 64
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- lr_scheduler_warmup_steps: 20
- num_epochs: 2

### Training results

| Training Loss | Epoch  | Step | Validation Loss | F1     | Precision | Recall | Accuracy |
|:-------------:|:------:|:----:|:---------------:|:------:|:---------:|:------:|:--------:|
| 0.0856        | 0.0933 | 50   | 0.0695          | 0.9122 | 0.9010    | 0.9238 | 0.9111   |
| 0.0593        | 0.1866 | 100  | 0.0685          | 0.9152 | 0.8970    | 0.9341 | 0.9135   |
| 0.0572        | 0.2799 | 150  | 0.0681          | 0.9149 | 0.8997    | 0.9306 | 0.9135   |
| 0.0549        | 0.3731 | 200  | 0.0679          | 0.9150 | 0.9054    | 0.9249 | 0.9141   |
| 0.0529        | 0.4664 | 250  | 0.0678          | 0.9174 | 0.9043    | 0.9308 | 0.9162   |
| 0.0776        | 0.5597 | 300  | 0.0673          | 0.9158 | 0.9079    | 0.9238 | 0.9151   |
| 0.0799        | 0.6530 | 350  | 0.0647          | 0.9201 | 0.8964    | 0.9450 | 0.9179   |
| 0.0806        | 0.7463 | 400  | 0.0647          | 0.9196 | 0.8968    | 0.9436 | 0.9175   |
| 0.0781        | 0.8396 | 450  | 0.0635          | 0.9193 | 0.8982    | 0.9415 | 0.9174   |
| 0.0771        | 0.9328 | 500  | 0.0633          | 0.9189 | 0.9019    | 0.9366 | 0.9174   |
| 0.0787        | 1.0261 | 550  | 0.0629          | 0.9202 | 0.8994    | 0.9420 | 0.9184   |
| 0.0737        | 1.1194 | 600  | 0.0627          | 0.9210 | 0.8989    | 0.9442 | 0.9190   |
| 0.0722        | 1.2127 | 650  | 0.0634          | 0.9212 | 0.8981    | 0.9455 | 0.9192   |
| 0.0684        | 1.3060 | 700  | 0.0630          | 0.9217 | 0.9065    | 0.9374 | 0.9204   |
| 0.0655        | 1.3993 | 750  | 0.0629          | 0.9228 | 0.8974    | 0.9496 | 0.9205   |
| 0.0739        | 1.4925 | 800  | 0.0625          | 0.9229 | 0.8993    | 0.9477 | 0.9208   |
| 0.0666        | 1.5858 | 850  | 0.0625          | 0.9233 | 0.8962    | 0.9521 | 0.9209   |
| 0.0703        | 1.6791 | 900  | 0.0621          | 0.9238 | 0.9001    | 0.9488 | 0.9218   |
| 0.0738        | 1.7724 | 950  | 0.0617          | 0.9227 | 0.9007    | 0.9458 | 0.9208   |
| 0.068         | 1.8657 | 1000 | 0.0620          | 0.9233 | 0.9002    | 0.9477 | 0.9213   |
| 0.069         | 1.9590 | 1050 | 0.0620          | 0.9236 | 0.9000    | 0.9485 | 0.9216   |


### Framework versions

- PEFT 0.10.0
- Transformers 4.40.2
- Pytorch 2.3.0+cu121
- Datasets 2.19.1
- Tokenizers 0.19.1",3254.0,2.877127359118973e-06
duskdagger/nnUD1fYy,68049acf6fa9802f240b8b6a,duskdagger,"{""tags"": [""text-to-image"", ""lora"", ""diffusers"", ""template:diffusion-lora""], ""widget"": [{""text"": ""-"", ""output"": {""url"": ""https://huggingface.co/duskdagger/nnUD1fYy/resolve/main/images/360_F_66233948_PZNkyexlCBb80NFf4UpS4RH7ScGC2LuI.jpg""}}], ""base_model"": ""black-forest-labs/FLUX.1-dev"", ""instance_prompt"": null}","[{'_id': '680b71dfa07785a516f270ef', 'adapter': 'lora', 'adapterWeightsPath': 'nnUD1fYy.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'fal-ai', 'providerDetails': None, 'providerId': 'fal-ai/flux-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}
 {'_id': '6818c94d5111343b72f18ab4', 'adapter': 'lora', 'adapterWeightsPath': 'nnUD1fYy.safetensors', 'features': None, 'isModelAuthor': False, 'provider': 'replicate', 'providerDetails': None, 'providerId': 'black-forest-labs/flux-dev-lora', 'status': 'live', 'task': 'text-to-image', 'type': 'tag-filter'}]",2025-04-20 06:57:33,0,0.0,,13,70,,"['diffusers' 'text-to-image' 'lora' 'template:diffusion-lora'
 'base_model:black-forest-labs/FLUX.1-dev'
 'base_model:adapter:black-forest-labs/FLUX.1-dev' 'region:us']",text-to-image,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'images/360_F_66233948_PZNkyexlCBb80NFf4UpS4RH7ScGC2LuI.jpg'}
 {'rfilename': 'nnUD1fYy.safetensors'}]",2025-04-20 06:57:19,diffusers,,,adapter,66aaa908fc35e079a941470d,black-forest-labs/FLUX.1-dev,duskdagger/nnUD1fYy,"tags:
  - text-to-image
  - lora
  - diffusers
  - template:diffusion-lora
widget:
- text: '-'
  output:
    url: images/360_F_66233948_PZNkyexlCBb80NFf4UpS4RH7ScGC2LuI.jpg
base_model: black-forest-labs/FLUX.1-dev
instance_prompt: null","# nnUD1fYy

<Gallery />



## Download model

Weights for this model are available in Safetensors format.

[Download](/duskdagger/nnUD1fYy/tree/main) them in the Files & versions tab.",183.0,2.858498737869153e-06
kk-aivio/825f504c-ef85-4f8e-ae13-0a5a1f581cb1,679418d14342735860e9493d,kk-aivio,"{""library_name"": ""peft"", ""license"": ""mit"", ""base_model"": ""fxmarty/really-tiny-falcon-testing"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""825f504c-ef85-4f8e-ae13-0a5a1f581cb1"", ""results"": []}]}",[],2025-01-24 22:49:18,0,0.0,"{""architectures"": [""FalconForCausalLM""], ""auto_map"": {""AutoConfig"": ""tiiuae/falcon-7b--configuration_falcon.FalconConfig"", ""AutoModel"": ""tiiuae/falcon-7b--modeling_falcon.FalconModel"", ""AutoModelForCausalLM"": ""tiiuae/falcon-7b--modeling_falcon.FalconForCausalLM"", ""AutoModelForQuestionAnswering"": ""tiiuae/falcon-7b--modeling_falcon.FalconForQuestionAnswering"", ""AutoModelForSequenceClassification"": ""tiiuae/falcon-7b--modeling_falcon.FalconForSequenceClassification"", ""AutoModelForTokenClassification"": ""tiiuae/falcon-7b--modeling_falcon.FalconForTokenClassification""}, ""model_type"": ""falcon"", ""tokenizer_config"": {""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""<|endoftext|>"", ""pad_token"": ""<|endoftext|>""}, ""peft"": {""base_model_name_or_path"": ""fxmarty/really-tiny-falcon-testing"", ""task_type"": ""CAUSAL_LM""}}",1,63,,"['peft' 'safetensors' 'falcon' 'axolotl' 'generated_from_trainer'
 'custom_code' 'base_model:fxmarty/really-tiny-falcon-testing'
 'base_model:adapter:fxmarty/really-tiny-falcon-testing' 'license:mit'
 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'last-checkpoint/README.md'}
 {'rfilename': 'last-checkpoint/adapter_config.json'}
 {'rfilename': 'last-checkpoint/adapter_model.safetensors'}
 {'rfilename': 'last-checkpoint/optimizer.pt'}
 {'rfilename': 'last-checkpoint/rng_state.pth'}
 {'rfilename': 'last-checkpoint/scheduler.pt'}
 {'rfilename': 'last-checkpoint/special_tokens_map.json'}
 {'rfilename': 'last-checkpoint/tokenizer.json'}
 {'rfilename': 'last-checkpoint/tokenizer_config.json'}
 {'rfilename': 'last-checkpoint/trainer_state.json'}
 {'rfilename': 'last-checkpoint/training_args.bin'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2025-01-24 22:48:49,peft,,,adapter,65056b684a8839a8bd5a72d8,fxmarty/really-tiny-falcon-testing,kk-aivio/825f504c-ef85-4f8e-ae13-0a5a1f581cb1,"library_name: peft
license: mit
base_model: fxmarty/really-tiny-falcon-testing
tags:
- axolotl
- generated_from_trainer
model-index:
- name: 825f504c-ef85-4f8e-ae13-0a5a1f581cb1
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: fxmarty/really-tiny-falcon-testing
bf16: auto
chat_template: llama3
dataset_prepared_path: null
datasets:
- data_files:
  - adb8f10c3929bd22_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/adb8f10c3929bd22_train_data.json
  type:
    field_input: Category Label
    field_instruction: Product Title
    field_output: Cluster Label
    format: '{instruction} {input}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
early_stopping_patience: null
eval_max_new_tokens: 128
eval_table_size: null
evals_per_epoch: 4
flash_attention: false
fp16: null
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 4
gradient_checkpointing: false
group_by_length: false
hub_model_id: kk-aivio/825f504c-ef85-4f8e-ae13-0a5a1f581cb1
hub_repo: null
hub_strategy: checkpoint
hub_token: null
learning_rate: 0.0002
load_in_4bit: false
load_in_8bit: false
local_rank: null
logging_steps: 1
lora_alpha: 16
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 8
lora_target_linear: true
lr_scheduler: cosine
max_steps: 10
micro_batch_size: 2
mlflow_experiment_name: /tmp/adb8f10c3929bd22_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 1
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
saves_per_epoch: 4
sequence_len: 512
strict: false
tf32: false
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: 1ea7c675-4778-48c0-b140-f57c55fbe73a
wandb_project: Birthday-SN56-11-Gradients-On-Demand
wandb_run: your_name
wandb_runid: 1ea7c675-4778-48c0-b140-f57c55fbe73a
warmup_steps: 10
weight_decay: 0.0
xformers_attention: null

```

</details><br>

# 825f504c-ef85-4f8e-ae13-0a5a1f581cb1

This model is a fine-tuned version of [fxmarty/really-tiny-falcon-testing](https://huggingface.co/fxmarty/really-tiny-falcon-testing) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 11.0923

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 8
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- training_steps: 10

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 44.3552       | 0.0002 | 1    | 11.0933         |
| 44.4248       | 0.0007 | 3    | 11.0933         |
| 44.4068       | 0.0014 | 6    | 11.0929         |
| 44.4072       | 0.0022 | 9    | 11.0923         |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3695.0,2.788893932372996e-06
MayBashendy/ArabicNewSplits7_usingWellWrittenEssays_FineTuningAraBERT_run1_AugV5_k8_task3_organization,67890ab562afb9408704f91d,MayBashendy,"{""library_name"": ""transformers"", ""base_model"": ""aubmindlab/bert-base-arabertv02"", ""tags"": [""generated_from_trainer""], ""model-index"": [{""name"": ""ArabicNewSplits7_usingWellWrittenEssays_FineTuningAraBERT_run1_AugV5_k8_task3_organization"", ""results"": []}]}",[],2025-01-16 13:46:27,0,0.0,"{""architectures"": [""BertForSequenceClassification""], ""model_type"": ""bert""}",1,59,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 135194113, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 135194113.0}","['transformers' 'safetensors' 'bert' 'text-classification'
 'generated_from_trainer' 'base_model:aubmindlab/bert-base-arabertv02'
 'base_model:finetune:aubmindlab/bert-base-arabertv02'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'training_args.bin'}]",2025-01-16 13:33:41,transformers,"{'auto_model': 'AutoModelForSequenceClassification', 'custom_class': None, 'pipeline_tag': 'text-classification', 'processor': 'AutoTokenizer'}",,finetune,621ffdc136468d709f1794dc,aubmindlab/bert-base-arabertv02,MayBashendy/ArabicNewSplits7_usingWellWrittenEssays_FineTuningAraBERT_run1_AugV5_k8_task3_organization,"library_name: transformers
base_model: aubmindlab/bert-base-arabertv02
tags:
- generated_from_trainer
model-index:
- name: ArabicNewSplits7_usingWellWrittenEssays_FineTuningAraBERT_run1_AugV5_k8_task3_organization
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# ArabicNewSplits7_usingWellWrittenEssays_FineTuningAraBERT_run1_AugV5_k8_task3_organization

This model is a fine-tuned version of [aubmindlab/bert-base-arabertv02](https://huggingface.co/aubmindlab/bert-base-arabertv02) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7121
- Qwk: -0.1074
- Mse: 0.7121
- Rmse: 0.8438

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 100

### Training results

| Training Loss | Epoch | Step | Validation Loss | Qwk     | Mse    | Rmse   |
|:-------------:|:-----:|:----:|:---------------:|:-------:|:------:|:------:|
| No log        | 0.1   | 2    | 3.6105          | -0.0154 | 3.6105 | 1.9001 |
| No log        | 0.2   | 4    | 2.0063          | 0.0737  | 2.0063 | 1.4164 |
| No log        | 0.3   | 6    | 1.5750          | -0.0052 | 1.5750 | 1.2550 |
| No log        | 0.4   | 8    | 1.5447          | 0.0045  | 1.5447 | 1.2429 |
| No log        | 0.5   | 10   | 0.8430          | -0.0390 | 0.8430 | 0.9181 |
| No log        | 0.6   | 12   | 0.8100          | -0.1244 | 0.8100 | 0.9000 |
| No log        | 0.7   | 14   | 1.0247          | -0.0677 | 1.0247 | 1.0123 |
| No log        | 0.8   | 16   | 1.1745          | -0.0500 | 1.1745 | 1.0837 |
| No log        | 0.9   | 18   | 1.2688          | 0.0     | 1.2688 | 1.1264 |
| No log        | 1.0   | 20   | 1.2170          | -0.0500 | 1.2170 | 1.1032 |
| No log        | 1.1   | 22   | 1.1158          | -0.1292 | 1.1158 | 1.0563 |
| No log        | 1.2   | 24   | 1.0329          | -0.1283 | 1.0329 | 1.0163 |
| No log        | 1.3   | 26   | 1.0156          | -0.0345 | 1.0156 | 1.0078 |
| No log        | 1.4   | 28   | 1.3154          | -0.0234 | 1.3154 | 1.1469 |
| No log        | 1.5   | 30   | 1.2443          | -0.0500 | 1.2443 | 1.1155 |
| No log        | 1.6   | 32   | 0.9414          | -0.0545 | 0.9414 | 0.9702 |
| No log        | 1.7   | 34   | 0.7682          | 0.0506  | 0.7682 | 0.8765 |
| No log        | 1.8   | 36   | 0.7625          | -0.0101 | 0.7625 | 0.8732 |
| No log        | 1.9   | 38   | 0.8906          | -0.0163 | 0.8906 | 0.9437 |
| No log        | 2.0   | 40   | 0.9122          | 0.0260  | 0.9122 | 0.9551 |
| No log        | 2.1   | 42   | 0.8041          | -0.0215 | 0.8041 | 0.8967 |
| No log        | 2.2   | 44   | 0.7653          | -0.0069 | 0.7653 | 0.8748 |
| No log        | 2.3   | 46   | 0.7655          | -0.0101 | 0.7655 | 0.8749 |
| No log        | 2.4   | 48   | 0.7848          | -0.0695 | 0.7848 | 0.8859 |
| No log        | 2.5   | 50   | 0.8288          | -0.0725 | 0.8288 | 0.9104 |
| No log        | 2.6   | 52   | 0.8874          | -0.0331 | 0.8874 | 0.9420 |
| No log        | 2.7   | 54   | 0.8174          | 0.0416  | 0.8174 | 0.9041 |
| No log        | 2.8   | 56   | 0.7871          | 0.0     | 0.7871 | 0.8872 |
| No log        | 2.9   | 58   | 0.8159          | -0.0551 | 0.8159 | 0.9033 |
| No log        | 3.0   | 60   | 0.9945          | -0.0408 | 0.9945 | 0.9972 |
| No log        | 3.1   | 62   | 0.8409          | 0.1318  | 0.8409 | 0.9170 |
| No log        | 3.2   | 64   | 0.8483          | -0.2435 | 0.8483 | 0.9210 |
| No log        | 3.3   | 66   | 0.8508          | -0.2435 | 0.8508 | 0.9224 |
| No log        | 3.4   | 68   | 0.8241          | -0.2006 | 0.8241 | 0.9078 |
| No log        | 3.5   | 70   | 0.7980          | -0.2101 | 0.7980 | 0.8933 |
| No log        | 3.6   | 72   | 0.8492          | -0.2435 | 0.8492 | 0.9215 |
| No log        | 3.7   | 74   | 0.8380          | -0.0030 | 0.8380 | 0.9154 |
| No log        | 3.8   | 76   | 0.8516          | -0.2006 | 0.8516 | 0.9228 |
| No log        | 3.9   | 78   | 0.8549          | -0.1992 | 0.8549 | 0.9246 |
| No log        | 4.0   | 80   | 0.8656          | -0.1204 | 0.8656 | 0.9304 |
| No log        | 4.1   | 82   | 0.8906          | 0.0357  | 0.8906 | 0.9437 |
| No log        | 4.2   | 84   | 0.9325          | -0.1093 | 0.9325 | 0.9657 |
| No log        | 4.3   | 86   | 0.8649          | -0.0699 | 0.8649 | 0.9300 |
| No log        | 4.4   | 88   | 0.7990          | -0.1635 | 0.7990 | 0.8939 |
| No log        | 4.5   | 90   | 0.8595          | -0.1833 | 0.8595 | 0.9271 |
| No log        | 4.6   | 92   | 0.7673          | -0.1067 | 0.7673 | 0.8760 |
| No log        | 4.7   | 94   | 0.7837          | 0.0296  | 0.7837 | 0.8852 |
| No log        | 4.8   | 96   | 0.7576          | -0.0499 | 0.7576 | 0.8704 |
| No log        | 4.9   | 98   | 0.7556          | -0.0499 | 0.7556 | 0.8693 |
| No log        | 5.0   | 100  | 0.9407          | -0.0391 | 0.9407 | 0.9699 |
| No log        | 5.1   | 102  | 1.0165          | -0.1214 | 1.0165 | 1.0082 |
| No log        | 5.2   | 104  | 0.7893          | -0.0091 | 0.7893 | 0.8884 |
| No log        | 5.3   | 106  | 0.7909          | -0.1001 | 0.7909 | 0.8893 |
| No log        | 5.4   | 108  | 0.8122          | -0.2006 | 0.8122 | 0.9012 |
| No log        | 5.5   | 110  | 0.8233          | -0.0595 | 0.8233 | 0.9074 |
| No log        | 5.6   | 112  | 0.9621          | 0.0043  | 0.9621 | 0.9808 |
| No log        | 5.7   | 114  | 0.8721          | -0.0550 | 0.8721 | 0.9338 |
| No log        | 5.8   | 116  | 0.8436          | -0.0532 | 0.8436 | 0.9185 |
| No log        | 5.9   | 118  | 0.8233          | -0.0532 | 0.8233 | 0.9073 |
| No log        | 6.0   | 120  | 0.8484          | -0.0145 | 0.8484 | 0.9211 |
| No log        | 6.1   | 122  | 0.9398          | 0.0214  | 0.9398 | 0.9694 |
| No log        | 6.2   | 124  | 0.8318          | -0.0118 | 0.8318 | 0.9120 |
| No log        | 6.3   | 126  | 0.8167          | 0.0     | 0.8167 | 0.9037 |
| No log        | 6.4   | 128  | 0.8238          | -0.0145 | 0.8238 | 0.9077 |
| No log        | 6.5   | 130  | 0.8263          | 0.0247  | 0.8263 | 0.9090 |
| No log        | 6.6   | 132  | 0.7763          | 0.0355  | 0.7763 | 0.8811 |
| No log        | 6.7   | 134  | 0.7439          | 0.0454  | 0.7439 | 0.8625 |
| No log        | 6.8   | 136  | 0.8259          | -0.1329 | 0.8259 | 0.9088 |
| No log        | 6.9   | 138  | 0.8297          | -0.1753 | 0.8297 | 0.9109 |
| No log        | 7.0   | 140  | 0.7905          | 0.0030  | 0.7905 | 0.8891 |
| No log        | 7.1   | 142  | 0.8196          | 0.0375  | 0.8196 | 0.9053 |
| No log        | 7.2   | 144  | 0.8503          | -0.1730 | 0.8503 | 0.9221 |
| No log        | 7.3   | 146  | 0.8754          | -0.1412 | 0.8754 | 0.9356 |
| No log        | 7.4   | 148  | 0.9132          | -0.1709 | 0.9132 | 0.9556 |
| No log        | 7.5   | 150  | 0.7885          | -0.0520 | 0.7885 | 0.8880 |
| No log        | 7.6   | 152  | 0.9114          | -0.0056 | 0.9114 | 0.9547 |
| No log        | 7.7   | 154  | 1.1910          | -0.0648 | 1.1910 | 1.0913 |
| No log        | 7.8   | 156  | 0.9422          | -0.0143 | 0.9422 | 0.9707 |
| No log        | 7.9   | 158  | 0.7327          | -0.0033 | 0.7327 | 0.8560 |
| No log        | 8.0   | 160  | 0.8270          | -0.0499 | 0.8270 | 0.9094 |
| No log        | 8.1   | 162  | 0.8446          | -0.0451 | 0.8446 | 0.9190 |
| No log        | 8.2   | 164  | 0.7794          | -0.0520 | 0.7794 | 0.8829 |
| No log        | 8.3   | 166  | 0.8011          | 0.0814  | 0.8011 | 0.8950 |
| No log        | 8.4   | 168  | 0.8043          | 0.0247  | 0.8043 | 0.8969 |
| No log        | 8.5   | 170  | 0.7808          | -0.0520 | 0.7808 | 0.8836 |
| No log        | 8.6   | 172  | 0.8984          | -0.1201 | 0.8984 | 0.9479 |
| No log        | 8.7   | 174  | 0.8881          | -0.1263 | 0.8881 | 0.9424 |
| No log        | 8.8   | 176  | 0.7662          | -0.0520 | 0.7662 | 0.8753 |
| No log        | 8.9   | 178  | 0.7992          | 0.0099  | 0.7992 | 0.8940 |
| No log        | 9.0   | 180  | 0.9820          | -0.0143 | 0.9820 | 0.9910 |
| No log        | 9.1   | 182  | 0.8249          | 0.0099  | 0.8249 | 0.9083 |
| No log        | 9.2   | 184  | 0.7441          | -0.0520 | 0.7441 | 0.8626 |
| No log        | 9.3   | 186  | 0.8031          | -0.1067 | 0.8031 | 0.8962 |
| No log        | 9.4   | 188  | 0.7615          | -0.0520 | 0.7615 | 0.8726 |
| No log        | 9.5   | 190  | 0.7445          | -0.0520 | 0.7445 | 0.8628 |
| No log        | 9.6   | 192  | 0.7626          | 0.0454  | 0.7626 | 0.8733 |
| No log        | 9.7   | 194  | 0.7804          | 0.0454  | 0.7804 | 0.8834 |
| No log        | 9.8   | 196  | 0.8459          | -0.0345 | 0.8459 | 0.9197 |
| No log        | 9.9   | 198  | 0.8819          | -0.1647 | 0.8819 | 0.9391 |
| No log        | 10.0  | 200  | 0.8031          | 0.0488  | 0.8031 | 0.8962 |
| No log        | 10.1  | 202  | 0.7860          | 0.0454  | 0.7860 | 0.8866 |
| No log        | 10.2  | 204  | 0.7613          | 0.0414  | 0.7613 | 0.8725 |
| No log        | 10.3  | 206  | 0.7542          | 0.0454  | 0.7542 | 0.8684 |
| No log        | 10.4  | 208  | 0.7609          | 0.0454  | 0.7609 | 0.8723 |
| No log        | 10.5  | 210  | 0.7874          | -0.0541 | 0.7874 | 0.8874 |
| No log        | 10.6  | 212  | 0.8226          | -0.0322 | 0.8226 | 0.9070 |
| No log        | 10.7  | 214  | 0.7948          | -0.0541 | 0.7948 | 0.8915 |
| No log        | 10.8  | 216  | 0.7555          | 0.0454  | 0.7555 | 0.8692 |
| No log        | 10.9  | 218  | 0.7606          | 0.0414  | 0.7606 | 0.8721 |
| No log        | 11.0  | 220  | 0.7658          | -0.0541 | 0.7658 | 0.8751 |
| No log        | 11.1  | 222  | 0.8259          | -0.0881 | 0.8259 | 0.9088 |
| No log        | 11.2  | 224  | 0.8325          | -0.1329 | 0.8325 | 0.9124 |
| No log        | 11.3  | 226  | 0.7250          | -0.0520 | 0.7250 | 0.8515 |
| No log        | 11.4  | 228  | 0.7269          | 0.1691  | 0.7269 | 0.8526 |
| No log        | 11.5  | 230  | 0.7374          | 0.1691  | 0.7374 | 0.8587 |
| No log        | 11.6  | 232  | 0.7053          | 0.1021  | 0.7053 | 0.8398 |
| No log        | 11.7  | 234  | 0.7768          | -0.1067 | 0.7768 | 0.8814 |
| No log        | 11.8  | 236  | 0.7752          | -0.0520 | 0.7752 | 0.8805 |
| No log        | 11.9  | 238  | 0.7680          | -0.0541 | 0.7680 | 0.8764 |
| No log        | 12.0  | 240  | 0.7733          | 0.0414  | 0.7733 | 0.8794 |
| No log        | 12.1  | 242  | 0.8509          | 0.0346  | 0.8509 | 0.9224 |
| No log        | 12.2  | 244  | 0.8611          | 0.0316  | 0.8611 | 0.9279 |
| No log        | 12.3  | 246  | 0.7583          | 0.1202  | 0.7583 | 0.8708 |
| No log        | 12.4  | 248  | 0.7904          | 0.0064  | 0.7904 | 0.8891 |
| No log        | 12.5  | 250  | 0.8538          | -0.1538 | 0.8538 | 0.9240 |
| No log        | 12.6  | 252  | 0.7928          | -0.0451 | 0.7928 | 0.8904 |
| No log        | 12.7  | 254  | 0.7428          | 0.1512  | 0.7428 | 0.8619 |
| No log        | 12.8  | 256  | 0.8312          | -0.0056 | 0.8312 | 0.9117 |
| No log        | 12.9  | 258  | 0.8416          | -0.0056 | 0.8416 | 0.9174 |
| No log        | 13.0  | 260  | 0.7585          | 0.0909  | 0.7585 | 0.8709 |
| No log        | 13.1  | 262  | 0.7917          | -0.1473 | 0.7917 | 0.8898 |
| No log        | 13.2  | 264  | 0.8267          | -0.0774 | 0.8267 | 0.9093 |
| No log        | 13.3  | 266  | 0.8030          | -0.0469 | 0.8030 | 0.8961 |
| No log        | 13.4  | 268  | 0.7762          | -0.0152 | 0.7762 | 0.8810 |
| No log        | 13.5  | 270  | 0.7758          | 0.0334  | 0.7758 | 0.8808 |
| No log        | 13.6  | 272  | 0.7741          | 0.0334  | 0.7741 | 0.8798 |
| No log        | 13.7  | 274  | 0.7797          | 0.0807  | 0.7797 | 0.8830 |
| No log        | 13.8  | 276  | 0.7770          | 0.0807  | 0.7770 | 0.8815 |
| No log        | 13.9  | 278  | 0.7604          | -0.0152 | 0.7604 | 0.8720 |
| No log        | 14.0  | 280  | 0.7505          | 0.0395  | 0.7505 | 0.8663 |
| No log        | 14.1  | 282  | 0.7452          | 0.0714  | 0.7452 | 0.8632 |
| No log        | 14.2  | 284  | 0.7359          | 0.0807  | 0.7359 | 0.8578 |
| No log        | 14.3  | 286  | 0.7468          | 0.0857  | 0.7468 | 0.8642 |
| No log        | 14.4  | 288  | 0.7715          | 0.0714  | 0.7715 | 0.8783 |
| No log        | 14.5  | 290  | 0.7626          | 0.0857  | 0.7626 | 0.8733 |
| No log        | 14.6  | 292  | 0.7969          | -0.1964 | 0.7969 | 0.8927 |
| No log        | 14.7  | 294  | 0.8148          | -0.1470 | 0.8148 | 0.9026 |
| No log        | 14.8  | 296  | 0.7894          | -0.1088 | 0.7894 | 0.8885 |
| No log        | 14.9  | 298  | 0.7734          | 0.0395  | 0.7734 | 0.8795 |
| No log        | 15.0  | 300  | 0.7739          | -0.0033 | 0.7739 | 0.8797 |
| No log        | 15.1  | 302  | 0.7759          | -0.0033 | 0.7759 | 0.8809 |
| No log        | 15.2  | 304  | 0.8015          | -0.1074 | 0.8015 | 0.8953 |
| No log        | 15.3  | 306  | 0.8282          | -0.1992 | 0.8282 | 0.9101 |
| No log        | 15.4  | 308  | 0.7896          | 0.0479  | 0.7896 | 0.8886 |
| No log        | 15.5  | 310  | 0.7956          | 0.0479  | 0.7956 | 0.8920 |
| No log        | 15.6  | 312  | 0.7856          | 0.1379  | 0.7856 | 0.8863 |
| No log        | 15.7  | 314  | 0.7714          | 0.1371  | 0.7714 | 0.8783 |
| No log        | 15.8  | 316  | 0.8113          | 0.1031  | 0.8113 | 0.9007 |
| No log        | 15.9  | 318  | 0.7709          | 0.1474  | 0.7709 | 0.8780 |
| No log        | 16.0  | 320  | 0.7325          | 0.1474  | 0.7325 | 0.8558 |
| No log        | 16.1  | 322  | 0.7553          | 0.1024  | 0.7553 | 0.8691 |
| No log        | 16.2  | 324  | 0.8279          | -0.2733 | 0.8279 | 0.9099 |
| No log        | 16.3  | 326  | 0.8307          | -0.1917 | 0.8307 | 0.9114 |
| No log        | 16.4  | 328  | 0.7748          | 0.0     | 0.7748 | 0.8802 |
| No log        | 16.5  | 330  | 0.7643          | 0.0355  | 0.7643 | 0.8743 |
| No log        | 16.6  | 332  | 0.7937          | -0.0264 | 0.7937 | 0.8909 |
| No log        | 16.7  | 334  | 0.8167          | 0.0454  | 0.8167 | 0.9037 |
| No log        | 16.8  | 336  | 0.8489          | -0.1470 | 0.8489 | 0.9214 |
| No log        | 16.9  | 338  | 0.8514          | -0.1833 | 0.8514 | 0.9227 |
| No log        | 17.0  | 340  | 0.8430          | -0.1992 | 0.8430 | 0.9181 |
| No log        | 17.1  | 342  | 0.8328          | 0.0454  | 0.8328 | 0.9126 |
| No log        | 17.2  | 344  | 0.8298          | -0.0704 | 0.8298 | 0.9109 |
| No log        | 17.3  | 346  | 0.8128          | 0.0318  | 0.8128 | 0.9016 |
| No log        | 17.4  | 348  | 0.8080          | -0.1026 | 0.8080 | 0.8989 |
| No log        | 17.5  | 350  | 0.8152          | -0.1547 | 0.8152 | 0.9029 |
| No log        | 17.6  | 352  | 0.8205          | -0.0892 | 0.8205 | 0.9058 |
| No log        | 17.7  | 354  | 0.8291          | -0.0488 | 0.8291 | 0.9105 |
| No log        | 17.8  | 356  | 0.8313          | -0.0628 | 0.8313 | 0.9118 |
| No log        | 17.9  | 358  | 0.8284          | 0.0338  | 0.8284 | 0.9101 |
| No log        | 18.0  | 360  | 0.8113          | 0.0869  | 0.8113 | 0.9007 |
| No log        | 18.1  | 362  | 0.8175          | -0.1331 | 0.8175 | 0.9042 |
| No log        | 18.2  | 364  | 0.8059          | -0.0469 | 0.8059 | 0.8977 |
| No log        | 18.3  | 366  | 0.8151          | -0.0469 | 0.8151 | 0.9028 |
| No log        | 18.4  | 368  | 0.7845          | 0.0479  | 0.7845 | 0.8857 |
| No log        | 18.5  | 370  | 0.7670          | 0.0967  | 0.7670 | 0.8758 |
| No log        | 18.6  | 372  | 0.7633          | 0.0759  | 0.7633 | 0.8737 |
| No log        | 18.7  | 374  | 0.7666          | 0.0807  | 0.7666 | 0.8755 |
| No log        | 18.8  | 376  | 0.7848          | 0.0967  | 0.7848 | 0.8859 |
| No log        | 18.9  | 378  | 0.8140          | -0.1547 | 0.8140 | 0.9022 |
| No log        | 19.0  | 380  | 0.8479          | -0.1905 | 0.8479 | 0.9208 |
| No log        | 19.1  | 382  | 0.8366          | -0.1470 | 0.8366 | 0.9146 |
| No log        | 19.2  | 384  | 0.8128          | -0.1547 | 0.8128 | 0.9016 |
| No log        | 19.3  | 386  | 0.7848          | -0.0560 | 0.7848 | 0.8859 |
| No log        | 19.4  | 388  | 0.7744          | 0.0479  | 0.7744 | 0.8800 |
| No log        | 19.5  | 390  | 0.7648          | 0.0479  | 0.7648 | 0.8745 |
| No log        | 19.6  | 392  | 0.7535          | -0.0033 | 0.7535 | 0.8681 |
| No log        | 19.7  | 394  | 0.7672          | -0.1074 | 0.7672 | 0.8759 |
| No log        | 19.8  | 396  | 0.7712          | -0.1074 | 0.7712 | 0.8782 |
| No log        | 19.9  | 398  | 0.7570          | 0.0479  | 0.7570 | 0.8700 |
| No log        | 20.0  | 400  | 0.7594          | 0.0967  | 0.7594 | 0.8714 |
| No log        | 20.1  | 402  | 0.7703          | 0.0524  | 0.7703 | 0.8777 |
| No log        | 20.2  | 404  | 0.7827          | -0.0520 | 0.7827 | 0.8847 |
| No log        | 20.3  | 406  | 0.7917          | -0.0499 | 0.7917 | 0.8898 |
| No log        | 20.4  | 408  | 0.7862          | 0.0033  | 0.7862 | 0.8867 |
| No log        | 20.5  | 410  | 0.8028          | -0.1547 | 0.8028 | 0.8960 |
| No log        | 20.6  | 412  | 0.8113          | 0.1023  | 0.8113 | 0.9007 |
| No log        | 20.7  | 414  | 0.8135          | -0.0252 | 0.8135 | 0.9020 |
| No log        | 20.8  | 416  | 0.8003          | 0.1023  | 0.8003 | 0.8946 |
| No log        | 20.9  | 418  | 0.8343          | -0.0345 | 0.8343 | 0.9134 |
| No log        | 21.0  | 420  | 0.8693          | -0.1146 | 0.8693 | 0.9324 |
| No log        | 21.1  | 422  | 0.8350          | -0.1398 | 0.8350 | 0.9138 |
| No log        | 21.2  | 424  | 0.7865          | 0.0     | 0.7865 | 0.8868 |
| No log        | 21.3  | 426  | 0.7712          | 0.1023  | 0.7712 | 0.8782 |
| No log        | 21.4  | 428  | 0.7768          | 0.1023  | 0.7768 | 0.8814 |
| No log        | 21.5  | 430  | 0.7876          | 0.0355  | 0.7876 | 0.8875 |
| No log        | 21.6  | 432  | 0.7994          | 0.0783  | 0.7994 | 0.8941 |
| No log        | 21.7  | 434  | 0.7939          | 0.1249  | 0.7939 | 0.8910 |
| No log        | 21.8  | 436  | 0.7747          | 0.0432  | 0.7747 | 0.8802 |
| No log        | 21.9  | 438  | 0.7595          | 0.1027  | 0.7595 | 0.8715 |
| No log        | 22.0  | 440  | 0.7675          | 0.0640  | 0.7675 | 0.8761 |
| No log        | 22.1  | 442  | 0.7598          | -0.0345 | 0.7598 | 0.8717 |
| No log        | 22.2  | 444  | 0.7393          | -0.1010 | 0.7393 | 0.8598 |
| No log        | 22.3  | 446  | 0.7053          | 0.0     | 0.7053 | 0.8399 |
| No log        | 22.4  | 448  | 0.7068          | 0.0436  | 0.7068 | 0.8407 |
| No log        | 22.5  | 450  | 0.7219          | 0.0914  | 0.7219 | 0.8497 |
| No log        | 22.6  | 452  | 0.7464          | -0.0032 | 0.7464 | 0.8639 |
| No log        | 22.7  | 454  | 0.7858          | 0.0122  | 0.7858 | 0.8865 |
| No log        | 22.8  | 456  | 0.8064          | -0.0837 | 0.8064 | 0.8980 |
| No log        | 22.9  | 458  | 0.8091          | -0.1823 | 0.8091 | 0.8995 |
| No log        | 23.0  | 460  | 0.7729          | 0.0967  | 0.7729 | 0.8791 |
| No log        | 23.1  | 462  | 0.7813          | 0.0318  | 0.7813 | 0.8839 |
| No log        | 23.2  | 464  | 0.7806          | -0.0204 | 0.7806 | 0.8835 |
| No log        | 23.3  | 466  | 0.7568          | 0.0318  | 0.7568 | 0.8699 |
| No log        | 23.4  | 468  | 0.7552          | 0.0318  | 0.7552 | 0.8690 |
| No log        | 23.5  | 470  | 0.7624          | 0.0     | 0.7624 | 0.8731 |
| No log        | 23.6  | 472  | 0.7961          | -0.1330 | 0.7961 | 0.8922 |
| No log        | 23.7  | 474  | 0.8045          | -0.0786 | 0.8045 | 0.8970 |
| No log        | 23.8  | 476  | 0.7936          | -0.0786 | 0.7936 | 0.8908 |
| No log        | 23.9  | 478  | 0.7754          | -0.0958 | 0.7754 | 0.8806 |
| No log        | 24.0  | 480  | 0.8044          | -0.0271 | 0.8044 | 0.8969 |
| No log        | 24.1  | 482  | 0.8074          | -0.0271 | 0.8074 | 0.8985 |
| No log        | 24.2  | 484  | 0.8126          | -0.0786 | 0.8126 | 0.9014 |
| No log        | 24.3  | 486  | 0.7906          | -0.0837 | 0.7906 | 0.8892 |
| No log        | 24.4  | 488  | 0.7602          | -0.1010 | 0.7602 | 0.8719 |
| No log        | 24.5  | 490  | 0.7335          | 0.0967  | 0.7335 | 0.8565 |
| No log        | 24.6  | 492  | 0.7286          | 0.0967  | 0.7286 | 0.8536 |
| No log        | 24.7  | 494  | 0.7397          | 0.0318  | 0.7397 | 0.8601 |
| No log        | 24.8  | 496  | 0.7378          | 0.0318  | 0.7378 | 0.8589 |
| No log        | 24.9  | 498  | 0.7651          | -0.1074 | 0.7651 | 0.8747 |
| 0.2273        | 25.0  | 500  | 0.8548          | -0.2101 | 0.8548 | 0.9245 |
| 0.2273        | 25.1  | 502  | 0.9021          | -0.1833 | 0.9021 | 0.9498 |
| 0.2273        | 25.2  | 504  | 0.9058          | -0.1263 | 0.9058 | 0.9517 |
| 0.2273        | 25.3  | 506  | 0.8703          | -0.1329 | 0.8703 | 0.9329 |
| 0.2273        | 25.4  | 508  | 0.7920          | -0.2067 | 0.7920 | 0.8899 |
| 0.2273        | 25.5  | 510  | 0.7785          | -0.2067 | 0.7785 | 0.8823 |
| 0.2273        | 25.6  | 512  | 0.8053          | -0.1547 | 0.8053 | 0.8974 |
| 0.2273        | 25.7  | 514  | 0.8261          | -0.2101 | 0.8261 | 0.9089 |
| 0.2273        | 25.8  | 516  | 0.7827          | -0.1074 | 0.7827 | 0.8847 |
| 0.2273        | 25.9  | 518  | 0.7293          | -0.0591 | 0.7293 | 0.8540 |
| 0.2273        | 26.0  | 520  | 0.7262          | -0.0591 | 0.7262 | 0.8521 |
| 0.2273        | 26.1  | 522  | 0.7529          | 0.0     | 0.7529 | 0.8677 |
| 0.2273        | 26.2  | 524  | 0.7624          | -0.0520 | 0.7624 | 0.8732 |
| 0.2273        | 26.3  | 526  | 0.7390          | 0.0     | 0.7390 | 0.8597 |
| 0.2273        | 26.4  | 528  | 0.7247          | 0.0909  | 0.7247 | 0.8513 |
| 0.2273        | 26.5  | 530  | 0.7498          | 0.0759  | 0.7498 | 0.8659 |
| 0.2273        | 26.6  | 532  | 0.7318          | 0.0909  | 0.7318 | 0.8554 |
| 0.2273        | 26.7  | 534  | 0.7491          | 0.0     | 0.7491 | 0.8655 |
| 0.2273        | 26.8  | 536  | 0.8430          | -0.0892 | 0.8430 | 0.9181 |
| 0.2273        | 26.9  | 538  | 0.8485          | -0.0892 | 0.8485 | 0.9211 |
| 0.2273        | 27.0  | 540  | 0.7804          | -0.1547 | 0.7804 | 0.8834 |
| 0.2273        | 27.1  | 542  | 0.7481          | 0.0     | 0.7481 | 0.8649 |
| 0.2273        | 27.2  | 544  | 0.7593          | 0.0318  | 0.7593 | 0.8714 |
| 0.2273        | 27.3  | 546  | 0.7498          | 0.0479  | 0.7498 | 0.8659 |
| 0.2273        | 27.4  | 548  | 0.7552          | 0.0     | 0.7552 | 0.8690 |
| 0.2273        | 27.5  | 550  | 0.7769          | -0.1470 | 0.7769 | 0.8814 |
| 0.2273        | 27.6  | 552  | 0.7704          | -0.1010 | 0.7704 | 0.8777 |
| 0.2273        | 27.7  | 554  | 0.7692          | -0.1074 | 0.7692 | 0.8770 |
| 0.2273        | 27.8  | 556  | 0.7517          | -0.0033 | 0.7517 | 0.8670 |
| 0.2273        | 27.9  | 558  | 0.7489          | 0.0914  | 0.7489 | 0.8654 |
| 0.2273        | 28.0  | 560  | 0.7468          | 0.0     | 0.7468 | 0.8642 |
| 0.2273        | 28.1  | 562  | 0.7407          | 0.0374  | 0.7407 | 0.8607 |
| 0.2273        | 28.2  | 564  | 0.7440          | 0.1021  | 0.7440 | 0.8625 |
| 0.2273        | 28.3  | 566  | 0.7437          | 0.0374  | 0.7437 | 0.8624 |
| 0.2273        | 28.4  | 568  | 0.7555          | 0.0479  | 0.7555 | 0.8692 |
| 0.2273        | 28.5  | 570  | 0.7757          | -0.1547 | 0.7757 | 0.8807 |
| 0.2273        | 28.6  | 572  | 0.7965          | -0.0892 | 0.7965 | 0.8925 |
| 0.2273        | 28.7  | 574  | 0.8215          | -0.0837 | 0.8215 | 0.9063 |
| 0.2273        | 28.8  | 576  | 0.8617          | -0.1678 | 0.8617 | 0.9283 |
| 0.2273        | 28.9  | 578  | 0.8728          | -0.1329 | 0.8728 | 0.9342 |
| 0.2273        | 29.0  | 580  | 0.8618          | -0.2006 | 0.8618 | 0.9284 |
| 0.2273        | 29.1  | 582  | 0.8083          | -0.0499 | 0.8083 | 0.8991 |
| 0.2273        | 29.2  | 584  | 0.7440          | 0.0033  | 0.7440 | 0.8625 |
| 0.2273        | 29.3  | 586  | 0.7171          | 0.0479  | 0.7171 | 0.8468 |
| 0.2273        | 29.4  | 588  | 0.7343          | -0.0152 | 0.7343 | 0.8569 |
| 0.2273        | 29.5  | 590  | 0.7488          | -0.0152 | 0.7488 | 0.8653 |
| 0.2273        | 29.6  | 592  | 0.7410          | -0.0125 | 0.7410 | 0.8608 |
| 0.2273        | 29.7  | 594  | 0.7336          | 0.0479  | 0.7336 | 0.8565 |
| 0.2273        | 29.8  | 596  | 0.7271          | 0.0     | 0.7271 | 0.8527 |
| 0.2273        | 29.9  | 598  | 0.7229          | 0.0     | 0.7229 | 0.8502 |
| 0.2273        | 30.0  | 600  | 0.7254          | 0.0555  | 0.7254 | 0.8517 |
| 0.2273        | 30.1  | 602  | 0.7289          | 0.1444  | 0.7289 | 0.8537 |
| 0.2273        | 30.2  | 604  | 0.7449          | 0.1318  | 0.7449 | 0.8631 |
| 0.2273        | 30.3  | 606  | 0.7606          | -0.0032 | 0.7606 | 0.8721 |
| 0.2273        | 30.4  | 608  | 0.7700          | -0.1547 | 0.7700 | 0.8775 |
| 0.2273        | 30.5  | 610  | 0.7952          | -0.0473 | 0.7952 | 0.8917 |
| 0.2273        | 30.6  | 612  | 0.7898          | -0.0499 | 0.7898 | 0.8887 |
| 0.2273        | 30.7  | 614  | 0.7492          | -0.0551 | 0.7492 | 0.8656 |
| 0.2273        | 30.8  | 616  | 0.7079          | 0.0     | 0.7079 | 0.8413 |
| 0.2273        | 30.9  | 618  | 0.7687          | -0.0204 | 0.7687 | 0.8767 |
| 0.2273        | 31.0  | 620  | 0.8394          | 0.0456  | 0.8394 | 0.9162 |
| 0.2273        | 31.1  | 622  | 0.8411          | 0.0392  | 0.8411 | 0.9171 |
| 0.2273        | 31.2  | 624  | 0.8003          | -0.0295 | 0.8003 | 0.8946 |
| 0.2273        | 31.3  | 626  | 0.7788          | 0.0479  | 0.7788 | 0.8825 |
| 0.2273        | 31.4  | 628  | 0.8523          | 0.0260  | 0.8523 | 0.9232 |
| 0.2273        | 31.5  | 630  | 0.9161          | -0.0477 | 0.9161 | 0.9571 |
| 0.2273        | 31.6  | 632  | 0.9214          | -0.0377 | 0.9214 | 0.9599 |
| 0.2273        | 31.7  | 634  | 0.8850          | 0.0748  | 0.8850 | 0.9407 |
| 0.2273        | 31.8  | 636  | 0.8437          | -0.0921 | 0.8437 | 0.9185 |
| 0.2273        | 31.9  | 638  | 0.8518          | 0.0113  | 0.8518 | 0.9229 |
| 0.2273        | 32.0  | 640  | 0.8748          | 0.0748  | 0.8748 | 0.9353 |
| 0.2273        | 32.1  | 642  | 0.8467          | 0.0871  | 0.8467 | 0.9201 |
| 0.2273        | 32.2  | 644  | 0.7967          | 0.0395  | 0.7967 | 0.8926 |
| 0.2273        | 32.3  | 646  | 0.7747          | 0.0496  | 0.7747 | 0.8802 |
| 0.2273        | 32.4  | 648  | 0.7711          | 0.0033  | 0.7711 | 0.8781 |
| 0.2273        | 32.5  | 650  | 0.7812          | 0.0541  | 0.7812 | 0.8839 |
| 0.2273        | 32.6  | 652  | 0.8060          | 0.0395  | 0.8060 | 0.8978 |
| 0.2273        | 32.7  | 654  | 0.8596          | -0.1194 | 0.8596 | 0.9271 |
| 0.2273        | 32.8  | 656  | 0.9094          | -0.0122 | 0.9094 | 0.9536 |
| 0.2273        | 32.9  | 658  | 0.9185          | -0.0285 | 0.9185 | 0.9584 |
| 0.2273        | 33.0  | 660  | 0.8752          | 0.0067  | 0.8752 | 0.9355 |
| 0.2273        | 33.1  | 662  | 0.8226          | 0.0409  | 0.8226 | 0.9070 |
| 0.2273        | 33.2  | 664  | 0.8328          | -0.0059 | 0.8328 | 0.9126 |
| 0.2273        | 33.3  | 666  | 0.8759          | -0.0513 | 0.8759 | 0.9359 |
| 0.2273        | 33.4  | 668  | 0.9069          | -0.0093 | 0.9069 | 0.9523 |
| 0.2273        | 33.5  | 670  | 0.9015          | -0.0591 | 0.9015 | 0.9495 |
| 0.2273        | 33.6  | 672  | 0.8665          | -0.1833 | 0.8665 | 0.9308 |
| 0.2273        | 33.7  | 674  | 0.7977          | 0.0     | 0.7977 | 0.8931 |
| 0.2273        | 33.8  | 676  | 0.7798          | 0.0436  | 0.7798 | 0.8831 |
| 0.2273        | 33.9  | 678  | 0.7827          | 0.0318  | 0.7827 | 0.8847 |
| 0.2273        | 34.0  | 680  | 0.7834          | 0.0318  | 0.7834 | 0.8851 |
| 0.2273        | 34.1  | 682  | 0.7929          | 0.0395  | 0.7929 | 0.8904 |
| 0.2273        | 34.2  | 684  | 0.8049          | -0.0595 | 0.8049 | 0.8971 |
| 0.2273        | 34.3  | 686  | 0.8139          | -0.0578 | 0.8139 | 0.9021 |
| 0.2273        | 34.4  | 688  | 0.8286          | -0.1538 | 0.8286 | 0.9103 |
| 0.2273        | 34.5  | 690  | 0.8426          | -0.0949 | 0.8426 | 0.9180 |
| 0.2273        | 34.6  | 692  | 0.8365          | -0.0870 | 0.8365 | 0.9146 |
| 0.2273        | 34.7  | 694  | 0.8286          | -0.0370 | 0.8286 | 0.9103 |
| 0.2273        | 34.8  | 696  | 0.8193          | -0.0059 | 0.8193 | 0.9051 |
| 0.2273        | 34.9  | 698  | 0.8337          | 0.0670  | 0.8337 | 0.9131 |
| 0.2273        | 35.0  | 700  | 0.8107          | -0.0204 | 0.8107 | 0.9004 |
| 0.2273        | 35.1  | 702  | 0.7891          | 0.0436  | 0.7891 | 0.8883 |
| 0.2273        | 35.2  | 704  | 0.7774          | -0.0520 | 0.7774 | 0.8817 |
| 0.2273        | 35.3  | 706  | 0.7502          | -0.0551 | 0.7502 | 0.8661 |
| 0.2273        | 35.4  | 708  | 0.7332          | -0.0033 | 0.7332 | 0.8563 |
| 0.2273        | 35.5  | 710  | 0.7215          | 0.0479  | 0.7215 | 0.8494 |
| 0.2273        | 35.6  | 712  | 0.7369          | 0.0863  | 0.7369 | 0.8585 |
| 0.2273        | 35.7  | 714  | 0.7525          | 0.0814  | 0.7525 | 0.8675 |
| 0.2273        | 35.8  | 716  | 0.7554          | 0.0814  | 0.7554 | 0.8691 |
| 0.2273        | 35.9  | 718  | 0.7531          | 0.0863  | 0.7531 | 0.8678 |
| 0.2273        | 36.0  | 720  | 0.7482          | 0.0436  | 0.7482 | 0.8650 |
| 0.2273        | 36.1  | 722  | 0.7580          | 0.0479  | 0.7580 | 0.8706 |
| 0.2273        | 36.2  | 724  | 0.7895          | -0.0520 | 0.7895 | 0.8885 |
| 0.2273        | 36.3  | 726  | 0.8240          | -0.0363 | 0.8240 | 0.9077 |
| 0.2273        | 36.4  | 728  | 0.8219          | -0.0826 | 0.8219 | 0.9066 |
| 0.2273        | 36.5  | 730  | 0.8020          | -0.0949 | 0.8020 | 0.8955 |
| 0.2273        | 36.6  | 732  | 0.7800          | -0.1018 | 0.7800 | 0.8832 |
| 0.2273        | 36.7  | 734  | 0.7648          | 0.0914  | 0.7648 | 0.8745 |
| 0.2273        | 36.8  | 736  | 0.7749          | -0.0295 | 0.7749 | 0.8803 |
| 0.2273        | 36.9  | 738  | 0.7704          | -0.0295 | 0.7704 | 0.8777 |
| 0.2273        | 37.0  | 740  | 0.7567          | 0.0183  | 0.7567 | 0.8699 |
| 0.2273        | 37.1  | 742  | 0.7455          | 0.0247  | 0.7455 | 0.8634 |
| 0.2273        | 37.2  | 744  | 0.7442          | 0.0214  | 0.7442 | 0.8627 |
| 0.2273        | 37.3  | 746  | 0.7436          | 0.0247  | 0.7436 | 0.8623 |
| 0.2273        | 37.4  | 748  | 0.7366          | -0.0152 | 0.7366 | 0.8582 |
| 0.2273        | 37.5  | 750  | 0.7225          | 0.0479  | 0.7225 | 0.8500 |
| 0.2273        | 37.6  | 752  | 0.7274          | 0.0     | 0.7274 | 0.8529 |
| 0.2273        | 37.7  | 754  | 0.7352          | 0.0     | 0.7352 | 0.8574 |
| 0.2273        | 37.8  | 756  | 0.7263          | 0.0524  | 0.7263 | 0.8522 |
| 0.2273        | 37.9  | 758  | 0.7269          | 0.0436  | 0.7269 | 0.8526 |
| 0.2273        | 38.0  | 760  | 0.7692          | 0.1096  | 0.7692 | 0.8771 |
| 0.2273        | 38.1  | 762  | 0.8081          | 0.0867  | 0.8081 | 0.8990 |
| 0.2273        | 38.2  | 764  | 0.7908          | 0.0909  | 0.7908 | 0.8892 |
| 0.2273        | 38.3  | 766  | 0.7507          | 0.1202  | 0.7507 | 0.8664 |
| 0.2273        | 38.4  | 768  | 0.7348          | 0.0436  | 0.7348 | 0.8572 |
| 0.2273        | 38.5  | 770  | 0.7514          | -0.0520 | 0.7514 | 0.8669 |
| 0.2273        | 38.6  | 772  | 0.7673          | -0.0949 | 0.7673 | 0.8760 |
| 0.2273        | 38.7  | 774  | 0.7488          | -0.0520 | 0.7488 | 0.8653 |
| 0.2273        | 38.8  | 776  | 0.7366          | 0.0436  | 0.7366 | 0.8583 |
| 0.2273        | 38.9  | 778  | 0.7556          | 0.1202  | 0.7556 | 0.8692 |
| 0.2273        | 39.0  | 780  | 0.7699          | 0.1899  | 0.7699 | 0.8774 |
| 0.2273        | 39.1  | 782  | 0.7598          | 0.1202  | 0.7598 | 0.8717 |
| 0.2273        | 39.2  | 784  | 0.7307          | 0.0395  | 0.7307 | 0.8548 |
| 0.2273        | 39.3  | 786  | 0.7131          | 0.0479  | 0.7131 | 0.8444 |
| 0.2273        | 39.4  | 788  | 0.6971          | 0.0524  | 0.6971 | 0.8349 |
| 0.2273        | 39.5  | 790  | 0.6870          | 0.0     | 0.6870 | 0.8289 |
| 0.2273        | 39.6  | 792  | 0.6911          | -0.0551 | 0.6911 | 0.8313 |
| 0.2273        | 39.7  | 794  | 0.6982          | -0.0551 | 0.6982 | 0.8356 |
| 0.2273        | 39.8  | 796  | 0.6940          | -0.0551 | 0.6940 | 0.8331 |
| 0.2273        | 39.9  | 798  | 0.6906          | 0.0     | 0.6906 | 0.8310 |
| 0.2273        | 40.0  | 800  | 0.6983          | 0.0479  | 0.6983 | 0.8357 |
| 0.2273        | 40.1  | 802  | 0.7245          | 0.1379  | 0.7245 | 0.8512 |
| 0.2273        | 40.2  | 804  | 0.7467          | 0.1691  | 0.7467 | 0.8641 |
| 0.2273        | 40.3  | 806  | 0.7478          | 0.1691  | 0.7478 | 0.8648 |
| 0.2273        | 40.4  | 808  | 0.7443          | 0.1691  | 0.7443 | 0.8627 |
| 0.2273        | 40.5  | 810  | 0.7392          | 0.1902  | 0.7392 | 0.8598 |
| 0.2273        | 40.6  | 812  | 0.7376          | 0.0479  | 0.7376 | 0.8588 |
| 0.2273        | 40.7  | 814  | 0.7529          | -0.1547 | 0.7529 | 0.8677 |
| 0.2273        | 40.8  | 816  | 0.8084          | -0.1547 | 0.8084 | 0.8991 |
| 0.2273        | 40.9  | 818  | 0.8366          | -0.2006 | 0.8366 | 0.9147 |
| 0.2273        | 41.0  | 820  | 0.8095          | -0.1547 | 0.8095 | 0.8997 |
| 0.2273        | 41.1  | 822  | 0.7758          | -0.0520 | 0.7758 | 0.8808 |
| 0.2273        | 41.2  | 824  | 0.7292          | -0.0520 | 0.7292 | 0.8539 |
| 0.2273        | 41.3  | 826  | 0.7024          | 0.0524  | 0.7024 | 0.8381 |
| 0.2273        | 41.4  | 828  | 0.7079          | 0.1444  | 0.7079 | 0.8414 |
| 0.2273        | 41.5  | 830  | 0.7165          | 0.1202  | 0.7165 | 0.8465 |
| 0.2273        | 41.6  | 832  | 0.7188          | 0.1202  | 0.7188 | 0.8478 |
| 0.2273        | 41.7  | 834  | 0.7066          | 0.1379  | 0.7066 | 0.8406 |
| 0.2273        | 41.8  | 836  | 0.7026          | 0.0524  | 0.7026 | 0.8382 |
| 0.2273        | 41.9  | 838  | 0.7161          | -0.0520 | 0.7161 | 0.8462 |
| 0.2273        | 42.0  | 840  | 0.7205          | -0.0520 | 0.7205 | 0.8488 |
| 0.2273        | 42.1  | 842  | 0.7259          | -0.0520 | 0.7259 | 0.8520 |
| 0.2273        | 42.2  | 844  | 0.7227          | -0.0520 | 0.7227 | 0.8501 |
| 0.2273        | 42.3  | 846  | 0.7076          | 0.0     | 0.7076 | 0.8412 |
| 0.2273        | 42.4  | 848  | 0.7047          | 0.0     | 0.7047 | 0.8394 |
| 0.2273        | 42.5  | 850  | 0.7032          | 0.0     | 0.7032 | 0.8386 |
| 0.2273        | 42.6  | 852  | 0.6973          | 0.0     | 0.6973 | 0.8351 |
| 0.2273        | 42.7  | 854  | 0.7244          | 0.1199  | 0.7244 | 0.8511 |
| 0.2273        | 42.8  | 856  | 0.7872          | 0.0909  | 0.7872 | 0.8873 |
| 0.2273        | 42.9  | 858  | 0.8247          | 0.0409  | 0.8247 | 0.9081 |
| 0.2273        | 43.0  | 860  | 0.7920          | 0.0512  | 0.7920 | 0.8900 |
| 0.2273        | 43.1  | 862  | 0.7595          | 0.1311  | 0.7595 | 0.8715 |
| 0.2273        | 43.2  | 864  | 0.7400          | 0.0914  | 0.7400 | 0.8602 |
| 0.2273        | 43.3  | 866  | 0.7300          | 0.0914  | 0.7300 | 0.8544 |
| 0.2273        | 43.4  | 868  | 0.7238          | 0.0863  | 0.7238 | 0.8508 |
| 0.2273        | 43.5  | 870  | 0.7371          | 0.0863  | 0.7371 | 0.8586 |
| 0.2273        | 43.6  | 872  | 0.7454          | 0.1691  | 0.7454 | 0.8634 |
| 0.2273        | 43.7  | 874  | 0.7310          | 0.1758  | 0.7310 | 0.8550 |
| 0.2273        | 43.8  | 876  | 0.7202          | 0.1379  | 0.7202 | 0.8487 |
| 0.2273        | 43.9  | 878  | 0.7088          | 0.0863  | 0.7088 | 0.8419 |
| 0.2273        | 44.0  | 880  | 0.7101          | 0.0524  | 0.7101 | 0.8426 |
| 0.2273        | 44.1  | 882  | 0.7329          | -0.0520 | 0.7329 | 0.8561 |
| 0.2273        | 44.2  | 884  | 0.7567          | -0.1547 | 0.7567 | 0.8699 |
| 0.2273        | 44.3  | 886  | 0.7755          | -0.0892 | 0.7755 | 0.8806 |
| 0.2273        | 44.4  | 888  | 0.7729          | -0.0949 | 0.7729 | 0.8792 |
| 0.2273        | 44.5  | 890  | 0.7681          | -0.1547 | 0.7681 | 0.8764 |
| 0.2273        | 44.6  | 892  | 0.7551          | -0.0520 | 0.7551 | 0.8689 |
| 0.2273        | 44.7  | 894  | 0.7627          | 0.0     | 0.7627 | 0.8734 |
| 0.2273        | 44.8  | 896  | 0.7763          | -0.0551 | 0.7763 | 0.8811 |
| 0.2273        | 44.9  | 898  | 0.7582          | -0.0551 | 0.7582 | 0.8708 |
| 0.2273        | 45.0  | 900  | 0.7389          | 0.0     | 0.7389 | 0.8596 |
| 0.2273        | 45.1  | 902  | 0.7174          | 0.0     | 0.7174 | 0.8470 |
| 0.2273        | 45.2  | 904  | 0.6995          | 0.0     | 0.6995 | 0.8363 |
| 0.2273        | 45.3  | 906  | 0.7122          | 0.0395  | 0.7122 | 0.8439 |
| 0.2273        | 45.4  | 908  | 0.7294          | 0.0395  | 0.7294 | 0.8540 |
| 0.2273        | 45.5  | 910  | 0.7309          | 0.0395  | 0.7309 | 0.8549 |
| 0.2273        | 45.6  | 912  | 0.7406          | -0.0541 | 0.7406 | 0.8606 |
| 0.2273        | 45.7  | 914  | 0.7657          | -0.1547 | 0.7657 | 0.8750 |
| 0.2273        | 45.8  | 916  | 0.7760          | -0.0949 | 0.7760 | 0.8809 |
| 0.2273        | 45.9  | 918  | 0.7624          | -0.0520 | 0.7624 | 0.8732 |
| 0.2273        | 46.0  | 920  | 0.7440          | -0.0520 | 0.7440 | 0.8625 |
| 0.2273        | 46.1  | 922  | 0.7251          | 0.0524  | 0.7251 | 0.8515 |
| 0.2273        | 46.2  | 924  | 0.7199          | 0.0524  | 0.7199 | 0.8484 |
| 0.2273        | 46.3  | 926  | 0.7148          | 0.0479  | 0.7148 | 0.8455 |
| 0.2273        | 46.4  | 928  | 0.7260          | 0.0395  | 0.7260 | 0.8520 |
| 0.2273        | 46.5  | 930  | 0.7575          | 0.1612  | 0.7575 | 0.8704 |
| 0.2273        | 46.6  | 932  | 0.8065          | 0.1395  | 0.8065 | 0.8981 |
| 0.2273        | 46.7  | 934  | 0.7879          | 0.1047  | 0.7879 | 0.8876 |
| 0.2273        | 46.8  | 936  | 0.7574          | -0.0595 | 0.7574 | 0.8703 |
| 0.2273        | 46.9  | 938  | 0.7420          | -0.0541 | 0.7420 | 0.8614 |
| 0.2273        | 47.0  | 940  | 0.7400          | -0.0520 | 0.7400 | 0.8602 |
| 0.2273        | 47.1  | 942  | 0.7480          | -0.0520 | 0.7480 | 0.8649 |
| 0.2273        | 47.2  | 944  | 0.7438          | -0.0520 | 0.7438 | 0.8624 |
| 0.2273        | 47.3  | 946  | 0.7368          | 0.0     | 0.7368 | 0.8584 |
| 0.2273        | 47.4  | 948  | 0.7331          | 0.0     | 0.7331 | 0.8562 |
| 0.2273        | 47.5  | 950  | 0.7319          | 0.0     | 0.7319 | 0.8555 |
| 0.2273        | 47.6  | 952  | 0.7374          | 0.0     | 0.7374 | 0.8587 |
| 0.2273        | 47.7  | 954  | 0.7549          | 0.0     | 0.7549 | 0.8688 |
| 0.2273        | 47.8  | 956  | 0.7684          | 0.0     | 0.7684 | 0.8766 |
| 0.2273        | 47.9  | 958  | 0.7658          | 0.0     | 0.7658 | 0.8751 |
| 0.2273        | 48.0  | 960  | 0.7522          | 0.0     | 0.7522 | 0.8673 |
| 0.2273        | 48.1  | 962  | 0.7399          | 0.0     | 0.7399 | 0.8602 |
| 0.2273        | 48.2  | 964  | 0.7239          | 0.0     | 0.7239 | 0.8508 |
| 0.2273        | 48.3  | 966  | 0.7250          | 0.0     | 0.7250 | 0.8515 |
| 0.2273        | 48.4  | 968  | 0.7289          | 0.0     | 0.7289 | 0.8537 |
| 0.2273        | 48.5  | 970  | 0.7316          | 0.0     | 0.7316 | 0.8553 |
| 0.2273        | 48.6  | 972  | 0.7390          | -0.0541 | 0.7390 | 0.8596 |
| 0.2273        | 48.7  | 974  | 0.7628          | -0.0560 | 0.7628 | 0.8734 |
| 0.2273        | 48.8  | 976  | 0.8115          | 0.0732  | 0.8115 | 0.9008 |
| 0.2273        | 48.9  | 978  | 0.8281          | 0.0226  | 0.8281 | 0.9100 |
| 0.2273        | 49.0  | 980  | 0.8387          | 0.0679  | 0.8387 | 0.9158 |
| 0.2273        | 49.1  | 982  | 0.8199          | 0.0798  | 0.8199 | 0.9055 |
| 0.2273        | 49.2  | 984  | 0.8090          | 0.0     | 0.8090 | 0.8994 |
| 0.2273        | 49.3  | 986  | 0.7821          | -0.0029 | 0.7821 | 0.8844 |
| 0.2273        | 49.4  | 988  | 0.7622          | -0.0118 | 0.7622 | 0.8731 |
| 0.2273        | 49.5  | 990  | 0.7194          | 0.0436  | 0.7194 | 0.8482 |
| 0.2273        | 49.6  | 992  | 0.6955          | 0.0524  | 0.6955 | 0.8340 |
| 0.2273        | 49.7  | 994  | 0.6850          | 0.0     | 0.6850 | 0.8277 |
| 0.2273        | 49.8  | 996  | 0.6791          | 0.0     | 0.6791 | 0.8241 |
| 0.2273        | 49.9  | 998  | 0.6778          | 0.0     | 0.6778 | 0.8233 |
| 0.0536        | 50.0  | 1000 | 0.6773          | 0.0     | 0.6773 | 0.8230 |
| 0.0536        | 50.1  | 1002 | 0.6849          | 0.0     | 0.6849 | 0.8276 |
| 0.0536        | 50.2  | 1004 | 0.6879          | 0.0     | 0.6879 | 0.8294 |
| 0.0536        | 50.3  | 1006 | 0.6799          | 0.0     | 0.6799 | 0.8245 |
| 0.0536        | 50.4  | 1008 | 0.6719          | 0.0     | 0.6719 | 0.8197 |
| 0.0536        | 50.5  | 1010 | 0.6772          | 0.0436  | 0.6772 | 0.8229 |
| 0.0536        | 50.6  | 1012 | 0.7073          | 0.0814  | 0.7073 | 0.8410 |
| 0.0536        | 50.7  | 1014 | 0.7522          | 0.1506  | 0.7522 | 0.8673 |
| 0.0536        | 50.8  | 1016 | 0.7917          | 0.1395  | 0.7917 | 0.8898 |
| 0.0536        | 50.9  | 1018 | 0.8000          | 0.1506  | 0.8000 | 0.8944 |
| 0.0536        | 51.0  | 1020 | 0.7612          | -0.0628 | 0.7612 | 0.8725 |
| 0.0536        | 51.1  | 1022 | 0.7378          | -0.0560 | 0.7378 | 0.8590 |
| 0.0536        | 51.2  | 1024 | 0.7265          | -0.1074 | 0.7265 | 0.8524 |
| 0.0536        | 51.3  | 1026 | 0.7202          | -0.1074 | 0.7202 | 0.8486 |
| 0.0536        | 51.4  | 1028 | 0.7121          | -0.1074 | 0.7121 | 0.8438 |


### Framework versions

- Transformers 4.44.2
- Pytorch 2.4.0+cu118
- Datasets 2.21.0
- Tokenizers 0.19.1",41896.0,2.7456152235472852e-06
E-katrin/train20_encoder_freezed_10ep_1e-5,683db465cfcb1837296303e1,E-katrin,"{""base_model"": ""xlm-roberta-base"", ""datasets"": ""E-katrin/train20"", ""language"": ""sv"", ""library_name"": ""transformers"", ""license"": ""gpl-3.0"", ""metrics"": [""accuracy"", ""f1""], ""pipeline_tag"": ""token-classification"", ""tags"": [""pytorch""], ""model-index"": [{""name"": ""E-katrin/train20_encoder_freezed_10ep_1e-5"", ""results"": [{""task"": {""type"": ""token-classification""}, ""dataset"": {""name"": ""train20"", ""type"": ""E-katrin/train20"", ""split"": ""validation""}, ""metrics"": [{""type"": ""f1"", ""value"": 0.7483831851253031, ""name"": ""Null F1"", ""verified"": false}, {""type"": ""f1"", ""value"": 0.013643256925648954, ""name"": ""Lemma F1"", ""verified"": false}, {""type"": ""f1"", ""value"": 0.04766304993570397, ""name"": ""Morphology F1"", ""verified"": false}, {""type"": ""accuracy"", ""value"": 0.5774121166791324, ""name"": ""Ud Jaccard"", ""verified"": false}, {""type"": ""accuracy"", ""value"": 0.4030037546933667, ""name"": ""Eud Jaccard"", ""verified"": false}, {""type"": ""f1"", ""value"": 0.7461145129726658, ""name"": ""Miscs F1"", ""verified"": false}, {""type"": ""f1"", ""value"": 0.46366651665566627, ""name"": ""Deepslot F1"", ""verified"": false}, {""type"": ""f1"", ""value"": 0.3545291300389656, ""name"": ""Semclass F1"", ""verified"": false}]}]}]}",[],2025-06-02 14:32:35,0,0.0,"{""architectures"": [""CobaldParser""], ""auto_map"": {""AutoConfig"": ""configuration.CobaldParserConfig"", ""AutoModel"": ""modeling_parser.CobaldParser""}, ""model_type"": ""cobald_parser""}",1,57,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 291169131, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 291169131.0}","['transformers' 'tensorboard' 'safetensors' 'cobald_parser'
 'feature-extraction' 'pytorch' 'token-classification' 'custom_code' 'sv'
 'dataset:E-katrin/train20' 'base_model:FacebookAI/xlm-roberta-base'
 'base_model:finetune:FacebookAI/xlm-roberta-base' 'license:gpl-3.0'
 'model-index' 'region:us']",token-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'configuration.py'}
 {'rfilename': 'dependency_classifier.py'} {'rfilename': 'encoder.py'}
 {'rfilename': 'mlp_classifier.py'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'modeling_parser.py'} {'rfilename': 'pipeline.py'}
 {'rfilename': 'runs/Jun02_13-32-02_117faea3d0b4/events.out.tfevents.1748871313.117faea3d0b4.1777.0'}
 {'rfilename': 'runs/Jun02_13-55-07_117faea3d0b4/events.out.tfevents.1748872568.117faea3d0b4.7773.0'}
 {'rfilename': 'runs/Jun02_13-58-32_117faea3d0b4/events.out.tfevents.1748872767.117faea3d0b4.8699.0'}
 {'rfilename': 'runs/Jun02_14-06-40_117faea3d0b4/events.out.tfevents.1748873257.117faea3d0b4.10819.0'}
 {'rfilename': 'runs/Jun02_14-18-09_117faea3d0b4/events.out.tfevents.1748873947.117faea3d0b4.13811.0'}
 {'rfilename': 'runs/Jun02_14-21-39_117faea3d0b4/events.out.tfevents.1748874155.117faea3d0b4.14771.0'}
 {'rfilename': 'runs/Jun02_14-24-47_117faea3d0b4/events.out.tfevents.1748874342.117faea3d0b4.15649.0'}
 {'rfilename': 'training_args.bin'} {'rfilename': 'utils.py'}]",2025-06-02 14:25:41,transformers,"{'auto_model': 'AutoModel', 'custom_class': 'modeling_parser.CobaldParser', 'pipeline_tag': 'feature-extraction', 'processor': None}",,finetune,621ffdc036468d709f174364,FacebookAI/xlm-roberta-base,E-katrin/train20_encoder_freezed_10ep_1e-5,"base_model: xlm-roberta-base
datasets: E-katrin/train20
language: sv
library_name: transformers
license: gpl-3.0
metrics:
- accuracy
- f1
pipeline_tag: token-classification
tags:
- pytorch
model-index:
- name: E-katrin/train20_encoder_freezed_10ep_1e-5
  results:
  - task:
      type: token-classification
    dataset:
      name: train20
      type: E-katrin/train20
      split: validation
    metrics:
    - type: f1
      value: 0.7483831851253031
      name: Null F1
    - type: f1
      value: 0.013643256925648954
      name: Lemma F1
    - type: f1
      value: 0.04766304993570397
      name: Morphology F1
    - type: accuracy
      value: 0.5774121166791324
      name: Ud Jaccard
    - type: accuracy
      value: 0.4030037546933667
      name: Eud Jaccard
    - type: f1
      value: 0.7461145129726658
      name: Miscs F1
    - type: f1
      value: 0.46366651665566627
      name: Deepslot F1
    - type: f1
      value: 0.3545291300389656
      name: Semclass F1","# Model Card for train20_encoder_freezed_10ep_1e-5

A transformer-based multihead parser for CoBaLD annotation.

This model parses a pre-tokenized CoNLL-U text and jointly labels each token with three tiers of tags:
* Grammatical tags (lemma, UPOS, XPOS, morphological features),
* Syntactic tags (basic and enhanced Universal Dependencies),
* Semantic tags (deep slot and semantic class).

## Model Sources

- **Repository:** https://github.com/CobaldAnnotation/CobaldParser
- **Paper:** https://dialogue-conf.org/wp-content/uploads/2025/04/BaiukIBaiukAPetrovaM.009.pdf
- **Demo:** [coming soon]

## Citation

```
@inproceedings{baiuk2025cobald,
  title={CoBaLD Parser: Joint Morphosyntactic and Semantic Annotation},
  author={Baiuk, Ilia and Baiuk, Alexandra and Petrova, Maria},
  booktitle={Proceedings of the International Conference ""Dialogue""},
  volume={I},
  year={2025}
}
```",886.0,2.722881275544602e-06
iRONic911/mistral-finetuned-samsum,659e29ea6dc6b399d2a5b31e,iRONic911,"{""license"": ""apache-2.0"", ""library_name"": ""peft"", ""tags"": [""generated_from_trainer""], ""base_model"": ""TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"", ""model-index"": [{""name"": ""mistral-finetuned-samsum"", ""results"": []}]}",[],2024-01-10 05:31:06,0,0.0,"{""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}, ""peft"": {""base_model_name_or_path"": ""TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"", ""task_type"": ""CAUSAL_LM""}}",0,55,,"['peft' 'safetensors' 'generated_from_trainer'
 'base_model:TheBloke/Mistral-7B-Instruct-v0.1-GPTQ'
 'base_model:adapter:TheBloke/Mistral-7B-Instruct-v0.1-GPTQ'
 'license:apache-2.0' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'}
 {'rfilename': 'adapter_model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2024-01-10 05:23:54,peft,,,adapter,6515ff5b7f18cec973af0828,TheBloke/Mistral-7B-Instruct-v0.1-GPTQ,iRONic911/mistral-finetuned-samsum,"license: apache-2.0
library_name: peft
tags:
- generated_from_trainer
base_model: TheBloke/Mistral-7B-Instruct-v0.1-GPTQ
model-index:
- name: mistral-finetuned-samsum
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# mistral-finetuned-samsum

This model is a fine-tuned version of [TheBloke/Mistral-7B-Instruct-v0.1-GPTQ](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0002
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: cosine
- training_steps: 250
- mixed_precision_training: Native AMP

### Training results



### Framework versions

- PEFT 0.7.1
- Transformers 4.37.0.dev0
- Pytorch 2.1.2+cu121
- Datasets 2.16.1
- Tokenizers 0.15.0",1026.0,2.6993494842104844e-06
jerryzh168/Qwen3-8B-INT8-INT4,68a4bee94503934664cae05d,jerryzh168,"{""base_model"": ""Qwen/Qwen3-8B"", ""tags"": [""transformers"", ""torchao"", ""qwen3""], ""license"": ""apache-2.0"", ""language"": [""en""]}",[],2025-09-09 19:56:48,0,0.0,"{""architectures"": [""Qwen3ForCausalLM""], ""model_type"": ""qwen3"", ""quantization_config"": {""quant_method"": ""torchao""}, ""tokenizer_config"": {""bos_token"": null, ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}, ""chat_template_jinja"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \""# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \""user\"" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '' %}\n    {%- endif %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in content %}\n                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\""name\"": \""' }}\n                {{- tool_call.name }}\n                {{- '\"", \""arguments\"": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}""}",16,52,,"['transformers' 'pytorch' 'qwen3' 'text-generation' 'torchao'
 'conversational' 'en' 'arxiv:2507.16099' 'base_model:Qwen/Qwen3-8B'
 'base_model:quantized:Qwen/Qwen3-8B' 'license:apache-2.0'
 'autotrain_compatible' 'text-generation-inference' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'chat_template.jinja'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'merges.txt'}
 {'rfilename': 'pytorch_model-00001-of-00002.bin'}
 {'rfilename': 'pytorch_model-00002-of-00002.bin'}
 {'rfilename': 'pytorch_model.bin.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2025-08-19 18:14:01,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,quantized,680da79d48c19d19cf82ab5e,Qwen/Qwen3-8B,jerryzh168/Qwen3-8B-INT8-INT4,"base_model: Qwen/Qwen3-8B
tags:
- transformers
- torchao
- qwen3
license: apache-2.0
language:
- en","# INT8-INT4 Qwen/Qwen3-8B model

- **Developed by:** jerryzh168
- **License:** apache-2.0
- **Quantized from Model :** Qwen/Qwen3-8B
- **Quantization Method :** INT8-INT4




# Running in a mobile app
(TODO: pte file name generation)
The [pte file](https://huggingface.co/jerryzh168/Qwen3-8B-INT8-INT4/blob/main/qwen3-4B-INT8-INT4-1024-cxt.pte) can be run with ExecuTorch on a mobile phone.  See the [instructions](https://pytorch.org/executorch/main/llm/llama-demo-ios.html) for doing this in iOS.
On iPhone 15 Pro, the model runs at (to be filled) tokens/sec and uses (to be filled) Mb of memory.

TODO: attach image


# Quantization Recipe

Install the required packages:
```Shell
pip install git+https://github.com/huggingface/transformers@main
pip install --pre torchao --index-url https://download.pytorch.org/whl/nightly/cu126
pip install torch
pip install accelerate
```


## Untie Embedding Weights
We want to quantize the embedding and lm_head differently.  Since those layers are tied, we first need to untie the model:

```Py
from transformers import (
  AutoModelForCausalLM,
  AutoProcessor,
  AutoTokenizer,
)
import torch

model_id = ""{base_model}""
untied_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=""auto"", device_map=""auto"")
tokenizer = AutoTokenizer.from_pretrained(model_id)

print(untied_model)
from transformers.modeling_utils import find_tied_parameters
print(""tied weights:"", find_tied_parameters(untied_model))
if getattr(untied_model.config.get_text_config(decoder=True), ""tie_word_embeddings""):
    setattr(untied_model.config.get_text_config(decoder=True), ""tie_word_embeddings"", False)

untied_model._tied_weights_keys = []
untied_model.lm_head.weight = torch.nn.Parameter(untied_model.lm_head.weight.clone())

print(""tied weights:"", find_tied_parameters(untied_model))

USER_ID = ""YOUR_USER_ID""
MODEL_NAME = model_id.split(""/"")[-1]
save_to = f""{{USER_ID}}/{{MODEL_NAME}}-untied-weights""

# save locally (we use this in the recipe)
save_to_local_path = f""{{MODEL_NAME}}-untied-weights""
untied_model.save_pretrained(save_to_local_path)
tokenizer.save_pretrained(save_to_local_path)


# or push to hub
untied_model.push_to_hub(save_to)
tokenizer.push_to_hub(save_to)
```

Note: to `push_to_hub` you need to run
```Shell
pip install -U ""huggingface_hub[cli]""
huggingface-cli login
```
and use a token with write access, from https://huggingface.co/settings/tokens

## Quantization


Use the following code to get the quantized model:
```Py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig

model_id = ""Qwen/Qwen3-8B""
model_to_quantize = ""f""{{MODEL_NAME}}-untied-weights""""


from torchao.quantization.quant_api import (
    IntxWeightOnlyConfig,
    Int8DynamicActivationIntxWeightConfig,
    ModuleFqnToConfig,
)
from torchao.quantization.granularity import PerGroup, PerAxis
embedding_config = IntxWeightOnlyConfig(
    weight_dtype=torch.int8,
    granularity=PerAxis(0),
)
linear_config = Int8DynamicActivationIntxWeightConfig(
    weight_dtype=torch.int4,
    weight_granularity=PerGroup(32),
    weight_scale_dtype=torch.bfloat16,
)
quant_config = ModuleFqnToConfig({{""_default"": linear_config, ""model.embed_tokens"": embedding_config}})
quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True, untie_embedding_weights=True, modules_to_not_convert=[])

quantized_model = AutoModelForCausalLM.from_pretrained(model_to_quantize, device_map=""auto"", torch_dtype=torch.bfloat16, quantization_config=quantization_config)
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Push to hub
USER_ID = ""YOUR_USER_ID""
MODEL_NAME = model_id.split(""/"")[-1]
save_to = f""{USER_ID}/{MODEL_NAME}-INT8-INT4""
quantized_model.push_to_hub(save_to, safe_serialization=False)
tokenizer.push_to_hub(save_to)

# Manual Testing
prompt = ""Hey, are you conscious? Can you talk to me?""
messages = [
    {
        ""role"": ""system"",
        ""content"": """",
    },
    {""role"": ""user"", ""content"": prompt},
]
templated_prompt = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)
print(""Prompt:"", prompt)
print(""Templated prompt:"", templated_prompt)
inputs = tokenizer(
    templated_prompt,
    return_tensors=""pt"",
).to(""cuda"")
generated_ids = quantized_model.generate(**inputs, max_new_tokens=128)
output_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(""Response:"", output_text[0][len(prompt):])
```

Note: to `push_to_hub` you need to run
```Shell
pip install -U ""huggingface_hub[cli]""
huggingface-cli login
```
and use a token with write access, from https://huggingface.co/settings/tokens

# Model Quality
We rely on [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) to evaluate the quality of the quantized model. Here we only run on mmlu for sanity check.

| Benchmark                        |                |                           |
|----------------------------------|----------------|---------------------------|
|                                  | Qwen/Qwen3-8B   | jerryzh168/Qwen3-8B-INT8-INT4         |
| mmlu                             | To be filled   | To be filled                      |


<details>
<summary> Reproduce Model Quality Results </summary>

Need to install lm-eval from source:
https://github.com/EleutherAI/lm-evaluation-harness#install

## baseline
```Shell
lm_eval --model hf --model_args pretrained=Qwen/Qwen3-8B --tasks mmlu --device cuda:0 --batch_size 8
```

## INT8-INT4
```Shell
export MODEL=jerryzh168/Qwen3-8B-INT8-INT4
lm_eval --model hf --model_args pretrained=$MODEL --tasks mmlu --device cuda:0 --batch_size 8
```
</details>









# Exporting to ExecuTorch

We can run the quantized model on a mobile phone using [ExecuTorch](https://github.com/pytorch/executorch).
Once ExecuTorch is [set-up](https://pytorch.org/executorch/main/getting-started.html), exporting and running the model on device is a breeze.

We first convert the [quantized checkpoint](https://huggingface.co/jerryzh168/Qwen3-8B-INT8-INT4/blob/main/pytorch_model.bin) to one ExecuTorch's LLM export script expects by renaming some of the checkpoint keys.
The following script does this for you.  We have uploaded the converted checkpoint [pytorch_model_converted.bin](https://huggingface.co/jerryzh168/Qwen3-8B-INT8-INT4/blob/main/pytorch_model_converted.bin) for convenience.
```Shell
python -m executorch.examples.models.qwen3.convert_weights $(huggingface-cli download jerryzh168/Qwen3-8B-INT8-INT4) pytorch_model_converted.bin
```

Once the checkpoint is converted, we can export to ExecuTorch's pte format with the XNNPACK delegate.
The below command exports with a max_seq_length/max_context_length of 1024, but it can be changed as desired.

(TODO: pte file name, model config path, model name auto generation)
```Shell
PARAMS=""executorch/examples/models/qwen3/4b_config.json""
python -m executorch.examples.models.llama.export_llama   --model ""qwen3-4b""   --checkpoint ""pytorch_model_converted.bin""   --params ""$PARAMS""   -kv   --use_sdpa_with_kv_cache   -d fp32
  -X   --metadata '{""get_bos_id"":199999, ""get_eos_ids"":[200020,199999]}'   --max_seq_length 1024   --max_context_length 1024   --output_name=""qwen3-4b-INT8-INT4-1024-cxt.pte""
```

After that you can run the model in a mobile app (see [Running in a mobile app](#running-in-a-mobile-app)).


# Paper: TorchAO: PyTorch-Native Training-to-Serving Model Optimization
The model's quantization is powered by **TorchAO**, a framework presented in the paper [TorchAO: PyTorch-Native Training-to-Serving Model Optimization](https://huggingface.co/papers/2507.16099).

**Abstract:** We present TorchAO, a PyTorch-native model optimization framework leveraging quantization and sparsity to provide an end-to-end, training-to-serving workflow for AI models. TorchAO supports a variety of popular model optimization techniques, including FP8 quantized training, quantization-aware training (QAT), post-training quantization (PTQ), and 2:4 sparsity, and leverages a novel tensor subclass abstraction to represent a variety of widely-used, backend agnostic low precision data types, including INT4, INT8, FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader ecosystem at each step of the model optimization pipeline, from pre-training (TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, vLLM, SGLang, ExecuTorch), connecting an otherwise fragmented space in a single, unified workflow. TorchAO has enabled recent launches of the quantized Llama 3.2 1B/3B and LlamaGuard3-8B models and is open-source at this https URL .

# Resources
*   **Official TorchAO GitHub Repository:** [https://github.com/pytorch/ao](https://github.com/pytorch/ao)
*   **TorchAO Documentation:** [https://docs.pytorch.org/ao/stable/index.html](https://docs.pytorch.org/ao/stable/index.html)


# Disclaimer
PyTorch has not performed safety evaluations or red teamed the quantized models. Performance characteristics, outputs, and behaviors may differ from the original models. Users are solely responsible for selecting appropriate use cases, evaluating and mitigating for accuracy, safety, and fairness, ensuring security, and complying with all applicable laws and regulations.

Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the licenses the models are released under, including any limitations of liability or disclaimers of warranties provided therein.",9542.0,2.66242710014108e-06
hyobi18220/jam_krx_qwen2.5_v7,6724ab643707c49ed35727d9,hyobi18220,"{""language"": [""en"", ""ko""], ""base_model"": [""unsloth/Qwen2.5-7B-Instruct""], ""tags"": [""krx""]}",[],2024-11-01 10:41:01,0,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) or (message.role == \""assistant\"" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|PAD_TOKEN|>"", ""unk_token"": null}}",1,52,"{'parameters': {'BF16': 7615616512.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 7615616512.0}","['safetensors' 'qwen2' 'krx' 'en' 'ko'
 'base_model:unsloth/Qwen2.5-7B-Instruct'
 'base_model:finetune:unsloth/Qwen2.5-7B-Instruct' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model-00001-of-00004.safetensors'}
 {'rfilename': 'model-00002-of-00004.safetensors'}
 {'rfilename': 'model-00003-of-00004.safetensors'}
 {'rfilename': 'model-00004-of-00004.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2024-11-01 10:20:20,,,,finetune,66eb44b9561b0d0436c6571b,unsloth/Qwen2.5-7B-Instruct,hyobi18220/jam_krx_qwen2.5_v7,,"---
language:
- en
- ko
base_model:
- unsloth/Qwen2.5-7B-Instruct
tags:
- krx
---",81.0,2.66242710014108e-06
dacson/s1-20250220_020221,67b68d68d4251be0aa5edb20,dacson,"{""base_model"": ""Qwen/Qwen2.5-0.5B-Instruct"", ""library_name"": ""transformers"", ""model_name"": ""s1-20250220_020221"", ""tags"": [""generated_from_trainer"", ""trl"", ""sft""], ""licence"": ""license""}","[{'_id': '6885bdcb12825c3fe623aaf8', 'adapter': None, 'adapterWeightsPath': None, 'features': None, 'isModelAuthor': False, 'provider': 'featherless-ai', 'providerDetails': None, 'providerId': 'dacson/s1-20250220_020221', 'status': 'live', 'task': 'conversational', 'type': 'single-model'}]",2025-02-20 02:27:57,0,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) or (message.role == \""assistant\"" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}}",0,47,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 494032768, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 494032768.0}","['transformers' 'safetensors' 'qwen2' 'text-generation'
 'generated_from_trainer' 'trl' 'sft' 'conversational'
 'base_model:Qwen/Qwen2.5-0.5B-Instruct'
 'base_model:finetune:Qwen/Qwen2.5-0.5B-Instruct' 'autotrain_compatible'
 'text-generation-inference' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.json'}]",2025-02-20 02:03:20,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,66e81c0ecfd2f50991047e92,Qwen/Qwen2.5-0.5B-Instruct,dacson/s1-20250220_020221,"base_model: Qwen/Qwen2.5-0.5B-Instruct
library_name: transformers
model_name: s1-20250220_020221
tags:
- generated_from_trainer
- trl
- sft
licence: license","# Model Card for s1-20250220_020221

This model is a fine-tuned version of [Qwen/Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct).
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""dacson/s1-20250220_020221"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/dcj216738-yy/s1/runs/7lavm2v0) 


This model was trained with SFT.

### Framework versions

- TRL: 0.15.0.dev0
- Transformers: 4.49.0.dev0
- Pytorch: 2.5.1
- Datasets: 3.2.0
- Tokenizers: 0.21.0

## Citations



Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```",1517.0,2.5959780051319914e-06
khaliloz/elaco,681156adda5ce4cbd7482abe,khaliloz,"{""base_model"": ""unsloth/llama-3-8b-bnb-4bit"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""llama"", ""trl"", ""sft""], ""license"": ""apache-2.0"", ""language"": [""en""]}",[],2025-04-29 22:55:01,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<|begin_of_text|>"", ""eos_token"": ""<|end_of_text|>"", ""pad_token"": ""<|reserved_special_token_250|>""}}",0,47,,"['transformers' 'pytorch' 'llama' 'text-generation'
 'text-generation-inference' 'unsloth' 'trl' 'sft' 'en'
 'base_model:unsloth/llama-3-8b-bnb-4bit'
 'base_model:finetune:unsloth/llama-3-8b-bnb-4bit' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'pytorch_model-00001-of-00004.bin'}
 {'rfilename': 'pytorch_model-00002-of-00004.bin'}
 {'rfilename': 'pytorch_model-00003-of-00004.bin'}
 {'rfilename': 'pytorch_model-00004-of-00004.bin'}
 {'rfilename': 'pytorch_model.bin.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}]",2025-04-29 22:46:05,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,66214ee7f3502429f9b50922,unsloth/llama-3-8b-bnb-4bit,khaliloz/elaco,"base_model: unsloth/llama-3-8b-bnb-4bit
tags:
- text-generation-inference
- transformers
- unsloth
- llama
- trl
- sft
license: apache-2.0
language:
- en","# Uploaded  model

- **Developed by:** khaliloz
- **License:** apache-2.0
- **Finetuned from model :** unsloth/llama-3-8b-bnb-4bit

This llama model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)",417.0,2.5959780051319914e-06
bbytxt/71d23b1a-d1ac-4ffa-90ed-840917a2614f,679317669d117a8afdfa094c,bbytxt,"{""library_name"": ""peft"", ""base_model"": ""fxmarty/tiny-llama-fast-tokenizer"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""71d23b1a-d1ac-4ffa-90ed-840917a2614f"", ""results"": []}]}",[],2025-01-24 04:31:14,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""peft"": {""base_model_name_or_path"": ""fxmarty/tiny-llama-fast-tokenizer"", ""task_type"": ""CAUSAL_LM""}}",1,47,,"['peft' 'safetensors' 'llama' 'axolotl' 'generated_from_trainer'
 'base_model:fxmarty/tiny-llama-fast-tokenizer'
 'base_model:adapter:fxmarty/tiny-llama-fast-tokenizer' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'last-checkpoint/README.md'}
 {'rfilename': 'last-checkpoint/adapter_config.json'}
 {'rfilename': 'last-checkpoint/adapter_model.safetensors'}
 {'rfilename': 'last-checkpoint/optimizer.pt'}
 {'rfilename': 'last-checkpoint/rng_state.pth'}
 {'rfilename': 'last-checkpoint/scheduler.pt'}
 {'rfilename': 'last-checkpoint/special_tokens_map.json'}
 {'rfilename': 'last-checkpoint/tokenizer.json'}
 {'rfilename': 'last-checkpoint/tokenizer.model'}
 {'rfilename': 'last-checkpoint/tokenizer_config.json'}
 {'rfilename': 'last-checkpoint/trainer_state.json'}
 {'rfilename': 'last-checkpoint/training_args.bin'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2025-01-24 04:30:30,peft,,,adapter,643cfac7e3fe9bb0fdee1fca,fxmarty/tiny-llama-fast-tokenizer,bbytxt/71d23b1a-d1ac-4ffa-90ed-840917a2614f,"library_name: peft
base_model: fxmarty/tiny-llama-fast-tokenizer
tags:
- axolotl
- generated_from_trainer
model-index:
- name: 71d23b1a-d1ac-4ffa-90ed-840917a2614f
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: fxmarty/tiny-llama-fast-tokenizer
bf16: true
chat_template: llama3
data_processes: 16
dataset_prepared_path: null
datasets:
- data_files:
  - b505cc6bbfc19103_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/b505cc6bbfc19103_train_data.json
  type:
    field_instruction: label_id
    field_output: instruct_text
    format: '{instruction}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
device_map: auto
do_eval: true
early_stopping_patience: 5
eval_batch_size: 4
eval_max_new_tokens: 128
eval_steps: 50
eval_table_size: null
evals_per_epoch: null
flash_attention: true
fp16: false
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 4
gradient_checkpointing: true
group_by_length: true
hub_model_id: bbytxt/71d23b1a-d1ac-4ffa-90ed-840917a2614f
hub_repo: null
hub_strategy: checkpoint
hub_token: null
learning_rate: 0.0001
load_in_4bit: false
load_in_8bit: false
local_rank: null
logging_steps: 1
lora_alpha: 128
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 64
lora_target_linear: true
lr_scheduler: cosine
max_grad_norm: 1.0
max_memory:
  0: 75GB
max_steps: 200
micro_batch_size: 8
mlflow_experiment_name: /tmp/b505cc6bbfc19103_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 3
optim_args:
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-5
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
save_steps: 50
saves_per_epoch: null
sequence_len: 1024
special_tokens:
  pad_token: </s>
strict: false
tf32: true
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: c1c489dc-d352-4e15-ba7a-e8b3887827a5
wandb_project: Gradients-On-Demand
wandb_run: your_name
wandb_runid: c1c489dc-d352-4e15-ba7a-e8b3887827a5
warmup_steps: 10
weight_decay: 0.0
xformers_attention: null

```

</details><br>

# 71d23b1a-d1ac-4ffa-90ed-840917a2614f

This model is a fine-tuned version of [fxmarty/tiny-llama-fast-tokenizer](https://huggingface.co/fxmarty/tiny-llama-fast-tokenizer) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 10.2782

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 8
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=adam_beta1=0.9,adam_beta2=0.95,adam_epsilon=1e-5
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- training_steps: 200

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 10.3762       | 0.0019 | 1    | 10.3786         |
| 10.2875       | 0.0952 | 50   | 10.2920         |
| 10.2824       | 0.1904 | 100  | 10.2838         |
| 10.2753       | 0.2856 | 150  | 10.2791         |
| 10.2744       | 0.3808 | 200  | 10.2782         |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3945.0,2.5959780051319914e-06
tuantmdev/d5f74582-c036-4979-94dd-035d3604ee5c,67ad70454b54f87d68d82483,tuantmdev,"{""library_name"": ""peft"", ""license"": ""apache-2.0"", ""base_model"": ""unsloth/codellama-7b"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""d5f74582-c036-4979-94dd-035d3604ee5c"", ""results"": []}]}",[],2025-02-13 05:47:53,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""peft"": {""base_model_name_or_path"": ""unsloth/codellama-7b"", ""task_type"": ""CAUSAL_LM""}}",1,45,,"['peft' 'safetensors' 'llama' 'axolotl' 'generated_from_trainer'
 'base_model:unsloth/codellama-7b'
 'base_model:adapter:unsloth/codellama-7b' 'license:apache-2.0'
 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'last-checkpoint/README.md'}
 {'rfilename': 'last-checkpoint/adapter_config.json'}
 {'rfilename': 'last-checkpoint/adapter_model.safetensors'}
 {'rfilename': 'last-checkpoint/optimizer.pt'}
 {'rfilename': 'last-checkpoint/rng_state.pth'}
 {'rfilename': 'last-checkpoint/scheduler.pt'}
 {'rfilename': 'last-checkpoint/special_tokens_map.json'}
 {'rfilename': 'last-checkpoint/tokenizer.json'}
 {'rfilename': 'last-checkpoint/tokenizer.model'}
 {'rfilename': 'last-checkpoint/tokenizer_config.json'}
 {'rfilename': 'last-checkpoint/trainer_state.json'}
 {'rfilename': 'last-checkpoint/training_args.bin'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2025-02-13 04:08:37,peft,,,adapter,65ba0d12e7c8381362909c94,unsloth/codellama-7b,tuantmdev/d5f74582-c036-4979-94dd-035d3604ee5c,"library_name: peft
license: apache-2.0
base_model: unsloth/codellama-7b
tags:
- axolotl
- generated_from_trainer
model-index:
- name: d5f74582-c036-4979-94dd-035d3604ee5c
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: unsloth/codellama-7b
bf16: auto
chat_template: llama3
dataset_prepared_path: null
datasets:
- data_files:
  - 5ce35b545f5e5359_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/5ce35b545f5e5359_train_data.json
  type:
    field_instruction: instruction
    field_output: responses
    format: '{instruction}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
eval_max_new_tokens: 128
eval_table_size: null
evals_per_epoch: 5
flash_attention: false
fp16: null
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 16
gradient_checkpointing: false
group_by_length: false
hub_model_id: tuantmdev/d5f74582-c036-4979-94dd-035d3604ee5c
hub_repo: null
hub_strategy: checkpoint
hub_token: null
learning_rate: 2e-05
load_in_4bit: false
load_in_8bit: false
local_rank: null
logging_steps: 40
lora_alpha: 16
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 8
lora_target_linear: true
lr_scheduler: cosine
max_steps: 200
micro_batch_size: 2
mlflow_experiment_name: /tmp/5ce35b545f5e5359_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 1
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
save_strategy: best
saves_per_epoch: 5
sequence_len: 512
strict: false
tf32: false
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: ce6ac142-858e-4ddb-a67e-747a04f0b32f
wandb_project: Gradients-On-Demand
wandb_run: unknown
wandb_runid: ce6ac142-858e-4ddb-a67e-747a04f0b32f
warmup_steps: 80
weight_decay: 0.01
xformers_attention: null

```

</details><br>

# d5f74582-c036-4979-94dd-035d3604ee5c

This model is a fine-tuned version of [unsloth/codellama-7b](https://huggingface.co/unsloth/codellama-7b) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: nan

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 16
- total_train_batch_size: 32
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 80
- training_steps: 200

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| No log        | 0.0001 | 1    | nan             |
| 0.0           | 0.0050 | 40   | nan             |
| 0.0           | 0.0099 | 80   | nan             |
| 0.0           | 0.0149 | 120  | nan             |
| 0.0           | 0.0199 | 160  | nan             |
| 0.0           | 0.0248 | 200  | nan             |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3684.0,2.567438070722287e-06
chansung/Qwen2.5-1.5B-CCRL-CUR-EDGE-ONLY-1E,6846e1b40f24b2c18a6c757a,chansung,"{""base_model"": ""Qwen/Qwen2.5-1.5B-Instruct"", ""datasets"": ""chansung/verifiable-coding-problems-python-v2"", ""library_name"": ""transformers"", ""model_name"": ""Qwen2.5-1.5B-CCRL-CUR-EDGE-ONLY-1E"", ""tags"": [""generated_from_trainer"", ""open-r1"", ""trl"", ""grpo""], ""licence"": ""license""}",[],2025-06-09 19:47:23,0,0.0,"{""architectures"": [""Qwen2ForCausalLM""], ""model_type"": ""qwen2"", ""tokenizer_config"": {""bos_token"": null, ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": null}, ""chat_template_jinja"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \""\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) or (message.role == \""assistant\"" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\""name\"": \""' }}\n            {{- tool_call.name }}\n            {{- '\"", \""arguments\"": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n""}",0,39,"{'parameters': {'BF16': 1777088000.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 1777088000.0}","['transformers' 'safetensors' 'qwen2' 'text-generation'
 'generated_from_trainer' 'open-r1' 'trl' 'grpo' 'conversational'
 'dataset:chansung/verifiable-coding-problems-python-v2'
 'arxiv:2402.03300' 'base_model:Qwen/Qwen2.5-1.5B-Instruct'
 'base_model:finetune:Qwen/Qwen2.5-1.5B-Instruct' 'autotrain_compatible'
 'text-generation-inference' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'all_results.json'}
 {'rfilename': 'chat_template.jinja'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'train_results.json'} {'rfilename': 'trainer_state.json'}
 {'rfilename': 'training_args.bin'} {'rfilename': 'vocab.json'}]",2025-06-09 13:29:24,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,66e98dd5899bdb384bd953b4,Qwen/Qwen2.5-1.5B-Instruct,chansung/Qwen2.5-1.5B-CCRL-CUR-EDGE-ONLY-1E,"base_model: Qwen/Qwen2.5-1.5B-Instruct
datasets: chansung/verifiable-coding-problems-python-v2
library_name: transformers
model_name: Qwen2.5-1.5B-CCRL-CUR-EDGE-ONLY-1E
tags:
- generated_from_trainer
- open-r1
- trl
- grpo
licence: license","# Model Card for Qwen2.5-1.5B-CCRL-CUR-EDGE-ONLY-1E

This model is a fine-tuned version of [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) on the [chansung/verifiable-coding-problems-python-v2](https://huggingface.co/datasets/chansung/verifiable-coding-problems-python-v2) dataset.
It has been trained using [TRL](https://github.com/huggingface/trl).

## Quick start

```python
from transformers import pipeline

question = ""If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?""
generator = pipeline(""text-generation"", model=""chansung/Qwen2.5-1.5B-CCRL-CUR-EDGE-ONLY-1E"", device=""cuda"")
output = generator([{""role"": ""user"", ""content"": question}], max_new_tokens=128, return_full_text=False)[0]
print(output[""generated_text""])
```

## Training procedure

[<img src=""https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg"" alt=""Visualize in Weights & Biases"" width=""150"" height=""24""/>](https://wandb.ai/chansung18/huggingface/runs/dklozatq) 


This model was trained with GRPO, a method introduced in [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://huggingface.co/papers/2402.03300).

### Framework versions

- TRL: 0.18.0.dev0
- Transformers: 4.52.0.dev0
- Pytorch: 2.6.0
- Datasets: 3.6.0
- Tokenizers: 0.21.1

## Citations

Cite GRPO as:

```bibtex
@article{zhihong2024deepseekmath,
    title        = {{DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}},
    author       = {Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
    year         = 2024,
    eprint       = {arXiv:2402.03300},
}

```

Cite TRL as:
    
```bibtex
@misc{vonwerra2022trl,
	title        = {{TRL: Transformer Reinforcement Learning}},
	author       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallou{\'e}dec},
	year         = 2020,
	journal      = {GitHub repository},
	publisher    = {GitHub},
	howpublished = {\url{https://github.com/huggingface/trl}}
}
```",2243.0,2.4737154953927905e-06
gates04/NF4-KD-SD-NID,67e49053765e3105effe7971,gates04,"{""library_name"": ""transformers"", ""base_model"": ""gates04/FP16-KD-NID"", ""tags"": [""generated_from_trainer""], ""metrics"": [""accuracy"", ""precision"", ""recall""], ""model-index"": [{""name"": ""NF4-KD-SD-NID"", ""results"": []}]}",[],2025-03-26 23:40:33,0,0.0,"{""architectures"": [""DistilBertForSequenceClassification""], ""model_type"": ""distilbert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",0,38,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 19037136, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 19037136.0}","['transformers' 'safetensors' 'distilbert' 'text-classification'
 'generated_from_trainer' 'base_model:gates04/FP16-KD-NID'
 'base_model:finetune:gates04/FP16-KD-NID' 'autotrain_compatible'
 'endpoints_compatible' 'region:us']",text-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2025-03-26 23:40:03,transformers,"{'auto_model': 'AutoModelForSequenceClassification', 'custom_class': None, 'pipeline_tag': 'text-classification', 'processor': 'AutoTokenizer'}",,finetune,67decf393029003db33fd2d0,gates04/FP16-KD-NID,gates04/NF4-KD-SD-NID,"library_name: transformers
base_model: gates04/FP16-KD-NID
tags:
- generated_from_trainer
metrics:
- accuracy
- precision
- recall
model-index:
- name: NF4-KD-SD-NID
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# NF4-KD-SD-NID

This model is a fine-tuned version of [gates04/FP16-KD-NID](https://huggingface.co/gates04/FP16-KD-NID) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0161
- Accuracy: 0.9923
- Precision: 0.9672
- Recall: 0.9496
- F1 score: 0.9554

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 500
- eval_batch_size: 500
- seed: 42
- optimizer: Use OptimizerNames.ADAMW_TORCH with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: linear
- num_epochs: 10
- mixed_precision_training: Native AMP

### Training results

| Training Loss | Epoch | Step  | Validation Loss | Accuracy | Precision | Recall | F1 score |
|:-------------:|:-----:|:-----:|:---------------:|:--------:|:---------:|:------:|:--------:|
| 0.0234        | 1.0   | 2376  | 0.0221          | 0.9909   | 0.9307    | 0.9111 | 0.9138   |
| 0.0243        | 2.0   | 4752  | 0.0201          | 0.9910   | 0.9751    | 0.9196 | 0.9268   |
| 0.0199        | 3.0   | 7128  | 0.0217          | 0.9909   | 0.9315    | 0.9148 | 0.9175   |
| 0.0181        | 4.0   | 9504  | 0.0189          | 0.9915   | 0.9331    | 0.9202 | 0.9233   |
| 0.0183        | 5.0   | 11880 | 0.0184          | 0.9917   | 0.9374    | 0.9220 | 0.9259   |
| 0.0194        | 6.0   | 14256 | 0.0175          | 0.9918   | 0.9545    | 0.9321 | 0.9368   |
| 0.0129        | 7.0   | 16632 | 0.0169          | 0.9921   | 0.9596    | 0.9353 | 0.9418   |
| 0.016         | 8.0   | 19008 | 0.0168          | 0.9921   | 0.9666    | 0.9539 | 0.9581   |
| 0.0142        | 9.0   | 21384 | 0.0162          | 0.9922   | 0.9673    | 0.9561 | 0.9600   |
| 0.013         | 10.0  | 23760 | 0.0161          | 0.9923   | 0.9672    | 0.9496 | 0.9554   |


### Framework versions

- Transformers 4.50.0
- Pytorch 2.6.0+cu124
- Datasets 3.4.1
- Tokenizers 0.21.1",2324.0,2.4567376963892824e-06
smirki/uigen-t3-preview-unreason,684b863d4a18b8ef696b9e3c,smirki,"{""base_model"": ""unsloth/Qwen3-4B-unsloth-bnb-4bit"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""qwen3""], ""license"": ""apache-2.0"", ""language"": [""en""]}",[],2025-06-13 02:14:08,0,0.0,"{""architectures"": [""Qwen3ForCausalLM""], ""model_type"": ""qwen3"", ""tokenizer_config"": {""bos_token"": null, ""chat_template"": ""{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0].role == 'system' %}\n        {{- messages[0].content + '\\n\\n' }}\n    {%- endif %}\n    {{- \""# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\"" }}\n    {%- for tool in tools %}\n        {{- \""\\n\"" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \""\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\""name\\\"": <function-name>, \\\""arguments\\\"": <args-json-object>}\\n</tool_call><|im_end|>\\n\"" }}\n{%- else %}\n    {%- if messages[0].role == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for forward_message in messages %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- set message = messages[index] %}\n    {%- set tool_start = '<tool_response>' %}\n    {%- set tool_start_length = tool_start|length %}\n    {%- set start_of_message = message.content[:tool_start_length] %}\n    {%- set tool_end = '</tool_response>' %}\n    {%- set tool_end_length = tool_end|length %}\n    {%- set start_pos = (message.content|length) - tool_end_length %}\n    {%- if start_pos < 0 %}\n        {%- set start_pos = 0 %}\n    {%- endif %}\n    {%- set end_of_message = message.content[start_pos:] %}\n    {%- if ns.multi_step_tool and message.role == \""user\"" and not(start_of_message == tool_start and end_of_message == tool_end) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \""user\"") or (message.role == \""system\"" and not loop.first) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \""assistant\"" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if '</think>' in message.content %}\n                {%- set content = (message.content.split('</think>')|last).lstrip('\\n') %}\n                {%- set reasoning_content = (message.content.split('</think>')|first).rstrip('\\n') %}\n                {%- set reasoning_content = (reasoning_content.split('<think>')|last).lstrip('\\n') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n            {%- else %}\n                {{- '<|im_start|>' + message.role + '\\n' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- '<|im_start|>' + message.role + '\\n' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- '\\n' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- '<tool_call>\\n{\""name\"": \""' }}\n                {{- tool_call.name }}\n                {{- '\"", \""arguments\"": ' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- '}\\n</tool_call>' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \""tool\"" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \""tool\"") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \""tool\"") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- '<think>\\n\\n</think>\\n\\n' }}\n    {%- endif %}\n{%- endif %}"", ""eos_token"": ""<|im_end|>"", ""pad_token"": ""<|vision_pad|>"", ""unk_token"": null}}",0,37,"{'parameters': {'BF16': 4022468096.0, 'BOOL': None, 'F16': None, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 4022468096.0}","['transformers' 'safetensors' 'qwen3' 'text-generation'
 'text-generation-inference' 'unsloth' 'conversational' 'en'
 'base_model:unsloth/Qwen3-4B-unsloth-bnb-4bit'
 'base_model:finetune:unsloth/Qwen3-4B-unsloth-bnb-4bit'
 'license:apache-2.0' 'autotrain_compatible' 'endpoints_compatible'
 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model-00001-of-00002.safetensors'}
 {'rfilename': 'model-00002-of-00002.safetensors'}
 {'rfilename': 'model.safetensors.index.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2025-06-13 02:00:29,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,680f35cad0d514746168a1bf,unsloth/Qwen3-4B-unsloth-bnb-4bit,smirki/uigen-t3-preview-unreason,"base_model: unsloth/Qwen3-4B-unsloth-bnb-4bit
tags:
- text-generation-inference
- transformers
- unsloth
- qwen3
license: apache-2.0
language:
- en","# Uploaded finetuned  model

- **Developed by:** smirki
- **License:** apache-2.0
- **Finetuned from model :** unsloth/Qwen3-4B-unsloth-bnb-4bit

This qwen3 model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)",431.0,2.4393188666293537e-06
rohitnagareddy/AdbhutMOE-Coding-Finetuned,684a738814e13be2c24b3f02,rohitnagareddy,"{""license"": ""mit"", ""language"": ""en"", ""tags"": [""mixture-of-experts"", ""moe"", ""coding"", ""code-generation"", ""fine-tuned"", ""lora"", ""instruction"", ""python"", ""adbhutmoe""], ""datasets"": [""TokenBender/code_instructions_122k_alpaca_style""], ""model_type"": ""mixtral"", ""base_model"": ""rohitnagareddy/AdbhutMOE""}",[],2025-06-12 06:28:28,0,0.0,"{""architectures"": [""MixtralForCausalLM""], ""model_type"": ""mixtral"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""eos_token"": ""</s>"", ""mask_token"": ""<mask>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>""}}",0,34,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': 15689984, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 15689984.0}","['safetensors' 'mixtral' 'mixture-of-experts' 'moe' 'coding'
 'code-generation' 'fine-tuned' 'lora' 'instruction' 'python' 'adbhutmoe'
 'en' 'dataset:TokenBender/code_instructions_122k_alpaca_style'
 'base_model:rohitnagareddy/AdbhutMOE'
 'base_model:adapter:rohitnagareddy/AdbhutMOE' 'license:mit' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'generation_config.json'} {'rfilename': 'merges.txt'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'vocab.json'}]",2025-06-12 06:28:24,,,,adapter,684a6c3588491ca31f8cbe4e,rohitnagareddy/AdbhutMOE,rohitnagareddy/AdbhutMOE-Coding-Finetuned,"license: mit
language: en
tags:
- mixture-of-experts
- moe
- coding
- code-generation
- fine-tuned
- lora
- instruction
- python
- adbhutmoe
datasets:
- TokenBender/code_instructions_122k_alpaca_style
model_type: mixtral
base_model: rohitnagareddy/AdbhutMOE","# AdbhutMOE-Coding-Finetuned - Fine-tuned Coding Assistant

This model is a fine-tuned version of the `rohitnagareddy/AdbhutMOE` Mixture-of-Experts (MoE) model, specialized for Python code generation and programming assistance tasks. It combines the efficiency of sparse MoE architecture with domain-specific fine-tuning for coding applications.

## 💻 Model Description

- **Base Model**: `rohitnagareddy/AdbhutMOE` (Custom MoE Architecture)
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Dataset**: `TokenBender/code_instructions_122k_alpaca_style` - A comprehensive dataset of coding instructions and solutions
- **Architecture**: Mixture-of-Experts with selective expert activation
- **Training**: Optimized for instruction-based code generation with memory-efficient techniques

## 🏗️ Architecture Details

This model is based on a custom Mixture-of-Experts architecture:
- **Experts per Layer**: 8 experts with 2 activated per token
- **Hidden Dimension**: 256
- **Attention Heads**: 4
- **Layers**: 4
- **Vocabulary**: Custom-trained tokenizer (~8K tokens)
- **Max Sequence Length**: 512 tokens

## ⚠️ Important Considerations

- **Verify All Code**: Generated code may contain errors or be suboptimal. Always test and review thoroughly.
- **Security**: Generated code has not been vetted for security vulnerabilities.
- **Educational Model**: This is a proof-of-concept model demonstrating MoE fine-tuning techniques.
- **Limited Training**: Model was trained with limited resources for demonstration purposes.

## 🚀 Usage

### Basic Text Generation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

model_id = ""rohitnagareddy/AdbhutMOE-Coding-Finetuned""

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map=""auto"",
    trust_remote_code=True
)

# Create a text generation pipeline
pipe = pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer
)

# Generate code
prompt = '''### Instruction:
Write a Python function that takes a list of integers and returns the sum of all even numbers in the list.

### Response:'''

response = pipe(prompt, max_new_tokens=150, temperature=0.2, do_sample=True)
print(response[0][""generated_text""])
```

### Direct Model Usage

```python
# For more control over generation
prompt = '''### Instruction:
Create a Python class for a simple calculator with basic arithmetic operations.

### Response:'''

inputs = tokenizer(prompt, return_tensors=""pt"")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.3,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id
    )

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

## 📊 Training Details

### Fine-tuning Configuration
- **Training Steps**: 500 (limited for demonstration)
- **Batch Size**: 1 (with 8 gradient accumulation steps)
- **Learning Rate**: 1e-4
- **Optimizer**: Paged AdamW 8-bit
- **LoRA Rank**: 8
- **LoRA Alpha**: 16
- **Target Modules**: All linear layers including MoE experts and gates

### Base Model Training
- **Pre-training Data**: AG News dataset sample
- **Architecture**: Custom Mixtral-based MoE
- **Training Steps**: 100 (base model pre-training)

## 🎯 Performance Notes

- **Efficiency**: MoE architecture provides parameter efficiency while maintaining performance
- **Memory**: Optimized for memory-efficient inference and training
- **Speed**: Sparse activation patterns enable faster inference compared to dense models of similar capability

## 🔄 Model Lineage

1. **Base Architecture**: Custom Mixtral MoE implementation
2. **Pre-training**: Trained on AG News dataset sample
3. **Fine-tuning**: LoRA adaptation on coding instruction dataset
4. **Optimization**: 4-bit quantization support for efficient deployment

## 📈 Intended Use Cases

- **Code Generation**: Creating Python functions and classes
- **Programming Education**: Demonstrating coding concepts
- **Research**: Studying MoE architectures for domain-specific tasks
- **Prototyping**: Quick code snippet generation

## 🚫 Limitations

- **Limited Scope**: Primarily trained on basic coding tasks
- **Language Focus**: Optimized for Python, limited other language support
- **Scale**: Small model size limits complex reasoning capabilities
- **Training Data**: Limited training iterations due to resource constraints

## 🤝 Contributing

This model serves as a foundation for further experimentation with MoE architectures in code generation. Contributions and improvements are welcome!

---
*Fine-tuned by rohitnagareddy using LoRA on the AdbhutMOE architecture.*
*This model demonstrates the application of parameter-efficient fine-tuning to Mixture-of-Experts models.*",4935.0,2.384171047230279e-06
azncoffee/gptapp,6854e629501f458da32c9ef1,azncoffee,"{""datasets"": [""FreedomIntelligence/medical-o1-reasoning-SFT""], ""base_model"": [""deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B""]}",[],2025-06-20 05:04:02,0,0.0,"{""model_type"": ""qwen2""}",0,34,,"['gguf' 'qwen2' 'dataset:FreedomIntelligence/medical-o1-reasoning-SFT'
 'base_model:deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
 'base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'
 'endpoints_compatible' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'unsloth.Q8_0.gguf'}]",2025-06-20 04:40:09,,,"{""total"": 1777088000, ""architecture"": ""qwen2"", ""context_length"": 131072, ""bos_token"": ""<\uff5cbegin\u2581of\u2581sentence\uff5c>"", ""eos_token"": ""<\uff5cend\u2581of\u2581sentence\uff5c>""}",quantized,678e11922b39b4ed1381531b,deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,azncoffee/gptapp,,"---
datasets:
- FreedomIntelligence/medical-o1-reasoning-SFT
base_model:
- deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
---",120.0,2.384171047230279e-06
aleegis/54d6b89e-e0a4-4c26-9d5e-4ce691279925,681379bdb6dbda9fd9440da5,aleegis,"{""library_name"": ""peft"", ""license"": ""apache-2.0"", ""base_model"": ""unsloth/Mistral-Nemo-Base-2407"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""54d6b89e-e0a4-4c26-9d5e-4ce691279925"", ""results"": []}]}",[],2025-05-01 15:29:10,0,0.0,"{""architectures"": [""MistralForCausalLM""], ""model_type"": ""mistral"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>""}, ""peft"": {""base_model_name_or_path"": ""unsloth/Mistral-Nemo-Base-2407"", ""task_type"": ""CAUSAL_LM""}}",1,26,,"['peft' 'safetensors' 'mistral' 'axolotl' 'generated_from_trainer'
 'base_model:unsloth/Mistral-Nemo-Base-2407'
 'base_model:adapter:unsloth/Mistral-Nemo-Base-2407' 'license:apache-2.0'
 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'last-checkpoint/README.md'}
 {'rfilename': 'last-checkpoint/adapter_config.json'}
 {'rfilename': 'last-checkpoint/adapter_model.safetensors'}
 {'rfilename': 'last-checkpoint/optimizer.pt'}
 {'rfilename': 'last-checkpoint/rng_state.pth'}
 {'rfilename': 'last-checkpoint/scheduler.pt'}
 {'rfilename': 'last-checkpoint/special_tokens_map.json'}
 {'rfilename': 'last-checkpoint/tokenizer.json'}
 {'rfilename': 'last-checkpoint/tokenizer_config.json'}
 {'rfilename': 'last-checkpoint/trainer_state.json'}
 {'rfilename': 'last-checkpoint/training_args.bin'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}]",2025-05-01 13:40:13,peft,,,adapter,66994145b364654e0aff964e,unsloth/Mistral-Nemo-Base-2407,aleegis/54d6b89e-e0a4-4c26-9d5e-4ce691279925,"library_name: peft
license: apache-2.0
base_model: unsloth/Mistral-Nemo-Base-2407
tags:
- axolotl
- generated_from_trainer
model-index:
- name: 54d6b89e-e0a4-4c26-9d5e-4ce691279925
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: unsloth/Mistral-Nemo-Base-2407
bf16: auto
chat_template: llama3
dataloader_num_workers: 12
dataset_prepared_path: null
datasets:
- data_files:
  - 63a491480b93f510_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/63a491480b93f510_train_data.json
  type:
    field_instruction: prompt
    field_output: best_response
    format: '{instruction}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
early_stopping_patience: null
eval_max_new_tokens: 128
eval_steps: null
eval_table_size: null
evals_per_epoch: null
flash_attention: true
fp16: null
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 8
gradient_checkpointing: false
group_by_length: false
hub_model_id: aleegis/54d6b89e-e0a4-4c26-9d5e-4ce691279925
hub_repo: null
hub_strategy: checkpoint
hub_token: null
learning_rate: 0.0001
load_in_4bit: false
load_in_8bit: false
local_rank: null
logging_steps: null
lora_alpha: 32
lora_dropout: 0.15
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 32
lora_target_linear: true
loraplus_lr_embedding: 1.0e-06
loraplus_lr_ratio: 16
lr_scheduler: cosine
max_grad_norm: 1
max_steps: 1500
micro_batch_size: 2
mlflow_experiment_name: /tmp/63a491480b93f510_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 200
optimizer: adamw_torch_fused
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
save_steps: null
save_total_limit: 10
saves_per_epoch: 0
sequence_len: 1024
strict: false
tf32: true
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.0
wandb_entity: null
wandb_mode: online
wandb_name: 13712427-fb73-4e43-b93c-61d36776a27f
wandb_project: Gradients-On-Demand
wandb_run: your_name
wandb_runid: 13712427-fb73-4e43-b93c-61d36776a27f
warmup_steps: 100
weight_decay: 0
xformers_attention: null

```

</details><br>

# 54d6b89e-e0a4-4c26-9d5e-4ce691279925

This model is a fine-tuned version of [unsloth/Mistral-Nemo-Base-2407](https://huggingface.co/unsloth/Mistral-Nemo-Base-2407) on the None dataset.

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 8
- total_train_batch_size: 16
- optimizer: Use OptimizerNames.ADAMW_TORCH_FUSED with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 100
- training_steps: 1500

### Training results



### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3408.0,2.210146150649982e-06
nhung03/78ab526c-9619-4090-a189-af18f107df0c,678a2bbdfff1fac73fb849ab,nhung03,"{""library_name"": ""peft"", ""license"": ""apache-2.0"", ""base_model"": ""princeton-nlp/Sheared-LLaMA-1.3B"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""78ab526c-9619-4090-a189-af18f107df0c"", ""results"": []}]}",[],2025-01-17 10:14:23,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""quantization_config"": {""load_in_4bit"": false, ""load_in_8bit"": true, ""quant_method"": ""bitsandbytes""}, ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""peft"": {""base_model_name_or_path"": ""princeton-nlp/Sheared-LLaMA-1.3B"", ""task_type"": ""CAUSAL_LM""}}",1,23,,"['peft' 'safetensors' 'llama' 'axolotl' 'generated_from_trainer'
 'base_model:princeton-nlp/Sheared-LLaMA-1.3B'
 'base_model:adapter:princeton-nlp/Sheared-LLaMA-1.3B'
 'license:apache-2.0' '8-bit' 'bitsandbytes' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2025-01-17 10:06:53,peft,,,adapter,65256c2539fd3599e85a7e85,princeton-nlp/Sheared-LLaMA-1.3B,nhung03/78ab526c-9619-4090-a189-af18f107df0c,"library_name: peft
license: apache-2.0
base_model: princeton-nlp/Sheared-LLaMA-1.3B
tags:
- axolotl
- generated_from_trainer
model-index:
- name: 78ab526c-9619-4090-a189-af18f107df0c
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: princeton-nlp/Sheared-LLaMA-1.3B
bf16: auto
chat_template: llama3
dataset_prepared_path: null
datasets:
- data_files:
  - 530091b391599b12_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/530091b391599b12_train_data.json
  type:
    field_input: text_description
    field_instruction: text
    field_output: transcription_normalised
    format: '{instruction} {input}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
early_stopping_patience: null
eval_max_new_tokens: 128
eval_table_size: null
evals_per_epoch: 1
flash_attention: true
fp16: null
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_clipping: 1.0
group_by_length: false
hub_model_id: nhung03/78ab526c-9619-4090-a189-af18f107df0c
hub_repo: null
hub_strategy: end
hub_token: null
learning_rate: 5.0e-05
load_in_4bit: true
load_in_8bit: true
local_rank: null
logging_steps: 1
lora_alpha: 16
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 8
lora_target_linear: true
lr_scheduler: cosine
max_steps: 200
micro_batch_size: 2
mlflow_experiment_name: /tmp/530091b391599b12_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 1
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
saves_per_epoch: 1
sequence_len: 1024
special_tokens:
  pad_token: </s>
strict: false
tf32: false
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: 00ef1763-7057-492d-8131-0453cc7a0c9d
wandb_project: Gradients-On-Demand
wandb_run: your_name
wandb_runid: 00ef1763-7057-492d-8131-0453cc7a0c9d
warmup_steps: 5
weight_decay: 0.01
xformers_attention: true

```

</details><br>

# 78ab526c-9619-4090-a189-af18f107df0c

This model is a fine-tuned version of [princeton-nlp/Sheared-LLaMA-1.3B](https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0316

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 8
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 5
- training_steps: 200

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.0915        | 0.4317 | 200  | 0.0316          |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3566.0,2.131162349736492e-06
sergioalves/8ebfa88e-cb78-4e04-b43d-c8f7fa5c2d90,68157872fd054f792ce92ddd,sergioalves,"{""library_name"": ""peft"", ""license"": ""apache-2.0"", ""base_model"": ""princeton-nlp/Sheared-LLaMA-1.3B"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""8ebfa88e-cb78-4e04-b43d-c8f7fa5c2d90"", ""results"": []}]}",[],2025-05-03 02:07:33,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""quantization_config"": {""load_in_4bit"": false, ""load_in_8bit"": true, ""quant_method"": ""bitsandbytes""}, ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""</s>"", ""pad_token"": ""</s>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""peft"": {""base_model_name_or_path"": ""princeton-nlp/Sheared-LLaMA-1.3B"", ""task_type"": ""CAUSAL_LM""}}",1,23,,"['peft' 'safetensors' 'llama' 'axolotl' 'generated_from_trainer'
 'base_model:princeton-nlp/Sheared-LLaMA-1.3B'
 'base_model:adapter:princeton-nlp/Sheared-LLaMA-1.3B'
 'license:apache-2.0' '8-bit' 'bitsandbytes' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2025-05-03 01:59:14,peft,,,adapter,65256c2539fd3599e85a7e85,princeton-nlp/Sheared-LLaMA-1.3B,sergioalves/8ebfa88e-cb78-4e04-b43d-c8f7fa5c2d90,"library_name: peft
license: apache-2.0
base_model: princeton-nlp/Sheared-LLaMA-1.3B
tags:
- axolotl
- generated_from_trainer
model-index:
- name: 8ebfa88e-cb78-4e04-b43d-c8f7fa5c2d90
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
absolute_data_files: true
adapter: lora
base_model: princeton-nlp/Sheared-LLaMA-1.3B
bf16: true
chat_template: llama3
dataset_prepared_path: /workspace/axolotl
datasets:
- data_files:
  - 25dd0e0b52267afa_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/25dd0e0b52267afa_train_data.json
  type:
    field_input: function_description_en
    field_instruction: system_message_en
    field_output: system_message_vi
    format: '{instruction} {input}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
early_stopping_patience: null
eval_max_new_tokens: 128
eval_table_size: null
evals_per_epoch: 1
flash_attention: true
fp16: null
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 1
gradient_checkpointing: true
gradient_clipping: 0.5
group_by_length: false
hub_model_id: sergioalves/8ebfa88e-cb78-4e04-b43d-c8f7fa5c2d90
hub_repo: null
hub_strategy: end
hub_token: null
learning_rate: 5.0e-06
load_in_4bit: false
load_in_8bit: true
local_rank: null
logging_steps: 1
lora_alpha: 64
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 32
lora_target_linear: true
lr_scheduler: cosine
max_steps: 200
micro_batch_size: 8
mixed_precision: bf16
mlflow_experiment_name: /tmp/25dd0e0b52267afa_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 1
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
saves_per_epoch: 1
sequence_len: 1024
special_tokens:
  pad_token: </s>
strict: false
tf32: false
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: 776caaf0-ecf3-4ed0-b0f0-1e847cb24ae0
wandb_project: s56-8
wandb_run: your_name
wandb_runid: 776caaf0-ecf3-4ed0-b0f0-1e847cb24ae0
warmup_steps: 5
weight_decay: 0.01
xformers_attention: true

```

</details><br>

# 8ebfa88e-cb78-4e04-b43d-c8f7fa5c2d90

This model is a fine-tuned version of [princeton-nlp/Sheared-LLaMA-1.3B](https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0571

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-06
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 5
- training_steps: 200

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 0.0905        | 0.0150 | 200  | 0.0571          |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3572.0,2.131162349736492e-06
prxy5606/d78e8e2f-4d69-44b8-af8f-ddf28ac2fcde,678f12628c745551e0c21a2c,prxy5606,"{""library_name"": ""peft"", ""base_model"": ""trl-internal-testing/tiny-random-LlamaForCausalLM"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""d78e8e2f-4d69-44b8-af8f-ddf28ac2fcde"", ""results"": []}]}",[],2025-01-21 03:20:23,0,0.0,"{""architectures"": [""LlamaForCausalLM""], ""model_type"": ""llama"", ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""</s>"", ""pad_token"": ""<unk>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": true}, ""peft"": {""base_model_name_or_path"": ""trl-internal-testing/tiny-random-LlamaForCausalLM"", ""task_type"": ""CAUSAL_LM""}}",1,22,,"['peft' 'safetensors' 'llama' 'axolotl' 'generated_from_trainer'
 'base_model:trl-internal-testing/tiny-random-LlamaForCausalLM'
 'base_model:adapter:trl-internal-testing/tiny-random-LlamaForCausalLM'
 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'} {'rfilename': 'config.json'}
 {'rfilename': 'last-checkpoint/README.md'}
 {'rfilename': 'last-checkpoint/adapter_config.json'}
 {'rfilename': 'last-checkpoint/adapter_model.safetensors'}
 {'rfilename': 'last-checkpoint/optimizer.pt'}
 {'rfilename': 'last-checkpoint/rng_state.pth'}
 {'rfilename': 'last-checkpoint/scheduler.pt'}
 {'rfilename': 'last-checkpoint/special_tokens_map.json'}
 {'rfilename': 'last-checkpoint/tokenizer.json'}
 {'rfilename': 'last-checkpoint/tokenizer.model'}
 {'rfilename': 'last-checkpoint/tokenizer_config.json'}
 {'rfilename': 'last-checkpoint/trainer_state.json'}
 {'rfilename': 'last-checkpoint/training_args.bin'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2025-01-21 03:20:02,peft,,,adapter,6423e49180b5c89bfa771a2a,trl-internal-testing/tiny-random-LlamaForCausalLM,prxy5606/d78e8e2f-4d69-44b8-af8f-ddf28ac2fcde,"library_name: peft
base_model: trl-internal-testing/tiny-random-LlamaForCausalLM
tags:
- axolotl
- generated_from_trainer
model-index:
- name: d78e8e2f-4d69-44b8-af8f-ddf28ac2fcde
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: trl-internal-testing/tiny-random-LlamaForCausalLM
bf16: true
chat_template: llama3
data_processes: 16
dataset_prepared_path: null
datasets:
- data_files:
  - dd22c8863ed4176b_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/dd22c8863ed4176b_train_data.json
  type:
    field_input: text
    field_instruction: title
    field_output: summary
    format: '{instruction} {input}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
device_map: auto
do_eval: true
early_stopping_patience: 5
eval_batch_size: 4
eval_max_new_tokens: 128
eval_steps: 50
eval_table_size: null
evals_per_epoch: null
flash_attention: true
fp16: false
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 4
gradient_checkpointing: true
group_by_length: true
hub_model_id: prxy5606/d78e8e2f-4d69-44b8-af8f-ddf28ac2fcde
hub_repo: null
hub_strategy: checkpoint
hub_token: null
learning_rate: 0.0001
load_in_4bit: false
load_in_8bit: false
local_rank: null
logging_steps: 1
lora_alpha: 128
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 64
lora_target_linear: true
lr_scheduler: cosine
max_grad_norm: 1.0
max_memory:
  0: 75GB
max_steps: 200
micro_batch_size: 8
mlflow_experiment_name: /tmp/dd22c8863ed4176b_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 3
optim_args:
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-5
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
save_steps: 50
saves_per_epoch: null
sequence_len: 1024
strict: false
tf32: true
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: ee0747e5-378f-43ac-83d3-8dd08d6876bf
wandb_project: Gradients-On-Demand
wandb_run: your_name
wandb_runid: ee0747e5-378f-43ac-83d3-8dd08d6876bf
warmup_steps: 10
weight_decay: 0.0
xformers_attention: null

```

</details><br>

# d78e8e2f-4d69-44b8-af8f-ddf28ac2fcde

This model is a fine-tuned version of [trl-internal-testing/tiny-random-LlamaForCausalLM](https://huggingface.co/trl-internal-testing/tiny-random-LlamaForCausalLM) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 10.2952

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 0.0001
- train_batch_size: 8
- eval_batch_size: 4
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 32
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=adam_beta1=0.9,adam_beta2=0.95,adam_epsilon=1e-5
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 10
- training_steps: 52

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 10.3598       | 0.0580 | 1    | 10.3596         |
| 10.5027       | 2.8986 | 50   | 10.2952         |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3825.0,2.1026224153267875e-06
trangtrannnnn/2d2f11b3-fb5f-49db-b6bd-d91a8f0e737e,678a7cb431d14f760824d059,trangtrannnnn,"{""library_name"": ""peft"", ""license"": ""mit"", ""base_model"": ""microsoft/Phi-3-mini-128k-instruct"", ""tags"": [""axolotl"", ""generated_from_trainer""], ""model-index"": [{""name"": ""2d2f11b3-fb5f-49db-b6bd-d91a8f0e737e"", ""results"": []}]}",[],2025-01-17 16:04:50,0,0.0,"{""architectures"": [""Phi3ForCausalLM""], ""auto_map"": {""AutoConfig"": ""microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config"", ""AutoModelForCausalLM"": ""microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM""}, ""model_type"": ""phi3"", ""quantization_config"": {""load_in_4bit"": false, ""load_in_8bit"": true, ""quant_method"": ""bitsandbytes""}, ""tokenizer_config"": {""bos_token"": ""<s>"", ""chat_template"": ""{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"", ""eos_token"": ""<|endoftext|>"", ""pad_token"": ""<|endoftext|>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""peft"": {""base_model_name_or_path"": ""microsoft/Phi-3-mini-128k-instruct"", ""task_type"": ""CAUSAL_LM""}}",1,22,,"['peft' 'safetensors' 'phi3' 'axolotl' 'generated_from_trainer'
 'custom_code' 'base_model:microsoft/Phi-3-mini-128k-instruct'
 'base_model:adapter:microsoft/Phi-3-mini-128k-instruct' 'license:mit'
 '8-bit' 'bitsandbytes' 'region:us']",,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'adapter_config.json'} {'rfilename': 'adapter_model.bin'}
 {'rfilename': 'adapter_model.safetensors'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'config.json'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}
 {'rfilename': 'training_args.bin'}]",2025-01-17 15:52:20,peft,,,adapter,66268fafb1ca2e1fed19a528,microsoft/Phi-3-mini-128k-instruct,trangtrannnnn/2d2f11b3-fb5f-49db-b6bd-d91a8f0e737e,"library_name: peft
license: mit
base_model: microsoft/Phi-3-mini-128k-instruct
tags:
- axolotl
- generated_from_trainer
model-index:
- name: 2d2f11b3-fb5f-49db-b6bd-d91a8f0e737e
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

[<img src=""https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png"" alt=""Built with Axolotl"" width=""200"" height=""32""/>](https://github.com/axolotl-ai-cloud/axolotl)
<details><summary>See axolotl config</summary>

axolotl version: `0.4.1`
```yaml
adapter: lora
base_model: microsoft/Phi-3-mini-128k-instruct
bf16: auto
chat_template: llama3
dataset_prepared_path: null
datasets:
- data_files:
  - e4f941e17d3c68d9_train_data.json
  ds_type: json
  format: custom
  path: /workspace/input_data/e4f941e17d3c68d9_train_data.json
  type:
    field_input: ''
    field_instruction: input_persona
    field_output: prompt
    format: '{instruction}'
    no_input_format: '{instruction}'
    system_format: '{system}'
    system_prompt: ''
debug: null
deepspeed: null
early_stopping_patience: null
eval_max_new_tokens: 128
eval_table_size: null
evals_per_epoch: 1
flash_attention: true
fp16: null
fsdp: null
fsdp_config: null
gradient_accumulation_steps: 4
gradient_checkpointing: true
gradient_clipping: 1.0
group_by_length: false
hub_model_id: trangtrannnnn/2d2f11b3-fb5f-49db-b6bd-d91a8f0e737e
hub_repo: null
hub_strategy: end
hub_token: null
learning_rate: 5.0e-05
load_in_4bit: true
load_in_8bit: true
local_rank: null
logging_steps: 1
lora_alpha: 16
lora_dropout: 0.05
lora_fan_in_fan_out: null
lora_model_dir: null
lora_r: 8
lora_target_linear: true
lr_scheduler: cosine
max_steps: 200
micro_batch_size: 2
mlflow_experiment_name: /tmp/e4f941e17d3c68d9_train_data.json
model_type: AutoModelForCausalLM
num_epochs: 1
optimizer: adamw_bnb_8bit
output_dir: miner_id_24
pad_to_sequence_len: true
resume_from_checkpoint: null
s2_attention: null
sample_packing: false
saves_per_epoch: 1
sequence_len: 1024
strict: false
tf32: false
tokenizer_type: AutoTokenizer
train_on_inputs: false
trust_remote_code: true
val_set_size: 0.05
wandb_entity: null
wandb_mode: online
wandb_name: afd4fc68-8483-4fb9-8bb8-d7f49bb38ced
wandb_project: Gradients-On-Demand
wandb_run: your_name
wandb_runid: afd4fc68-8483-4fb9-8bb8-d7f49bb38ced
warmup_steps: 5
weight_decay: 0.01
xformers_attention: true

```

</details><br>

# 2d2f11b3-fb5f-49db-b6bd-d91a8f0e737e

This model is a fine-tuned version of [microsoft/Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.7151

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 2
- eval_batch_size: 2
- seed: 42
- gradient_accumulation_steps: 4
- total_train_batch_size: 8
- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: cosine
- lr_scheduler_warmup_steps: 5
- training_steps: 200

### Training results

| Training Loss | Epoch  | Step | Validation Loss |
|:-------------:|:------:|:----:|:---------------:|
| 2.656         | 0.0851 | 200  | 0.7151          |


### Framework versions

- PEFT 0.13.2
- Transformers 4.46.0
- Pytorch 2.5.0+cu124
- Datasets 3.0.1
- Tokenizers 0.20.1",3513.0,2.1026224153267875e-06
diegogs1451/bert-finetuned-ner,68ba90c72198594a639a4e21,diegogs1451,"{""library_name"": ""transformers"", ""license"": ""apache-2.0"", ""base_model"": ""bert-base-cased"", ""tags"": [""generated_from_trainer""], ""metrics"": [""precision"", ""recall"", ""f1"", ""accuracy""], ""model-index"": [{""name"": ""bert-finetuned-ner"", ""results"": []}]}",[],2025-09-05 07:37:56,0,0.0,"{""architectures"": [""BertForTokenClassification""], ""model_type"": ""bert"", ""tokenizer_config"": {""cls_token"": ""[CLS]"", ""mask_token"": ""[MASK]"", ""pad_token"": ""[PAD]"", ""sep_token"": ""[SEP]"", ""unk_token"": ""[UNK]""}}",3,18,"{'parameters': {'BF16': None, 'BOOL': None, 'F16': None, 'F32': 107726601, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 107726601.0}","['transformers' 'tensorboard' 'safetensors' 'bert' 'token-classification'
 'generated_from_trainer' 'base_model:google-bert/bert-base-cased'
 'base_model:finetune:google-bert/bert-base-cased' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",token-classification,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'config.json'} {'rfilename': 'model.safetensors'}
 {'rfilename': 'runs/Sep05_07-26-07_16a00f906d56/events.out.tfevents.1757057224.16a00f906d56.530.0'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer_config.json'} {'rfilename': 'training_args.bin'}
 {'rfilename': 'vocab.txt'}]",2025-09-05 07:27:03,transformers,"{'auto_model': 'AutoModelForTokenClassification', 'custom_class': None, 'pipeline_tag': 'token-classification', 'processor': 'AutoTokenizer'}",,finetune,621ffdc036468d709f174331,google-bert/bert-base-cased,diegogs1451/bert-finetuned-ner,"library_name: transformers
license: apache-2.0
base_model: bert-base-cased
tags:
- generated_from_trainer
metrics:
- precision
- recall
- f1
- accuracy
model-index:
- name: bert-finetuned-ner
  results: []","<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# bert-finetuned-ner

This model is a fine-tuned version of [bert-base-cased](https://huggingface.co/bert-base-cased) on an unknown dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0628
- Precision: 0.9329
- Recall: 0.9505
- F1: 0.9416
- Accuracy: 0.9862

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 2e-05
- train_batch_size: 8
- eval_batch_size: 8
- seed: 42
- optimizer: Use OptimizerNames.ADAMW_TORCH_FUSED with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments
- lr_scheduler_type: linear
- num_epochs: 3

### Training results

| Training Loss | Epoch | Step | Validation Loss | Precision | Recall | F1     | Accuracy |
|:-------------:|:-----:|:----:|:---------------:|:---------:|:------:|:------:|:--------:|
| 0.079         | 1.0   | 1756 | 0.0676          | 0.9020    | 0.9308 | 0.9162 | 0.9815   |
| 0.0364        | 2.0   | 3512 | 0.0674          | 0.9266    | 0.9460 | 0.9362 | 0.9848   |
| 0.0217        | 3.0   | 5268 | 0.0628          | 0.9329    | 0.9505 | 0.9416 | 0.9862   |


### Framework versions

- Transformers 4.56.0
- Pytorch 2.8.0+cu126
- Datasets 4.0.0
- Tokenizers 0.22.0",1597.0,1.974503211233854e-06
yuan571/gemma-3-270M-finetune-0829-data7to64-r16-lora16,68b1cfc1b2667c51e61bc462,yuan571,"{""base_model"": ""unsloth/gemma-3-270m-it"", ""tags"": [""text-generation-inference"", ""transformers"", ""unsloth"", ""gemma3_text""], ""license"": ""apache-2.0"", ""language"": [""en""]}",[],2025-08-29 16:05:59,0,0.0,"{""architectures"": [""Gemma3ForCausalLM""], ""model_type"": ""gemma3_text"", ""tokenizer_config"": {""bos_token"": ""<bos>"", ""eos_token"": ""<end_of_turn>"", ""pad_token"": ""<pad>"", ""unk_token"": ""<unk>"", ""use_default_system_prompt"": false}, ""chat_template_jinja"": ""{{ bos_token }}\n{%- if messages[0]['role'] == 'system' -%}\n    {%- if messages[0]['content'] is string -%}\n        {%- set first_user_prefix = messages[0]['content'] + '\n\n' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\n\n' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \""\"" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\""Conversation roles must alternate user/assistant/user/assistant/...\"") }}\n    {%- endif -%}\n    {%- if (message['role'] == 'assistant') -%}\n        {%- set role = \""model\"" -%}\n    {%- else -%}\n        {%- set role = message['role'] -%}\n    {%- endif -%}\n    {{ '<start_of_turn>' + role + '\n' + (first_user_prefix if loop.first else \""\"") }}\n    {%- if message['content'] is string -%}\n        {{ message['content'] | trim }}\n    {%- elif message['content'] is iterable -%}\n        {%- for item in message['content'] -%}\n            {%- if item['type'] == 'image' -%}\n                {{ '<start_of_image>' }}\n            {%- elif item['type'] == 'text' -%}\n                {{ item['text'] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\""Invalid content type\"") }}\n    {%- endif -%}\n    {{ '<end_of_turn>\n' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{ '<start_of_turn>model\n' }}\n{%- endif -%}\n""}",2,18,"{'parameters': {'BF16': 167828096.0, 'BOOL': None, 'F16': 100270080, 'F32': None, 'F64': None, 'F8_E4M3': None, 'F8_E5M2': None, 'I16': None, 'I32': None, 'I64': None, 'I8': None, 'U16': None, 'U32': None, 'U64': None, 'U8': None, 'fastllm': None}, 'total': 268098176.0}","['transformers' 'safetensors' 'gemma3_text' 'text-generation'
 'text-generation-inference' 'unsloth' 'conversational' 'en'
 'base_model:unsloth/gemma-3-270m-it'
 'base_model:finetune:unsloth/gemma-3-270m-it' 'license:apache-2.0'
 'autotrain_compatible' 'endpoints_compatible' 'region:us']",text-generation,"[{'rfilename': '.gitattributes'} {'rfilename': 'README.md'}
 {'rfilename': 'added_tokens.json'} {'rfilename': 'chat_template.jinja'}
 {'rfilename': 'config.json'} {'rfilename': 'generation_config.json'}
 {'rfilename': 'model.safetensors'}
 {'rfilename': 'special_tokens_map.json'} {'rfilename': 'tokenizer.json'}
 {'rfilename': 'tokenizer.model'} {'rfilename': 'tokenizer_config.json'}]",2025-08-29 16:05:21,transformers,"{'auto_model': 'AutoModelForCausalLM', 'custom_class': None, 'pipeline_tag': 'text-generation', 'processor': 'AutoTokenizer'}",,finetune,689bdf086396de9ac2c39d01,unsloth/gemma-3-270m-it,yuan571/gemma-3-270M-finetune-0829-data7to64-r16-lora16,"base_model: unsloth/gemma-3-270m-it
tags:
- text-generation-inference
- transformers
- unsloth
- gemma3_text
license: apache-2.0
language:
- en","# Uploaded finetuned  model

- **Developed by:** yuan571
- **License:** apache-2.0
- **Finetuned from model :** unsloth/gemma-3-270m-it

This gemma3_text model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface's TRL library.

[<img src=""https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png"" width=""200""/>](https://github.com/unslothai/unsloth)",428.0,1.974503211233854e-06
